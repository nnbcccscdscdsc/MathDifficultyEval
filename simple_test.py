#!/usr/bin/env python3
"""
æœ€ç®€å•çš„æ¨ç†æµ‹è¯•
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def simple_test():
    # ä½¿ç”¨æœ€å¯é çš„æ¨¡å‹
    model_name = "microsoft/DialoGPT-medium"
    
    print(f"ğŸ§ª æœ€ç®€å•æµ‹è¯•: {model_name}")
    
    # 1. åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # 2. åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(model_name)
    model = model.to('cuda')
    
    # 3. æœ€ç®€å•æµ‹è¯•
    prompt = "Hello"
    print(f"è¾“å…¥: {prompt}")
    
    # ç¼–ç 
    inputs = tokenizer.encode(prompt, return_tensors="pt").to('cuda')
    print(f"è¾“å…¥tokens: {inputs}")
    
    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_new_tokens=10,
            do_sample=False,
            num_beams=1
        )
    
    # è§£ç 
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"å®Œæ•´è¾“å‡º: {result}")
    
    # æå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
    if prompt in result:
        answer = result[len(prompt):].strip()
    else:
        answer = result.strip()
    
    print(f"ç”Ÿæˆç­”æ¡ˆ: {answer}")
    
    return True

if __name__ == "__main__":
    simple_test() 