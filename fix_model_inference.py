#!/usr/bin/env python3
"""
ä¿®å¤æ¨¡å‹æ¨ç†é—®é¢˜
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_model_fixed(model_name):
    """æµ‹è¯•å•ä¸ªæ¨¡å‹ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰"""
    logger.info(f"ğŸ§ª æµ‹è¯•æ¨¡å‹: {model_name}")
    
    try:
        # 1. åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, local_files_only=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # 2. åŠ è½½æ¨¡å‹
        logger.info("åŠ è½½æ¨¡å‹...")
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            local_files_only=True
        )
        
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        # 3. æµ‹è¯•æ¨ç†
        test_question = "What is 2 + 2?"
        
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ­£ç¡®çš„æç¤ºæ ¼å¼
        if "mistral" in model_name.lower():
            # Mistralæ ¼å¼ - ä¸éœ€è¦<s>æ ‡ç­¾
            prompt = f"[INST] {test_question} [/INST]"
        elif "llama" in model_name.lower():
            # Llamaæ ¼å¼
            prompt = f"[INST] {test_question} [/INST]"
        elif "longalpaca" in model_name.lower():
            # LongAlpacaæ ¼å¼
            prompt = f"<|im_start|>user\n{test_question}<|im_end|>\n<|im_start|>assistant\n"
        else:
            # é»˜è®¤æ ¼å¼
            prompt = f"{test_question}\nAnswer:"
        
        logger.info(f"æç¤º: {prompt}")
        
        # ç¼–ç è¾“å…¥
        inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
        input_ids = inputs.input_ids.to(model.device)
        attention_mask = inputs.attention_mask.to(model.device)
        
        logger.info(f"è¾“å…¥é•¿åº¦: {input_ids.shape[1]}")
        
        logger.info("å¼€å§‹ç”Ÿæˆ...")
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                input_ids,
                attention_mask=attention_mask,
                max_new_tokens=50,
                do_sample=False,
                num_beams=1,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                temperature=0.1,
                repetition_penalty=1.1
            )
        
        logger.info(f"ç”Ÿæˆå®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {outputs.shape}")
        
        # è§£ç å®Œæ•´è¾“å‡º
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        logger.info(f"å®Œæ•´è¾“å‡º: {full_response}")
        
        # ä¿®å¤ç­”æ¡ˆæå–é€»è¾‘
        if "mistral" in model_name.lower() or "llama" in model_name.lower():
            # å¯¹äºMistral/Llamaï¼ŒæŸ¥æ‰¾[/INST]åçš„å†…å®¹
            if "[/INST]" in full_response:
                answer = full_response.split("[/INST]")[-1].strip()
            else:
                answer = full_response.strip()
        elif "longalpaca" in model_name.lower():
            # å¯¹äºLongAlpacaï¼ŒæŸ¥æ‰¾assistantåçš„å†…å®¹
            if "<|im_start|>assistant" in full_response:
                answer = full_response.split("<|im_start|>assistant")[-1].strip()
            else:
                answer = full_response.strip()
        else:
            # é»˜è®¤æå–
            if prompt in full_response:
                answer = full_response[len(prompt):].strip()
            else:
                answer = full_response.strip()
        
        logger.info(f"æå–çš„ç­”æ¡ˆ: '{answer}'")
        
        if answer.strip() and answer.strip() != prompt.strip():
            logger.info("âœ… æ¨ç†æˆåŠŸï¼")
        else:
            logger.warning("âš ï¸ ç”Ÿæˆäº†ç©ºç­”æ¡ˆæˆ–é‡å¤æç¤º")
        
        return answer.strip()
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return ""

def main():
    """ä¸»å‡½æ•°"""
    models = [
        "mistralai/Mistral-7B-Instruct-v0.2",
        "Yukang/LongAlpaca-70B-16k"
    ]
    
    results = {}
    
    for model_name in models:
        logger.info(f"\n{'='*80}")
        answer = test_model_fixed(model_name)
        results[model_name] = answer
        logger.info(f"{'='*80}")
    
    # æ€»ç»“ç»“æœ
    logger.info("\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    for model_name, answer in results.items():
        status = "âœ… æˆåŠŸ" if answer and answer != "What is 2 + 2?" else "âŒ å¤±è´¥"
        logger.info(f"{model_name}: {status} - '{answer}'")

if __name__ == "__main__":
    main() 