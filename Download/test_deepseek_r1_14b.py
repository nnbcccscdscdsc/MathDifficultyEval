#!/usr/bin/env python3
"""
æµ‹è¯•DeepSeek-R1-Distill-Qwen-14Bæ•°å­¦æ¨¡å‹
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_deepseek_r1_14b():
    """æµ‹è¯•DeepSeek-R1-Distill-Qwen-14Bæ¨¡å‹"""
    model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
    
    logger.info(f"ğŸ§® æµ‹è¯•DeepSeek-R1-Distill-Qwen-14Bæ•°å­¦æ¨¡å‹: {model_name}")
    
    try:
        # 1. åŠ è½½tokenizer
        logger.info("åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        tokenizer.pad_token = tokenizer.eos_token
        
        # 2. åŠ è½½æ¨¡å‹ - ä½¿ç”¨4bité‡åŒ–èŠ‚çœå†…å­˜
        logger.info("åŠ è½½æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16
        )
        
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        # 3. æµ‹è¯•æ•°å­¦é—®é¢˜
        test_questions = [
            "What is 2 + 2?",
            "Calculate: 15 * 14 = ?",
            "What is the square root of 16?",
            "Solve for x: x + 5 = 12",
            "What is 3 to the power of 4?",
            "Find the area of a circle with radius 5",
            "What is the derivative of x^2?",
            "Solve the quadratic equation: x^2 + 5x + 6 = 0"
        ]
        
        for i, question in enumerate(test_questions, 1):
            logger.info(f"\n--- æµ‹è¯•é—®é¢˜ {i}: {question} ---")
            
            # DeepSeek-R1æ¨èçš„æç¤ºæ ¼å¼
            # æ³¨æ„ï¼šä¸è¦æ·»åŠ system promptï¼Œæ‰€æœ‰æŒ‡ä»¤éƒ½åœ¨user promptä¸­
            prompt = f"<think>\nPlease reason step by step, and put your final answer within \\boxed{{}}.\n</think>\n\n{question}"
            
            logger.info(f"æç¤º: {prompt}")
            
            # ç¼–ç è¾“å…¥
            inputs = tokenizer(prompt, return_tensors="pt")
            input_ids = inputs.input_ids.to(model.device)
            attention_mask = inputs.attention_mask.to(model.device)
            
            logger.info(f"è¾“å…¥é•¿åº¦: {input_ids.shape[1]}")
            
            logger.info("å¼€å§‹ç”Ÿæˆ...")
            
            # ç”Ÿæˆ - ä½¿ç”¨æ¨èçš„å‚æ•°
            with torch.no_grad():
                outputs = model.generate(
                    input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=400,  # 14Bæ¨¡å‹å¯ä»¥ç”Ÿæˆæ›´é•¿å†…å®¹
                    do_sample=True,
                    temperature=0.6,  # æ¨èæ¸©åº¦
                    top_p=0.95,       # æ¨ètop_p
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            logger.info(f"ç”Ÿæˆå®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {outputs.shape}")
            
            # è§£ç 
            full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            logger.info(f"å®Œæ•´è¾“å‡º: {full_response}")
            
            # æå–ç­”æ¡ˆ - æŸ¥æ‰¾boxedå†…å®¹
            if "\\boxed{" in full_response:
                start = full_response.find("\\boxed{") + 14
                end = full_response.find("}", start)
                if end != -1:
                    answer = full_response[start:end]
                else:
                    answer = full_response[len(prompt):].strip()
            else:
                answer = full_response[len(prompt):].strip()
            
            logger.info(f"æå–çš„ç­”æ¡ˆ: '{answer}'")
            
            if answer.strip():
                logger.info("âœ… ç”ŸæˆæˆåŠŸï¼")
            else:
                logger.warning("âš ï¸ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆç­”æ¡ˆ")
        
        return "æµ‹è¯•å®Œæˆ"
        
    except Exception as e:
        logger.error(f"âŒ DeepSeek-R1-14Bæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return ""

if __name__ == "__main__":
    test_deepseek_r1_14b() 