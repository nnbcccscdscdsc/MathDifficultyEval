#!/usr/bin/env python3
"""
ä»åŸå§‹æ•°æ®ä¸­é€‰å–500ä¸ªæ ·æœ¬ï¼Œç¡®ä¿æ¯ä¸ªéš¾åº¦çº§åˆ«éƒ½å‡åŒ€åŒ…å«
"""

import pandas as pd
import numpy as np
import os
from collections import defaultdict

def create_balanced_dataset(input_file, output_file, target_samples=500):
    """
    ä»åŸå§‹æ•°æ®ä¸­åˆ›å»ºå¹³è¡¡çš„æ•°æ®é›†
    
    Args:
        input_file: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„
        target_samples: ç›®æ ‡æ ·æœ¬æ•°é‡
    """
    print(f"ğŸ“Š åŠ è½½åŸå§‹æ•°æ®é›†: {input_file}")
    
    # è¯»å–åŸå§‹æ•°æ®
    df = pd.read_csv(input_file)
    print(f"âœ… åŸå§‹æ•°æ®é›†åŒ…å« {len(df)} ä¸ªæ ·æœ¬")
    
    # æ£€æŸ¥æ•°æ®åˆ—
    required_columns = ['id', 'problem', 'solution', 'answer', 'difficulty', 'topic']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"âŒ ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")
        return False
    
    # åˆ†æéš¾åº¦åˆ†å¸ƒ
    difficulty_counts = df['difficulty'].value_counts().sort_index()
    print(f"\nğŸ“ˆ åŸå§‹æ•°æ®éš¾åº¦åˆ†å¸ƒ:")
    for difficulty, count in difficulty_counts.items():
        print(f"  éš¾åº¦ {difficulty}: {count} ä¸ªæ ·æœ¬")
    
    # è·å–æ‰€æœ‰å”¯ä¸€çš„éš¾åº¦çº§åˆ«
    unique_difficulties = sorted(df['difficulty'].unique())
    num_difficulties = len(unique_difficulties)
    
    print(f"\nğŸ¯ ç›®æ ‡: ä» {num_difficulties} ä¸ªéš¾åº¦çº§åˆ«ä¸­é€‰å– {target_samples} ä¸ªæ ·æœ¬")
    
    # è®¡ç®—æ¯ä¸ªéš¾åº¦çº§åˆ«åº”è¯¥é€‰å–çš„æ ·æœ¬æ•°
    base_samples_per_difficulty = target_samples // num_difficulties
    remaining_samples = target_samples % num_difficulties
    
    print(f"ğŸ“Š æ¯ä¸ªéš¾åº¦çº§åˆ«åŸºç¡€æ ·æœ¬æ•°: {base_samples_per_difficulty}")
    print(f"ğŸ“Š å‰©ä½™æ ·æœ¬æ•°: {remaining_samples}")
    
    # åˆ›å»ºå¹³è¡¡çš„æ•°æ®é›†
    balanced_samples = []
    
    for i, difficulty in enumerate(unique_difficulties):
        # è·å–å½“å‰éš¾åº¦çº§åˆ«çš„æ‰€æœ‰æ ·æœ¬
        difficulty_df = df[df['difficulty'] == difficulty]
        available_samples = len(difficulty_df)
        
        # è®¡ç®—å½“å‰éš¾åº¦çº§åˆ«åº”é€‰å–çš„æ ·æœ¬æ•°
        if i < remaining_samples:
            samples_needed = base_samples_per_difficulty + 1
        else:
            samples_needed = base_samples_per_difficulty
        
        # å¦‚æœå¯ç”¨æ ·æœ¬æ•°ä¸è¶³ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨æ ·æœ¬
        if available_samples <= samples_needed:
            samples_needed = available_samples
            print(f"âš ï¸ éš¾åº¦ {difficulty}: å¯ç”¨æ ·æœ¬ä¸è¶³ï¼Œä½¿ç”¨å…¨éƒ¨ {available_samples} ä¸ªæ ·æœ¬")
        else:
            print(f"âœ… éš¾åº¦ {difficulty}: éšæœºé€‰æ‹© {samples_needed} ä¸ªæ ·æœ¬ï¼ˆå…± {available_samples} ä¸ªï¼‰")
        
        # éšæœºé‡‡æ ·
        if available_samples > 0:
            sampled = difficulty_df.sample(n=samples_needed, random_state=42)
            balanced_samples.append(sampled)
    
    # åˆå¹¶æ‰€æœ‰é‡‡æ ·çš„æ ·æœ¬
    if balanced_samples:
        result_df = pd.concat(balanced_samples, ignore_index=True)
        
        # é‡æ–°æ’åºï¼Œç¡®ä¿éšæœºæ€§
        result_df = result_df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        print(f"\nâœ… å¹³è¡¡é‡‡æ ·å®Œæˆï¼Œå…± {len(result_df)} ä¸ªæ ·æœ¬")
        
        # æ˜¾ç¤ºæœ€ç»ˆéš¾åº¦åˆ†å¸ƒ
        final_difficulty_counts = result_df['difficulty'].value_counts().sort_index()
        print(f"\nğŸ“Š æœ€ç»ˆæ•°æ®éš¾åº¦åˆ†å¸ƒ:")
        for difficulty, count in final_difficulty_counts.items():
            print(f"  éš¾åº¦ {difficulty}: {count} ä¸ªæ ·æœ¬")
        
        # ä¿å­˜ç»“æœ
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        result_df.to_csv(output_file, index=False)
        print(f"\nğŸ’¾ å¹³è¡¡æ•°æ®é›†å·²ä¿å­˜åˆ°: {output_file}")
        
        # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
        print(f"\nğŸ“‹ æ•°æ®é›†ä¿¡æ¯:")
        print(f"  æ€»æ ·æœ¬æ•°: {len(result_df)}")
        print(f"  éš¾åº¦çº§åˆ«æ•°: {len(unique_difficulties)}")
        print(f"  å¹³å‡æ¯ä¸ªéš¾åº¦çº§åˆ«: {len(result_df) / len(unique_difficulties):.1f} ä¸ªæ ·æœ¬")
        
        # æ˜¾ç¤ºä¸»é¢˜åˆ†å¸ƒ
        topic_counts = result_df['topic'].value_counts().head(10)
        print(f"\nğŸ“š ä¸»è¦ä¸»é¢˜åˆ†å¸ƒ (å‰10):")
        for topic, count in topic_counts.items():
            print(f"  {topic}: {count} ä¸ªæ ·æœ¬")
        
        return True
    else:
        print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æ ·æœ¬")
        return False

def main():
    """ä¸»å‡½æ•°"""
    # æ–‡ä»¶è·¯å¾„
    input_file = "data/processed/deepmath_evaluation_dataset.csv"
    output_file = "data/processed/balanced_500_samples.csv"
    
    print("ğŸš€ å¼€å§‹åˆ›å»ºå¹³è¡¡æ•°æ®é›†")
    print("=" * 50)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return
    
    # åˆ›å»ºå¹³è¡¡æ•°æ®é›†
    success = create_balanced_dataset(input_file, output_file, target_samples=500)
    
    if success:
        print("\nğŸ‰ å¹³è¡¡æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼")
        print(f"ğŸ“ æ–‡ä»¶ä½ç½®: {output_file}")
    else:
        print("\nâŒ å¹³è¡¡æ•°æ®é›†åˆ›å»ºå¤±è´¥")

if __name__ == "__main__":
    main() 