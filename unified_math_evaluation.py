#!/usr/bin/env python3
"""
ç»Ÿä¸€æ•°å­¦è¯„ä¼°è„šæœ¬
æ”¯æŒå¤šç§æ¨¡å‹å’Œé…ç½®çš„æ•°å­¦è¯„ä¼°ï¼ŒåŒ…å«æ™ºèƒ½æ¨¡å‹æ£€æµ‹å’Œè¿œç¨‹å›é€€
"""

import argparse
import os
import sys
import openai
import logging
from math_evaluation_framework import MathEvaluationFramework, MODEL_CONFIGS

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å…³é—­OpenAIåº“çš„è°ƒè¯•æ—¥å¿—
logging.getLogger("openai").setLevel(logging.ERROR)
logging.getLogger("httpx").setLevel(logging.ERROR)
logging.getLogger("urllib3").setLevel(logging.ERROR)

def check_local_model_availability(model_name: str) -> bool:
    """æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å¯ç”¨"""
    try:
        from transformers import AutoTokenizer
        # å°è¯•åŠ è½½tokenizeræ¥æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            local_files_only=False  # å…è®¸ä»ç¼“å­˜åŠ è½½
        )
        logger.info(f"âœ… æœ¬åœ°æ¨¡å‹ {model_name} å¯ç”¨")
        return True
        
    except Exception as e:
        logger.warning(f"âŒ æœ¬åœ°æ¨¡å‹ {model_name} ä¸å¯ç”¨: {e}")
        return False

def test_remote_model_connection(model_name: str, hf_token: str) -> bool:
    """æµ‹è¯•è¿œç¨‹æ¨¡å‹è¿æ¥"""
    try:
        if not hf_token:
            logger.error("âŒ æœªè®¾ç½®HF_TOKENï¼Œæ— æ³•æµ‹è¯•è¿œç¨‹æ¨¡å‹")
            return False
        
        # è®¾ç½®OpenAIå®¢æˆ·ç«¯
        openai.api_key = hf_token
        openai.api_base = "https://router.huggingface.co/v1"
        
        # ä¸º70Bæ¨¡å‹æ·»åŠ :novitaåç¼€
        if "70B" in model_name:
            model_name_with_suffix = f"{model_name}:novita"
        else:
            model_name_with_suffix = model_name
        
        # å‘é€ç®€å•æµ‹è¯•è¯·æ±‚
        test_prompt = "What is 2 + 2?"
        completion = openai.ChatCompletion.create(
            model=model_name_with_suffix,
            messages=[{"role": "user", "content": test_prompt}],
            max_tokens=50,
            temperature=0.1,
            top_p=0.9
        )
        
        logger.info(f"âœ… è¿œç¨‹æ¨¡å‹ {model_name_with_suffix} è¿æ¥æ­£å¸¸")
        return True
        
    except Exception as e:
        logger.warning(f"âŒ è¿œç¨‹æ¨¡å‹ {model_name} è¿æ¥å¤±è´¥: {e}")
        return False

def select_best_model(model_key: str, hf_token: str = None) -> tuple:
    """é€‰æ‹©æœ€ä½³å¯ç”¨æ¨¡å‹"""
    logger.info(f"ğŸ” æ£€æµ‹æ¨¡å‹å¯ç”¨æ€§ï¼Œé¦–é€‰: {model_key}")
    
    # è¿œç¨‹æ¨¡å‹æ˜ å°„
    remote_models = {
        "deepseek_r1_7b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "deepseek_r1_14b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        "deepseek_r1_32b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
        "deepseek_r1_70b": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
    }
    
    # 70Bæ¨¡å‹ç›´æ¥èµ°è¿œç«¯APIï¼Œè·³è¿‡æœ¬åœ°æ£€æŸ¥
    if model_key == "deepseek_r1_70b" and hf_token:
        logger.info("ğŸš€ 70Bæ¨¡å‹ç›´æ¥ä½¿ç”¨è¿œç«¯API")
        remote_model_name = remote_models[model_key]
        if test_remote_model_connection(remote_model_name, hf_token):
            remote_config = {
                "name": remote_model_name,
                "type": "remote",
                "description": f"è¿œç¨‹{model_key}æ¨¡å‹"
            }
            return ("remote", remote_config)
        else:
            raise ValueError("âŒ 70Bè¿œç«¯æ¨¡å‹è¿æ¥å¤±è´¥")
    
    # å…¶ä»–æ¨¡å‹å…ˆæ£€æŸ¥æœ¬åœ°ï¼Œå†æ£€æŸ¥è¿œç¨‹
    # æ£€æŸ¥æœ¬åœ°æ¨¡å‹
    if model_key in MODEL_CONFIGS:
        local_model_name = MODEL_CONFIGS[model_key]['name']
        if check_local_model_availability(local_model_name):
            return ("local", MODEL_CONFIGS[model_key])
    
    # æ£€æŸ¥è¿œç¨‹æ¨¡å‹
    if model_key in remote_models and hf_token:
        remote_model_name = remote_models[model_key]
        if test_remote_model_connection(remote_model_name, hf_token):
            # åˆ›å»ºè¿œç¨‹æ¨¡å‹é…ç½®
            remote_config = {
                "name": remote_model_name,
                "type": "remote",
                "description": f"è¿œç¨‹{model_key}æ¨¡å‹"
            }
            return ("remote", remote_config)
    
    # å°è¯•å…¶ä»–æ¨¡å‹å¤§å°
    model_sizes = ["7b", "14b", "32b", "70b"]
    for size in model_sizes:
        # æ£€æŸ¥æœ¬åœ°
        test_key = f"deepseek_r1_{size}"
        if test_key in MODEL_CONFIGS:
            local_model_name = MODEL_CONFIGS[test_key]['name']
            if check_local_model_availability(local_model_name):
                return ("local", MODEL_CONFIGS[test_key])
        
        # æ£€æŸ¥è¿œç¨‹
        if test_key in remote_models and hf_token:
            remote_model_name = remote_models[test_key]
            if test_remote_model_connection(remote_model_name, hf_token):
                remote_config = {
                    "name": remote_model_name,
                    "type": "remote",
                    "description": f"è¿œç¨‹{test_key}æ¨¡å‹"
                }
                return ("remote", remote_config)
    
    raise ValueError("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")

def main():
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€æ•°å­¦è¯„ä¼°è„šæœ¬")
    parser.add_argument("-m", "--model", type=str, default="deepseek_r1_7b", 
                       help="æ¨¡å‹åç§°")
    parser.add_argument("-s", "--samples", type=int, default=50, 
                       help="æ ·æœ¬æ•°é‡")
    parser.add_argument("--no-openai", action="store_true", 
                       help="ç¦ç”¨OpenAIè¯„ä¼°ï¼Œåªè¿›è¡Œæ¨¡å‹ç”Ÿæˆæµ‹è¯•")
    parser.add_argument("--dataset", type=str, default="data/processed/fixed_200_samples.csv",
                       help="æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--hf-token", type=str, 
                       help="Hugging Face API Token")
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆç»Ÿä¸€æ•°å­¦è¯„ä¼°")
    print(f"ğŸ¤– é¦–é€‰æ¨¡å‹: {args.model}")
    print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {args.samples}")
    if args.no_openai:
        print("ğŸš« OpenAIè¯„ä¼°: å·²ç¦ç”¨")
    else:
        print("âœ… OpenAIè¯„ä¼°: å·²å¯ç”¨")
    print("=" * 50)
    
    # è·å–HF Token
    hf_token = args.hf_token or os.getenv("HF_TOKEN")
    if hf_token:
        print(f"ğŸ”‘ HF Token: {hf_token[:10]}...{hf_token[-4:]}")
    else:
        print("âš ï¸ æœªè®¾ç½®HF_TOKENï¼Œå°†åªä½¿ç”¨æœ¬åœ°æ¨¡å‹")
    
    # æ™ºèƒ½é€‰æ‹©æœ€ä½³æ¨¡å‹
    try:
        model_type, model_config = select_best_model(args.model, hf_token)
        print(f"âœ… é€‰æ‹©æ¨¡å‹ç±»å‹: {model_type}")
        print(f"ğŸ“‹ æ¨¡å‹ä¿¡æ¯: {model_config['description']}")
    except ValueError as e:
        print(f"âŒ {e}")
        return
    
    # æ£€æŸ¥æ•°æ®é›†
    if not os.path.exists(args.dataset):
        print(f"âŒ æ•°æ®é›†ä¸å­˜åœ¨: {args.dataset}")
        return
    
    print(f"âœ… æ‰¾åˆ°æ ·æœ¬æ–‡ä»¶: {args.dataset}")
    
    # è®¾ç½®OpenAI APIå¯†é’¥
    openai_api_key = None
    if not args.no_openai:
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            print("âš ï¸ æœªè®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
            print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export OPENAI_API_KEY='your-api-key'")
            print("æˆ–è€…ä½¿ç”¨ --no-openai å‚æ•°ç¦ç”¨OpenAIè¯„ä¼°")
            return
    
    print("\nğŸ”§ åˆ›å»ºè¯„ä¼°æ¡†æ¶...")
    
    # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºä¸åŒçš„è¯„ä¼°æ¡†æ¶
    if model_type == "local":
        # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        framework = MathEvaluationFramework(
            model_config=model_config,
            openai_api_key=openai_api_key,
            max_samples=args.samples
        )
    else:
        # ä½¿ç”¨è¿œç¨‹æ¨¡å‹
        print("ğŸš€ ä½¿ç”¨è¿œç¨‹æ¨¡å‹è¯„ä¼°...")
        
        # åˆ›å»ºè¿œç¨‹è¯„ä¼°æ¡†æ¶
        class RemoteMathEvaluationFramework:
            def __init__(self, model_config, openai_api_key=None, max_samples=200):
                self.model_config = model_config
                self.model_name = model_config['name']
                self.max_samples = max_samples
                self.openai_api_key = openai_api_key
                
                # è®¾ç½®æ—¥å¿—
                import logging
                self.logger = logging.getLogger(__name__)
                
                # ç”Ÿæˆè¿è¡Œæ ‡è¯†ç¬¦
                from datetime import datetime
                import random
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                random_suffix = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=4))
                self.run_id = f"{timestamp}_{random_suffix}"
                self.logger.info(f"ğŸ†” æœ¬æ¬¡è¿è¡ŒID: {self.run_id}")
                
                # è®¾ç½®è¿œç¨‹API
                import openai
                self.openai_client = openai
                if openai_api_key:
                    openai.api_key = openai_api_key
                
                # è®¾ç½®HF API
                self.hf_token = os.getenv("HF_TOKEN")
                if self.hf_token:
                    openai.api_key = self.hf_token
                    openai.api_base = "https://router.huggingface.co/v1"
            
            def load_dataset(self, dataset_path):
                """åŠ è½½æ•°æ®é›†"""
                import pandas as pd
                df = pd.read_csv(dataset_path)
                dataset = []
                for _, row in df.iterrows():
                    sample = {
                        'id': row['id'],
                        'problem': row['problem'],
                        'solution': row['solution'],
                        'answer': row['answer'],
                        'difficulty': row['difficulty'],
                        'topic': row['topic'],
                        'difficulty_score': row['difficulty_score']
                    }
                    dataset.append(sample)
                return dataset[:self.max_samples]  # é™åˆ¶æ ·æœ¬æ•°é‡
            
            def generate_response(self, problem):
                """ä½¿ç”¨è¿œç¨‹æ¨¡å‹ç”Ÿæˆå›ç­”"""
                try:
                    # DeepSeek-R1æ¨èçš„æç¤ºæ ¼å¼
                    prompt = f"<think>\nPlease reason step by step, and put your final answer within \\boxed{{}}.\n</think>\n\n{problem}\n\n<think>\n"
                    
                    # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„APIè®¾ç½®
                    original_api_base = self.openai_client.api_base
                    original_api_key = self.openai_client.api_key
                    
                    # ä½¿ç”¨HF Router API (ä¸32BæˆåŠŸé…ç½®ä¸€è‡´)
                    self.openai_client.api_base = "https://router.huggingface.co/v1"
                    self.openai_client.api_key = self.hf_token
                    
                    # ä¸º70Bæ¨¡å‹æ·»åŠ :novitaåç¼€ï¼ˆä¸32BæˆåŠŸé…ç½®ä¸€è‡´ï¼‰
                    model_name_with_suffix = f"{self.model_name}:novita"
                    
                    completion = self.openai_client.ChatCompletion.create(
                        model=model_name_with_suffix,
                        messages=[{"role": "user", "content": prompt}],
                        max_tokens=500,
                        temperature=0.1,  # é™ä½æ¸©åº¦ï¼Œæé«˜ç¨³å®šæ€§
                        top_p=0.9
                    )
                    
                    # æ¢å¤åŸå§‹è®¾ç½®
                    self.openai_client.api_base = original_api_base
                    self.openai_client.api_key = original_api_key
                    
                    response = completion.choices[0].message.content
                    return response.replace(prompt, "").strip()
                    
                except Exception as e:
                    self.logger.error(f"âŒ è¿œç¨‹ç”Ÿæˆå¤±è´¥: {e}")
                    return f"ç”Ÿæˆå¤±è´¥: {str(e)}"
            
            def evaluate_with_openai(self, problem, model_response, correct_answer, standard_solution):
                """ä½¿ç”¨OpenAIè¯„ä¼°æ¨¡å‹å›ç­”ï¼ŒåŒ…å«æ ‡å‡†è§£æ³•å‚è€ƒ"""
                if not self.openai_api_key:
                    return {"error": "OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"}
                
                try:
                    # ä¸´æ—¶åˆ‡æ¢åˆ°OpenAI API
                    original_api_base = self.openai_client.api_base
                    self.openai_client.api_base = "https://api.openai.com/v1"
                    self.openai_client.api_key = self.openai_api_key
                    
                    evaluation_prompt = f"""
You are a professional mathematical education evaluator. Please evaluate the quality of the answer to the following mathematical problem.

Problem: {problem}

Correct Answer: {correct_answer}

Standard Solution: {standard_solution}

Model Response: {model_response}

Please evaluate from the following aspects and give a score from 1 to 10:

1. Answer Correctness (1-10 points): Whether the final answer is correct
2. Reasoning Logic (1-10 points): Whether the reasoning process is clear and logical, compared to the standard solution
3. Step Completeness (1-10 points): Whether all necessary solution steps are shown, considering what the standard solution covers
4. Mathematical Accuracy (1-10 points): Whether mathematical calculations and formulas are accurate
5. Expression Clarity (1-10 points): Whether the expression is clear and easy to understand

IMPORTANT: You must respond with ONLY a valid JSON object. Do not include any other text, explanations, or markdown formatting.

Required JSON format:
{{
    "answer_correctness": <score>,
    "reasoning_logic": <score>,
    "step_completeness": <score>,
    "mathematical_accuracy": <score>,
    "expression_clarity": <score>,
    "overall_score": <average_of_all_scores>,
    "comments": "<brief_evaluation_comments>"
}}

Example:
{{
    "answer_correctness": 8,
    "reasoning_logic": 7,
    "step_completeness": 9,
    "mathematical_accuracy": 8,
    "expression_clarity": 7,
    "overall_score": 7.8,
    "comments": "Good reasoning but could be more detailed in steps"
}}
"""
                    
                    response = self.openai_client.ChatCompletion.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": "You are a professional mathematical education evaluator."},
                            {"role": "user", "content": evaluation_prompt}
                        ],
                        temperature=0.3
                    )
                    
                    # æ¢å¤HF APIè®¾ç½®
                    self.openai_client.api_base = original_api_base
                    self.openai_client.api_key = self.hf_token
                    
                    response_content = response.choices[0].message.content
                    
                    # è§£æJSONå“åº”
                    import json
                    import re
                    
                    try:
                        evaluation = json.loads(response_content)
                        return evaluation
                    except json.JSONDecodeError as e:
                        self.logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
                        self.logger.warning(f"åŸå§‹å“åº”: {response_content[:200]}...")  # åªæ˜¾ç¤ºå‰200å­—ç¬¦
                        
                        # å°è¯•æå–JSONéƒ¨åˆ†
                        try:
                            # æŸ¥æ‰¾JSONå¯¹è±¡
                            json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
                            if json_match:
                                json_str = json_match.group()
                                evaluation = json.loads(json_str)
                                self.logger.info(f"âœ… æˆåŠŸæå–JSONéƒ¨åˆ†")
                                return evaluation
                        except:
                            pass
                        
                        # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                        try:
                            # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                            cleaned_response = response_content.replace('```json', '').replace('```', '').strip()
                            evaluation = json.loads(cleaned_response)
                            self.logger.info(f"âœ… æˆåŠŸä¿®å¤JSONæ ¼å¼")
                            return evaluation
                        except:
                            pass
                        
                        return {
                            "raw_response": response_content,
                            "error": "JSON parsing failed",
                            "parse_error": str(e),
                            "error_type": "json_decode_error"
                        }
                        
                except Exception as e:
                    self.logger.error(f"âŒ OpenAI evaluation failed: {e}")
                    return {
                        "error": f"Evaluation failed: {e}",
                        "error_type": "openai_api_error",
                        "exception": str(e)
                    }
            
            def run_evaluation(self, dataset_path):
                """è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹"""
                self.logger.info("ğŸš€ å¼€å§‹å®Œæ•´è¯„ä¼°æµç¨‹")
                self.logger.info(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {dataset_path}")
                dataset = self.load_dataset(dataset_path)
                self.logger.info(f"âœ… åŠ è½½äº† {len(dataset)} ä¸ªæ ·æœ¬")
                
                # æ£€æŸ¥å·²æœ‰ç»“æœï¼Œå®ç°æ–­ç‚¹ç»­ä¼ 
                start_index = self._check_existing_results()
                if start_index > 0:
                    self.logger.info(f"ğŸ”„ å‘ç°å·²æœ‰ç»“æœï¼Œä»ç¬¬ {start_index + 1} ä¸ªæ ·æœ¬å¼€å§‹ç»§ç»­è¯„ä¼°")
                    dataset = dataset[start_index:]
                else:
                    self.logger.info(f"ğŸ†• å¼€å§‹å…¨æ–°è¯„ä¼°")
                
                results = []
                successful_generations = 0
                successful_evaluations = 0
                
                self.logger.info(f"ğŸ“ å¼€å§‹è¯„ä¼° {len(dataset)} ä¸ªæ ·æœ¬...")
                self.logger.info(f"â±ï¸ é¢„è®¡éœ€è¦æ—¶é—´: {len(dataset) * 30 / 60:.1f} åˆ†é’Ÿï¼ˆå‡è®¾æ¯ä¸ªæ ·æœ¬30ç§’ï¼‰")
                
                from tqdm import tqdm
                for i, sample in enumerate(tqdm(dataset, desc="è¯„ä¼°è¿›åº¦")):
                    global_index = start_index + i + 1
                    self.logger.info(f"\n--- æ ·æœ¬ {global_index}/351: {sample['id']} ---")
                    
                    # ç”Ÿæˆæ¨¡å‹å›ç­”
                    model_response = self.generate_response(sample['problem'])
                    
                    if model_response and not model_response.startswith("ç”Ÿæˆå¤±è´¥"):
                        successful_generations += 1
                        
                        # OpenAIè¯„ä¼°
                        evaluation = self.evaluate_with_openai(
                            sample['problem'], 
                            model_response, 
                            sample['answer'],
                            sample['solution']  # ä¼ å…¥æ ‡å‡†è§£æ³•
                        )
                        
                        if isinstance(evaluation, dict) and "error" not in evaluation:
                            successful_evaluations += 1
                            self.logger.info(f"âœ… è¯„ä¼°å®Œæˆï¼Œæ€»åˆ†: {evaluation.get('overall_score', 0):.2f}")
                        else:
                            # è¯¦ç»†é”™è¯¯ä¿¡æ¯å¤„ç†
                            if isinstance(evaluation, dict):
                                error_msg = evaluation.get('error', 'æœªçŸ¥é”™è¯¯')
                                parse_error = evaluation.get('parse_error', '')
                                raw_response = evaluation.get('raw_response', '')[:200]  # åªæ˜¾ç¤ºå‰200å­—ç¬¦
                                
                                if parse_error:
                                    self.logger.warning(f"âš ï¸ è¯„ä¼°å¤±è´¥: {error_msg}")
                                    self.logger.warning(f"è§£æé”™è¯¯: {parse_error}")
                                    if raw_response:
                                        self.logger.warning(f"åŸå§‹å“åº”: {raw_response}...")
                                else:
                                    self.logger.warning(f"âš ï¸ è¯„ä¼°å¤±è´¥: {error_msg}")
                            else:
                                self.logger.warning(f"âš ï¸ è¯„ä¼°å¤±è´¥: {str(evaluation)}")
                            
                            # ä¸ºè¯„ä¼°å¤±è´¥çš„æ ·æœ¬åˆ›å»ºç‰¹æ®Šè¯„ä¼°
                            failed_evaluation = {
                                "answer_correctness": 0,
                                "reasoning_logic": 0,
                                "step_completeness": 0,
                                "mathematical_accuracy": 0,
                                "expression_clarity": 0,
                                "overall_score": 0,
                                "comments": f"è¯„ä¼°å¤±è´¥: {str(evaluation)}",
                                "error": "evaluation_failed"
                            }
                            evaluation = failed_evaluation
                        
                        # ä¿å­˜ç»“æœ
                        result = {
                            "id": sample['id'],
                            "problem": sample['problem'],
                            "correct_answer": sample['answer'],
                            "standard_solution": sample['solution'],  # æ·»åŠ åŸå§‹è§£æ³•
                            "model_response": model_response,
                            "difficulty": sample['difficulty'],
                            "topic": sample['topic'],
                            "evaluation": evaluation,
                            "generation_status": "success"
                        }
                        results.append(result)
                    else:
                        self.logger.warning(f"âŒ ç”Ÿæˆå¤±è´¥: {model_response}")
                        
                        # ä¸ºç”Ÿæˆå¤±è´¥çš„æ ·æœ¬åˆ›å»ºç‰¹æ®Šè¯„ä¼°
                        failed_evaluation = {
                            "answer_correctness": 0,
                            "reasoning_logic": 0,
                            "step_completeness": 0,
                            "mathematical_accuracy": 0,
                            "expression_clarity": 0,
                            "overall_score": 0,
                            "comments": f"ç”Ÿæˆå¤±è´¥: {model_response}",
                            "error": "generation_failed"
                        }
                        
                        result = {
                            "id": sample['id'],
                            "problem": sample['problem'],
                            "correct_answer": sample['answer'],
                            "standard_solution": sample['solution'],  # æ·»åŠ åŸå§‹è§£æ³•
                            "model_response": model_response,
                            "difficulty": sample['difficulty'],
                            "topic": sample['topic'],
                            "evaluation": failed_evaluation,
                            "generation_status": "failed"
                        }
                        results.append(result)
                    
                    # æ¯10ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
                    if (i + 1) % 10 == 0:
                        global_count = start_index + i + 1
                        self.save_intermediate_results(results, global_count)
                        self.logger.info(f"ğŸ’¾ å·²å¤„ç† {global_count}/351 ä¸ªæ ·æœ¬ï¼Œä¸­é—´ç»“æœå·²ä¿å­˜")
                
                # è®¡ç®—æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
                if results:
                    scores = [r['evaluation'].get('overall_score', 0) for r in results if r.get('generation_status') == 'success' and 'overall_score' in r['evaluation']]
                    avg_score = sum(scores) / len(scores) if scores else 0
                    
                    final_results = {
                        "results": results,
                        "summary": {
                            "total_evaluated": len(results),
                            "successful_generations": successful_generations,
                            "failed_generations": len(results) - successful_generations,
                            "generation_success_rate": successful_generations / len(results) if results else 0,
                            "evaluation_success_rate": successful_evaluations / len(results) if results else 0,
                            "average_overall_score": avg_score
                        }
                    }
                    
                    self.logger.info(f"\nğŸ“‹ è¯„ä¼°æ‘˜è¦:")
                    self.logger.info(f"æ€»è¯„ä¼°æ ·æœ¬: {len(results)}")
                    self.logger.info(f"ç”ŸæˆæˆåŠŸç‡: {successful_generations / len(results) * 100:.1f}%")
                    self.logger.info(f"å¹³å‡æ€»åˆ†: {avg_score:.2f}")
                    
                    return final_results
                else:
                    return {"error": "æ²¡æœ‰æœ‰æ•ˆç»“æœ"}
            
            def save_intermediate_results(self, results, count):
                """ä¿å­˜ä¸­é—´ç»“æœ"""
                import json
                import os
                from datetime import datetime
                
                # åˆ›å»ºæ¨¡å‹ä¸“ç”¨ç›®å½•
                model_safe_name = self.model_name.replace('/', '_').replace('-', '_')
                model_dir = f"data/intermediate/{model_safe_name}"
                os.makedirs(model_dir, exist_ok=True)
                
                # ä½¿ç”¨è¿è¡ŒIDåˆ›å»ºå­ç›®å½•
                run_dir = f"{model_dir}/{self.run_id}"
                os.makedirs(run_dir, exist_ok=True)
                
                filename = f"{run_dir}/intermediate_results_{count}.json"
                
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                
                self.logger.info(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: {filename}")
    
            def _check_existing_results(self):
                """æ£€æŸ¥å·²æœ‰ç»“æœï¼Œè¿”å›åº”è¯¥å¼€å§‹çš„æ ·æœ¬ç´¢å¼•"""
                import os
                import json
                import glob
                
                # åˆ›å»ºæ¨¡å‹ä¸“ç”¨ç›®å½•
                model_safe_name = self.model_name.replace('/', '_').replace('-', '_')
                model_dir = f"data/intermediate/{model_safe_name}"
                
                if not os.path.exists(model_dir):
                    return 0
                
                # æŸ¥æ‰¾æ‰€æœ‰è¿è¡Œç›®å½•
                run_dirs = glob.glob(f"{model_dir}/*")
                if not run_dirs:
                    return 0
                
                # æ‰¾åˆ°æœ€æ–°çš„è¿è¡Œç›®å½•
                latest_run_dir = max(run_dirs, key=os.path.getctime)
                
                # æŸ¥æ‰¾è¯¥è¿è¡Œç›®å½•ä¸‹çš„æ‰€æœ‰ä¸­é—´ç»“æœæ–‡ä»¶
                result_files = glob.glob(f"{latest_run_dir}/intermediate_results_*.json")
                if not result_files:
                    return 0
                
                # æ‰¾åˆ°æœ€å¤§çš„æ ·æœ¬æ•°é‡
                max_count = 0
                for file_path in result_files:
                    try:
                        filename = os.path.basename(file_path)
                        # æå–æ–‡ä»¶åä¸­çš„æ•°å­—ï¼Œå¦‚ intermediate_results_30.json -> 30
                        count_str = filename.replace('intermediate_results_', '').replace('.json', '')
                        count = int(count_str)
                        max_count = max(max_count, count)
                    except:
                        continue
                
                self.logger.info(f"ğŸ“ å‘ç°å·²æœ‰ç»“æœç›®å½•: {latest_run_dir}")
                self.logger.info(f"ğŸ“Š å·²å¤„ç†æ ·æœ¬æ•°é‡: {max_count}")
                
                return max_count
        
        # åˆ›å»ºè¿œç¨‹è¯„ä¼°æ¡†æ¶å®ä¾‹
        framework = RemoteMathEvaluationFramework(
            model_config=model_config,
            openai_api_key=openai_api_key,
            max_samples=args.samples
        )
    
    # è¿è¡Œè¯„ä¼°
    try:
        results = framework.run_evaluation(args.dataset)
        print("ğŸ‰ è¯„ä¼°å®Œæˆï¼")
        return results
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main() 