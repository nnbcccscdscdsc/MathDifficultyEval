#!/usr/bin/env python3
"""
å¯¹æ¯”ä¸åŒçš„Mistralæ¨¡å‹ç‰ˆæœ¬
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import logging
import time

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_mistral_model(model_name):
    """æµ‹è¯•å•ä¸ªMistralæ¨¡å‹"""
    logger.info(f"ğŸ§ª æµ‹è¯•æ¨¡å‹: {model_name}")
    
    try:
        # 1. åŠ è½½tokenizer
        logger.info("åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, local_files_only=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # 2. åŠ è½½æ¨¡å‹
        logger.info("åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            local_files_only=True
        )
        
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        # 3. æµ‹è¯•æ¨ç†
        test_question = "What is 2 + 2?"
        
        # å°è¯•ä¸åŒçš„æç¤ºæ ¼å¼
        test_prompts = [
            {
                "name": "ç®€å•æ ¼å¼",
                "prompt": f"{test_question}\nAnswer:"
            },
            {
                "name": "Mistralæ ¼å¼",
                "prompt": f"[INST] {test_question} [/INST]"
            },
            {
                "name": "è¯¦ç»†æ ¼å¼",
                "prompt": f"[INST] Please solve this math problem: {test_question} [/INST]"
            }
        ]
        
        best_answer = ""
        best_prompt = ""
        
        for test_case in test_prompts:
            prompt = test_case['prompt']
            logger.info(f"æµ‹è¯•æç¤º: {test_case['name']}")
            
            # ç¼–ç è¾“å…¥
            inputs = tokenizer(prompt, return_tensors="pt")
            input_ids = inputs.input_ids.to(model.device)
            
            logger.info("å¼€å§‹ç”Ÿæˆ...")
            start_time = time.time()
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = model.generate(
                    input_ids,
                    max_new_tokens=20,
                    do_sample=False,
                    num_beams=1,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            end_time = time.time()
            generation_time = end_time - start_time
            
            # è§£ç 
            full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–ç­”æ¡ˆ
            if prompt in full_response:
                answer = full_response[len(prompt):].strip()
            else:
                answer = full_response.strip()
            
            logger.info(f"ç­”æ¡ˆ: '{answer}' (è€—æ—¶: {generation_time:.2f}ç§’)")
            
            # æ£€æŸ¥ç­”æ¡ˆè´¨é‡
            if "4" in answer or "four" in answer.lower():
                best_answer = answer
                best_prompt = test_case['name']
                logger.info("âœ… æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆï¼")
                break
            elif answer.strip() and not best_answer:
                best_answer = answer
                best_prompt = test_case['name']
        
        logger.info(f"æœ€ä½³ç­”æ¡ˆ: '{best_answer}' (ä½¿ç”¨æç¤º: {best_prompt})")
        return best_answer.strip()
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return ""

def main():
    """ä¸»å‡½æ•°"""
    # æµ‹è¯•ä¸åŒçš„Mistralæ¨¡å‹
    models = [
        "mistral-community/Mistral-7B-v0.2",
        "mistralai/Mistral-7B-Instruct-v0.2"
    ]
    
    results = {}
    
    for model_name in models:
        logger.info(f"\n{'='*80}")
        answer = test_mistral_model(model_name)
        results[model_name] = answer
        logger.info(f"{'='*80}")
    
    # æ€»ç»“ç»“æœ
    logger.info("\nğŸ“Š å¯¹æ¯”ç»“æœæ€»ç»“:")
    for model_name, answer in results.items():
        if "4" in answer or "four" in answer.lower():
            status = "âœ… æ­£ç¡®ç­”æ¡ˆ"
        elif answer.strip():
            status = "âš ï¸ æœ‰ç­”æ¡ˆä½†ä¸æ­£ç¡®"
        else:
            status = "âŒ æ— ç­”æ¡ˆ"
        logger.info(f"{model_name}: {status} - '{answer}'")

if __name__ == "__main__":
    main() 