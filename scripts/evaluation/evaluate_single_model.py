#!/usr/bin/env python3
"""
å•ä¸ªæ¨¡å‹è¯„ä¼°è„šæœ¬

ä½¿ç”¨æ–¹æ³•ï¼š
python scripts/evaluate_single_model.py --model mistral-7b --dataset deepmath_evaluation_dataset --max-samples 100
"""

import os
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
import time
from datetime import datetime
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from scripts.model_evaluation import ModelEvaluator
from scripts.results_analysis import ResultsAnalyzer
from scripts.utils import ConfigLoader, setup_logging

class SingleModelEvaluator:
    """å•ä¸ªæ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.config = ConfigLoader.load_config(config_path)
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ï¼ˆæ›´æ–°ä¸ºå®é™…è¦æµ‹è¯•çš„æ¨¡å‹ï¼‰
        self.supported_models = [
            "mistral-community/Mistral-7B-v0.2",
            "lmsys/longchat-7b-16k", 
            "Yukang/LongAlpaca-13B-16k",
            "Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-hf",
            "Yukang/LongAlpaca-70B-16k"
        ]
        
        # æ¨¡å‹GPUé…ç½®
        self.model_gpu_config = {
            "mistral-community/Mistral-7B-v0.2": 1,
            "lmsys/longchat-7b-16k": 1,
            "Yukang/LongAlpaca-13B-16k": 2,
            "Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-hf": 4,
            "Yukang/LongAlpaca-70B-16k": 4
        }
    
    def evaluate_model(self, model_name: str, dataset_name: str, 
                      quantization: str = "4bit", max_samples: Optional[int] = None,
                      num_gpus: Optional[int] = None):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        self.logger.info(f"å¼€å§‹è¯„ä¼°æ¨¡å‹: {model_name}")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒ
        if model_name not in self.supported_models:
            self.logger.error(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
            return None
        
        # ç¡®å®šGPUæ•°é‡
        if num_gpus is None:
            num_gpus = self.model_gpu_config.get(model_name, 1)
        
        self.logger.info(f"ä½¿ç”¨GPUæ•°é‡: {num_gpus}")
        
        try:
            # åˆ›å»ºè¯„ä¼°å™¨
            evaluator = ModelEvaluator()
            
            # åŠ è½½æ¨¡å‹ï¼ˆæ”¯æŒå¤šGPUï¼‰
            self.logger.info(f"åŠ è½½æ¨¡å‹: {model_name} (GPUæ•°é‡: {num_gpus})")
            evaluator.load_model(model_name, quantization, num_gpus=num_gpus)
            
            # è¯„ä¼°æ•°æ®é›†
            self.logger.info(f"è¯„ä¼°æ•°æ®é›†: {dataset_name}")
            results = evaluator.evaluate_dataset(dataset_name, max_samples)
            
            if not results:
                self.logger.error("è¯„ä¼°ç»“æœä¸ºç©º")
                return None
            
            # ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            summary = evaluator.save_results(results, model_name, dataset_name)
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(results)
            
            self.logger.info(f"æ¨¡å‹ {model_name} è¯„ä¼°å®Œæˆï¼Œå…± {len(results)} ä¸ªæ ·æœ¬")
            
            # æ‰“å°æ‘˜è¦
            if 'openai_score' in df.columns:
                avg_openai_score = df['openai_score'].mean()
                self.logger.info(f"æ¨¡å‹ {model_name} å¹³å‡OpenAIè¯„åˆ†: {avg_openai_score:.2f}")
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            self.generate_analysis_report(df, model_name, dataset_name)
            
            return {
                'model_name': model_name,
                'dataset_name': dataset_name,
                'total_samples': len(results),
                'avg_openai_score': df['openai_score'].mean() if 'openai_score' in df.columns else 0,
                'avg_accuracy': df['accuracy'].mean() if 'accuracy' in df.columns else 0,
                'num_gpus': num_gpus,
                'timestamp': timestamp,
                'results_file': f"{model_name.replace('/', '_')}_{dataset_name}_{timestamp}.csv"
            }
            
        except Exception as e:
            self.logger.error(f"è¯„ä¼°æ¨¡å‹ {model_name} å¤±è´¥: {e}")
            return None
        finally:
            # æ¸…ç†å†…å­˜
            try:
                del evaluator
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            except:
                pass
    
    def generate_analysis_report(self, df: pd.DataFrame, model_name: str, dataset_name: str):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        try:
            # åˆ›å»ºåˆ†æå™¨
            analyzer = ResultsAnalyzer()
            
            # ç”Ÿæˆéš¾åº¦åˆ†æå›¾
            analyzer.analyze_accuracy_by_difficulty(df, model_name)
            
            # ç”Ÿæˆäº¤äº’å¼å›¾è¡¨
            analyzer.create_interactive_plots(df, model_name)
            
            # ç”Ÿæˆé”™è¯¯æ¨¡å¼åˆ†æ
            analyzer.analyze_error_patterns(df, model_name)
            
            self.logger.info(f"åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: results/plots/{model_name.replace('/', '_')}_*.png")
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆåˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
    
    def print_summary(self, result: Dict):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        if not result:
            return
        
        print("\n" + "="*60)
        print(f"ğŸ‰ æ¨¡å‹è¯„ä¼°å®Œæˆï¼")
        print("="*60)
        print(f"æ¨¡å‹: {result['model_name']}")
        print(f"æ•°æ®é›†: {result['dataset_name']}")
        print(f"æ ·æœ¬æ•°: {result['total_samples']}")
        print(f"GPUæ•°é‡: {result['num_gpus']}")
        print(f"å¹³å‡OpenAIè¯„åˆ†: {result['avg_openai_score']:.2f}")
        print(f"å¹³å‡å‡†ç¡®ç‡: {result['avg_accuracy']:.4f}")
        print(f"è¯„ä¼°æ—¶é—´: {result['timestamp']}")
        print(f"ç»“æœæ–‡ä»¶: {result['results_file']}")
        print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å•ä¸ªæ¨¡å‹è¯„ä¼°è„šæœ¬")
    parser.add_argument("--model", type=str, required=True,
                       choices=[
                           "mistral-community/Mistral-7B-v0.2",
                           "lmsys/longchat-7b-16k", 
                           "Yukang/LongAlpaca-13B-16k",
                           "Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-hf",
                           "Yukang/LongAlpaca-70B-16k"
                       ],
                       help="è¦è¯„ä¼°çš„æ¨¡å‹")
    parser.add_argument("--dataset", type=str, default="deepmath_evaluation_dataset",
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--quantization", type=str, default="4bit",
                       choices=["none", "4bit", "8bit"],
                       help="é‡åŒ–æ–¹å¼")
    parser.add_argument("--max-samples", type=int, default=None,
                       help="æœ€å¤§æ ·æœ¬æ•°é‡")
    parser.add_argument("--num-gpus", type=int, default=None,
                       help="GPUæ•°é‡ï¼ˆé»˜è®¤æ ¹æ®æ¨¡å‹è‡ªåŠ¨è®¾ç½®ï¼‰")
    parser.add_argument("--config", type=str, default="configs/config.yaml",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = SingleModelEvaluator(args.config)
    
    try:
        # è¯„ä¼°æ¨¡å‹
        result = evaluator.evaluate_model(
            model_name=args.model,
            dataset_name=args.dataset,
            quantization=args.quantization,
            max_samples=args.max_samples,
            num_gpus=args.num_gpus
        )
        
        # æ‰“å°æ‘˜è¦
        evaluator.print_summary(result)
        
        if result:
            print(f"\nâœ… è¯„ä¼°æˆåŠŸï¼")
            print(f"ğŸ“ ç»“æœæ–‡ä»¶: results/{result['results_file']}")
            print(f"ğŸ“ˆ åˆ†æå›¾è¡¨: results/plots/{args.model.replace('/', '_')}_*.png")
        else:
            print(f"\nâŒ è¯„ä¼°å¤±è´¥ï¼")
            return 1
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 