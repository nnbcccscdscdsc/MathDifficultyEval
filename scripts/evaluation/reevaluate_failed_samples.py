#!/usr/bin/env python3
"""
ÈáçÊñ∞ËØÑ‰º∞Â§±Ë¥•Ê†∑Êú¨ËÑöÊú¨
‰∏ìÈó®Áî®‰∫éÈáçÊñ∞ËØÑ‰º∞Âõ†ÁΩëÁªúÈóÆÈ¢òÁ≠âÂØºËá¥ÁöÑËØÑ‰º∞Â§±Ë¥•Ê†∑Êú¨
"""

import json
import os
import glob
import logging
import openai
import time
from typing import List, Dict, Any
from tqdm import tqdm

# ËÆæÁΩÆÊó•Âøó
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FailedSampleReevaluator:
    def __init__(self, openai_api_key: str = None):
        """ÂàùÂßãÂåñÈáçÊñ∞ËØÑ‰º∞Âô®"""
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            raise ValueError("‚ùå Êú™ËÆæÁΩÆOPENAI_API_KEYÁéØÂ¢ÉÂèòÈáè")
        
        # ËÆæÁΩÆOpenAIÂÆ¢Êà∑Á´Ø
        openai.api_key = self.openai_api_key
        self.openai_client = openai
        
        logger.info(f"üîë OpenAI API Key: {self.openai_api_key[:10]}...{self.openai_api_key[-4:]}")
    
    def find_intermediate_files(self, model_name: str, run_id: str = None) -> List[str]:
        """Êü•ÊâæÊåáÂÆöÊ®°ÂûãÁöÑ‰∏≠Èó¥ÁªìÊûúÊñá‰ª∂"""
        model_safe_name = model_name.replace('/', '_').replace('-', '_')
        base_path = f"data/intermediate/{model_safe_name}"
        
        if run_id:
            # ÊåáÂÆöËøêË°åID
            run_path = f"{base_path}/{run_id}"
            if os.path.exists(run_path):
                pattern = os.path.join(run_path, "intermediate_results_*.json")
                files = glob.glob(pattern)
                logger.info(f"üìÅ ÊâæÂà∞ÊåáÂÆöËøêË°åIDÁöÑÊñá‰ª∂: {len(files)} ‰∏™")
                return sorted(files)
            else:
                logger.error(f"‚ùå Êú™ÊâæÂà∞ËøêË°åID: {run_id}")
                return []
        else:
            # Êü•ÊâæÊúÄÊñ∞ÁöÑËøêË°åID
            if not os.path.exists(base_path):
                logger.error(f"‚ùå Êú™ÊâæÂà∞Ê®°ÂûãÁõÆÂΩï: {base_path}")
                return []
            
            # Ëé∑ÂèñÊâÄÊúâËøêË°åIDÁõÆÂΩï
            run_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]
            if not run_dirs:
                logger.error(f"‚ùå Êú™ÊâæÂà∞‰ªª‰ΩïËøêË°åÁõÆÂΩï")
                return []
            
            # ÈÄâÊã©ÊúÄÊñ∞ÁöÑËøêË°åID
            latest_run = sorted(run_dirs)[-1]
            logger.info(f"üìÅ ‰ΩøÁî®ÊúÄÊñ∞ËøêË°åID: {latest_run}")
            
            run_path = f"{base_path}/{latest_run}"
            pattern = os.path.join(run_path, "intermediate_results_*.json")
            files = glob.glob(pattern)
            logger.info(f"üìÅ ÊâæÂà∞Êñá‰ª∂: {len(files)} ‰∏™")
            return sorted(files)
    
    def load_all_results(self, files: List[str]) -> List[Dict[str, Any]]:
        """Âä†ËΩΩÊâÄÊúâ‰∏≠Èó¥ÁªìÊûúÊñá‰ª∂"""
        if not files:
            return []
        
        # Âè™Âä†ËΩΩÊúÄÊñ∞ÁöÑÊñá‰ª∂ÔºåÈÅøÂÖçÈáçÂ§çÂä†ËΩΩ
        latest_file = files[-1]
        
        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logger.info(f"‚úÖ Âä†ËΩΩÊúÄÊñ∞Êñá‰ª∂: {os.path.basename(latest_file)} ({len(data)} ‰∏™Ê†∑Êú¨)")
                return data
        except Exception as e:
            logger.error(f"‚ùå Âä†ËΩΩÊñá‰ª∂Â§±Ë¥• {latest_file}: {e}")
            return []
    
    def identify_failed_samples(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ËØÜÂà´ËØÑ‰º∞Â§±Ë¥•ÁöÑÊ†∑Êú¨"""
        failed_samples = []
        
        for i, result in enumerate(results):
            evaluation = result.get('evaluation', {})
            
            # Ê£ÄÊü•ÊòØÂê¶ÊúâÈîôËØØ‰ø°ÊÅØ
            if isinstance(evaluation, dict):
                if 'error' in evaluation:
                    failed_samples.append({
                        'index': i,
                        'sample_id': result.get('id', f'unknown_{i}'),
                        'result': result,
                        'error': evaluation.get('error', 'unknown_error')
                    })
                    logger.info(f"‚ùå ÂèëÁé∞Â§±Ë¥•Ê†∑Êú¨ {i}: {result.get('id', 'unknown')} - {evaluation.get('error', 'unknown_error')}")
                elif evaluation.get('overall_score', 0) == 0 and 'comments' in evaluation:
                    # Ê£ÄÊü•ÊòØÂê¶ÊòØÁîüÊàêÂ§±Ë¥•ÊàñËØÑ‰º∞Â§±Ë¥•
                    comments = evaluation.get('comments', '')
                    if 'ÁîüÊàêÂ§±Ë¥•' in comments or 'ËØÑ‰º∞Â§±Ë¥•' in comments:
                        failed_samples.append({
                            'index': i,
                            'sample_id': result.get('id', f'unknown_{i}'),
                            'result': result,
                            'error': comments
                        })
                        logger.info(f"‚ùå ÂèëÁé∞Â§±Ë¥•Ê†∑Êú¨ {i}: {result.get('id', 'unknown')} - {comments}")
        
        logger.info(f"üìä ÊÄªÂÖ±ÂèëÁé∞ {len(failed_samples)} ‰∏™Â§±Ë¥•Ê†∑Êú¨")
        return failed_samples
    
    def reevaluate_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """ÈáçÊñ∞ËØÑ‰º∞Âçï‰∏™Ê†∑Êú¨"""
        try:
            # ‰ΩøÁî®OpenAIËØÑ‰º∞
            evaluation = self.evaluate_with_openai(
                sample['problem'],
                sample['model_response'],
                sample['correct_answer'],
                sample.get('standard_solution', '')
            )
            
            if isinstance(evaluation, dict) and 'error' not in evaluation:
                logger.info(f"‚úÖ ÈáçÊñ∞ËØÑ‰º∞ÊàêÂäü: {sample.get('id', 'unknown')} - ÊÄªÂàÜ: {evaluation.get('overall_score', 0):.2f}")
                return evaluation
            else:
                logger.warning(f"‚ö†Ô∏è ÈáçÊñ∞ËØÑ‰º∞Â§±Ë¥•: {sample.get('id', 'unknown')} - {evaluation}")
                return evaluation
                
        except Exception as e:
            logger.error(f"‚ùå ÈáçÊñ∞ËØÑ‰º∞ÂºÇÂ∏∏: {sample.get('id', 'unknown')} - {e}")
            return {
                "error": f"Reevaluation failed: {e}",
                "error_type": "reevaluation_exception"
            }
    
    def evaluate_with_openai(self, problem: str, model_response: str, correct_answer: str, standard_solution: str = "") -> Dict[str, Any]:
        """‰ΩøÁî®OpenAIËØÑ‰º∞Ê®°ÂûãÂõûÁ≠î"""
        try:
            # ÊûÑÂª∫ËØÑ‰º∞ÊèêÁ§∫
            if standard_solution:
                evaluation_prompt = f"""
Please evaluate the quality of the answer to the following mathematical problem. You have access to the standard solution for reference.

Problem: {problem}

Correct Answer: {correct_answer}

Standard Solution: {standard_solution}

Model Response: {model_response}

Please evaluate from the following aspects and give a score from 1 to 10:

1. Answer Correctness (1-10 points): Whether the final answer is correct
2. Reasoning Logic (1-10 points): Whether the reasoning process is clear and logical, compared to the standard solution
3. Step Completeness (1-10 points): Whether all necessary solution steps are shown, considering what the standard solution covers
4. Mathematical Accuracy (1-10 points): Whether mathematical calculations and formulas are accurate
5. Expression Clarity (1-10 points): Whether the expression is clear and easy to understand

IMPORTANT: You must respond with ONLY a valid JSON object. Do not include any other text, explanations, or markdown formatting.

CRITICAL: In the "comments" field, avoid using backslashes (\) or special characters that could break JSON parsing. Use simple text only.

Please return the evaluation result in JSON format:
{{
    "answer_correctness": score,
    "reasoning_logic": score,
    "step_completeness": score,
    "mathematical_accuracy": score,
    "expression_clarity": score,
    "overall_score": total_score/5,
    "comments": "Detailed evaluation with reference to standard solution"
}}
"""
            else:
                evaluation_prompt = f"""
Please evaluate the quality of the answer to the following mathematical problem.

Problem: {problem}

Correct Answer: {correct_answer}

Model Response: {model_response}

Please evaluate from the following aspects and give a score from 1 to 10:

1. Answer Correctness (1-10 points): Whether the final answer is correct
2. Reasoning Logic (1-10 points): Whether the reasoning process is clear and logical
3. Step Completeness (1-10 points): Whether all solution steps are shown
4. Mathematical Accuracy (1-10 points): Whether mathematical calculations and formulas are accurate
5. Expression Clarity (1-10 points): Whether the expression is clear and easy to understand

IMPORTANT: You must respond with ONLY a valid JSON object. Do not include any other text, explanations, or markdown formatting.

CRITICAL: In the "comments" field, avoid using backslashes (\) or special characters that could break JSON parsing. Use simple text only.

Please return the evaluation result in JSON format:
{{
    "answer_correctness": score,
    "reasoning_logic": score,
    "step_completeness": score,
    "mathematical_accuracy": score,
    "expression_clarity": score,
    "overall_score": total_score/5,
    "comments": "Detailed evaluation"
}}
"""
            
            response = self.openai_client.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a professional mathematical education evaluator."},
                    {"role": "user", "content": evaluation_prompt}
                ],
                temperature=0.3
            )
            
            response_content = response.choices[0].message.content
            
            # Ëß£ÊûêJSONÂìçÂ∫î
            import re
            try:
                evaluation = json.loads(response_content)
                return evaluation
            except json.JSONDecodeError as e:
                logger.warning(f"‚ö†Ô∏è JSONËß£ÊûêÂ§±Ë¥•: {e}")
                
                # Â∞ùËØïÊèêÂèñJSONÈÉ®ÂàÜ
                try:
                    json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group()
                        evaluation = json.loads(json_str)
                        logger.info(f"‚úÖ ÊàêÂäüÊèêÂèñJSONÈÉ®ÂàÜ")
                        return evaluation
                except:
                    pass
                
                # Â∞ùËØï‰øÆÂ§çÂ∏∏ËßÅÁöÑJSONÊ†ºÂºèÈóÆÈ¢ò
                try:
                    cleaned_response = response_content.replace('```json', '').replace('```', '').strip()
                    evaluation = json.loads(cleaned_response)
                    logger.info(f"‚úÖ ÊàêÂäü‰øÆÂ§çJSONÊ†ºÂºè")
                    return evaluation
                except:
                    pass
                
                return {
                    "raw_response": response_content,
                    "error": "JSON parsing failed",
                    "parse_error": str(e),
                    "error_type": "json_decode_error"
                }
                
        except Exception as e:
            logger.error(f"‚ùå OpenAI evaluation failed: {e}")
            return {
                "error": f"Evaluation failed: {e}",
                "error_type": "openai_api_error",
                "exception": str(e)
            }
    
    def update_results(self, all_results: List[Dict[str, Any]], failed_samples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Êõ¥Êñ∞ÁªìÊûúÂàóË°®ÔºåÊõøÊç¢Â§±Ë¥•ÁöÑËØÑ‰º∞"""
        updated_results = all_results.copy()
        
        for failed_info in failed_samples:
            index = failed_info['index']
            original_result = failed_info['result']
            
            # ÈáçÊñ∞ËØÑ‰º∞
            new_evaluation = self.reevaluate_sample(original_result)
            
            # Êõ¥Êñ∞ÁªìÊûú
            updated_results[index]['evaluation'] = new_evaluation
            
            # Ê∑ªÂä†ÈáçÊñ∞ËØÑ‰º∞Ê†áËÆ∞
            updated_results[index]['reevaluated'] = True
            updated_results[index]['reevaluation_time'] = time.strftime("%Y-%m-%d %H:%M:%S")
            
            logger.info(f"üîÑ Â∑≤Êõ¥Êñ∞Ê†∑Êú¨ {index}: {original_result.get('id', 'unknown')}")
        
        return updated_results
    
    def save_updated_results(self, updated_results: List[Dict[str, Any]], model_name: str, run_id: str = None) -> str:
        """‰øùÂ≠òÊõ¥Êñ∞ÂêéÁöÑÁªìÊûú"""
        model_safe_name = model_name.replace('/', '_').replace('-', '_')
        base_path = f"data/intermediate/{model_safe_name}"
        
        if not run_id:
            # ‰ΩøÁî®ÊúÄÊñ∞ÁöÑËøêË°åID
            run_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]
            run_id = sorted(run_dirs)[-1]
        
        # ÂàõÂª∫Â§á‰ªΩÁõÆÂΩï
        backup_dir = f"{base_path}/{run_id}_backup_{int(time.time())}"
        os.makedirs(backup_dir, exist_ok=True)
        
        # Â§á‰ªΩÂéüÂßãÊñá‰ª∂
        original_files = glob.glob(f"{base_path}/{run_id}/intermediate_results_*.json")
        for file_path in original_files:
            backup_path = os.path.join(backup_dir, os.path.basename(file_path))
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            with open(backup_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üíæ ÂéüÂßãÊñá‰ª∂Â∑≤Â§á‰ªΩÂà∞: {backup_dir}")
        
        # ÈáçÊñ∞ÁªÑÁªáÁªìÊûúÂà∞Êñá‰ª∂
        samples_per_file = 10
        file_count = 0
        
        for i in range(0, len(updated_results), samples_per_file):
            file_count += 1
            batch_results = updated_results[i:i + samples_per_file]
            
            # ‰øùÂ≠òÂà∞ÂéüÊñá‰ª∂‰ΩçÁΩÆ
            file_path = f"{base_path}/{run_id}/intermediate_results_{i + len(batch_results)}.json"
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(batch_results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"üíæ ‰øùÂ≠òÊõ¥Êñ∞Êñá‰ª∂: {os.path.basename(file_path)} ({len(batch_results)} ‰∏™Ê†∑Êú¨)")
        
        return backup_dir
    
    def run_reevaluation(self, model_name: str, run_id: str = None):
        """ËøêË°åÂÆåÊï¥ÁöÑÈáçÊñ∞ËØÑ‰º∞ÊµÅÁ®ã"""
        logger.info("üöÄ ÂºÄÂßãÈáçÊñ∞ËØÑ‰º∞Â§±Ë¥•Ê†∑Êú¨")
        logger.info(f"ü§ñ Ê®°Âûã: {model_name}")
        if run_id:
            logger.info(f"üÜî ËøêË°åID: {run_id}")
        
        try:
            # 1. Êü•Êâæ‰∏≠Èó¥ÁªìÊûúÊñá‰ª∂
            files = self.find_intermediate_files(model_name, run_id)
            if not files:
                logger.error("‚ùå Êú™ÊâæÂà∞‰∏≠Èó¥ÁªìÊûúÊñá‰ª∂")
                return
            
            # 2. Âä†ËΩΩÊâÄÊúâÁªìÊûú
            all_results = self.load_all_results(files)
            if not all_results:
                logger.error("‚ùå Êú™Âä†ËΩΩÂà∞‰ªª‰ΩïÁªìÊûú")
                return
            
            # 3. ËØÜÂà´Â§±Ë¥•Ê†∑Êú¨
            failed_samples = self.identify_failed_samples(all_results)
            if not failed_samples:
                logger.info("‚úÖ Ê≤°ÊúâÂèëÁé∞Â§±Ë¥•ÁöÑÊ†∑Êú¨")
                return
            
            # 4. ÈáçÊñ∞ËØÑ‰º∞Â§±Ë¥•Ê†∑Êú¨
            logger.info(f"üîÑ ÂºÄÂßãÈáçÊñ∞ËØÑ‰º∞ {len(failed_samples)} ‰∏™Â§±Ë¥•Ê†∑Êú¨...")
            for failed_info in tqdm(failed_samples, desc="ÈáçÊñ∞ËØÑ‰º∞ËøõÂ∫¶"):
                # Ê∑ªÂä†Âª∂ËøüÈÅøÂÖçAPIÈôêÂà∂
                time.sleep(1)
            
            # 5. Êõ¥Êñ∞ÁªìÊûú
            updated_results = self.update_results(all_results, failed_samples)
            
            # 6. ‰øùÂ≠òÊõ¥Êñ∞ÂêéÁöÑÁªìÊûú
            backup_dir = self.save_updated_results(updated_results, model_name, run_id)
            
            logger.info("üéâ ÈáçÊñ∞ËØÑ‰º∞ÂÆåÊàêÔºÅ")
            logger.info(f"üìä Â§ÑÁêÜ‰∫Ü {len(failed_samples)} ‰∏™Â§±Ë¥•Ê†∑Êú¨")
            logger.info(f"üíæ ÂéüÂßãÊñá‰ª∂Â§á‰ªΩÂà∞: {backup_dir}")
            
        except Exception as e:
            logger.error(f"‚ùå ÈáçÊñ∞ËØÑ‰º∞Â§±Ë¥•: {e}")
            import traceback
            traceback.print_exc()

def main():
    """‰∏ªÂáΩÊï∞"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ÈáçÊñ∞ËØÑ‰º∞Â§±Ë¥•Ê†∑Êú¨")
    parser.add_argument("--model", type=str, required=True, 
                       help="Ê®°ÂûãÂêçÁß∞ (‰æãÂ¶Ç: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)")
    parser.add_argument("--run-id", type=str, 
                       help="ËøêË°åID (ÂèØÈÄâÔºåÈªòËÆ§‰ΩøÁî®ÊúÄÊñ∞ÁöÑ)")
    parser.add_argument("--openai-key", type=str, 
                       help="OpenAI API Key (ÂèØÈÄâÔºåÈªòËÆ§‰ΩøÁî®ÁéØÂ¢ÉÂèòÈáè)")
    
    args = parser.parse_args()
    
    try:
        # ÂàõÂª∫ÈáçÊñ∞ËØÑ‰º∞Âô®
        reevaluator = FailedSampleReevaluator(args.openai_key)
        
        # ËøêË°åÈáçÊñ∞ËØÑ‰º∞
        reevaluator.run_reevaluation(args.model, args.run_id)
        
    except Exception as e:
        logger.error(f"‚ùå Á®ãÂ∫èÊâßË°åÂ§±Ë¥•: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 