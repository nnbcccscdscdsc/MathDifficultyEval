#!/usr/bin/env python3
"""
é‡æ–°è¯„ä¼°å¤±è´¥æ ·æœ¬è„šæœ¬
ä¸“é—¨ç”¨äºé‡æ–°è¯„ä¼°å› ç½‘ç»œé—®é¢˜ç­‰å¯¼è‡´çš„è¯„ä¼°å¤±è´¥æ ·æœ¬
"""

import json
import os
import glob
import logging
import openai
import time
from typing import List, Dict, Any
from tqdm import tqdm

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FailedSampleReevaluator:
    def __init__(self, openai_api_key: str = None):
        """åˆå§‹åŒ–é‡æ–°è¯„ä¼°å™¨"""
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            raise ValueError("âŒ æœªè®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        
        # è®¾ç½®OpenAIå®¢æˆ·ç«¯
        openai.api_key = self.openai_api_key
        self.openai_client = openai
        
        logger.info(f"ğŸ”‘ OpenAI API Key: {self.openai_api_key[:10]}...{self.openai_api_key[-4:]}")
    
    def find_intermediate_files(self, model_name: str, run_id: str = None) -> List[str]:
        """æŸ¥æ‰¾æŒ‡å®šæ¨¡å‹çš„ä¸­é—´ç»“æœæ–‡ä»¶"""
        model_safe_name = model_name.replace('/', '_').replace('-', '_')
        base_path = f"data/intermediate/{model_safe_name}"
        
        if run_id:
            # æŒ‡å®šè¿è¡ŒID
            run_path = f"{base_path}/{run_id}"
            if os.path.exists(run_path):
                pattern = os.path.join(run_path, "intermediate_results_*.json")
                files = glob.glob(pattern)
                logger.info(f"ğŸ“ æ‰¾åˆ°æŒ‡å®šè¿è¡ŒIDçš„æ–‡ä»¶: {len(files)} ä¸ª")
                return sorted(files)
            else:
                logger.error(f"âŒ æœªæ‰¾åˆ°è¿è¡ŒID: {run_id}")
                return []
        else:
            # æŸ¥æ‰¾æœ€æ–°çš„è¿è¡ŒID
            if not os.path.exists(base_path):
                logger.error(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹ç›®å½•: {base_path}")
                return []
            
            # è·å–æ‰€æœ‰è¿è¡ŒIDç›®å½•
            run_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]
            if not run_dirs:
                logger.error(f"âŒ æœªæ‰¾åˆ°ä»»ä½•è¿è¡Œç›®å½•")
                return []
            
            # é€‰æ‹©æœ€æ–°çš„è¿è¡ŒID
            latest_run = sorted(run_dirs)[-1]
            logger.info(f"ğŸ“ ä½¿ç”¨æœ€æ–°è¿è¡ŒID: {latest_run}")
            
            run_path = f"{base_path}/{latest_run}"
            pattern = os.path.join(run_path, "intermediate_results_*.json")
            files = glob.glob(pattern)
            logger.info(f"ğŸ“ æ‰¾åˆ°æ–‡ä»¶: {len(files)} ä¸ª")
            return sorted(files)
    
    def load_all_results(self, files: List[str]) -> List[Dict[str, Any]]:
        """åŠ è½½æ‰€æœ‰ä¸­é—´ç»“æœæ–‡ä»¶"""
        if not files:
            return []
        
        # åªåŠ è½½æœ€æ–°çš„æ–‡ä»¶ï¼Œé¿å…é‡å¤åŠ è½½
        latest_file = files[-1]
        
        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logger.info(f"âœ… åŠ è½½æœ€æ–°æ–‡ä»¶: {os.path.basename(latest_file)} ({len(data)} ä¸ªæ ·æœ¬)")
                return data
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {latest_file}: {e}")
            return []
    
    def identify_failed_samples(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¯†åˆ«è¯„ä¼°å¤±è´¥çš„æ ·æœ¬"""
        failed_samples = []
        
        for i, result in enumerate(results):
            evaluation = result.get('evaluation', {})
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯
            if isinstance(evaluation, dict):
                if 'error' in evaluation:
                    failed_samples.append({
                        'index': i,
                        'sample_id': result.get('id', f'unknown_{i}'),
                        'result': result,
                        'error': evaluation.get('error', 'unknown_error')
                    })
                    logger.info(f"âŒ å‘ç°å¤±è´¥æ ·æœ¬ {i}: {result.get('id', 'unknown')} - {evaluation.get('error', 'unknown_error')}")
                elif evaluation.get('overall_score', 0) == 0 and 'comments' in evaluation:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç”Ÿæˆå¤±è´¥æˆ–è¯„ä¼°å¤±è´¥
                    comments = evaluation.get('comments', '')
                    if 'ç”Ÿæˆå¤±è´¥' in comments or 'è¯„ä¼°å¤±è´¥' in comments:
                        failed_samples.append({
                            'index': i,
                            'sample_id': result.get('id', f'unknown_{i}'),
                            'result': result,
                            'error': comments
                        })
                        logger.info(f"âŒ å‘ç°å¤±è´¥æ ·æœ¬ {i}: {result.get('id', 'unknown')} - {comments}")
        
        logger.info(f"ğŸ“Š æ€»å…±å‘ç° {len(failed_samples)} ä¸ªå¤±è´¥æ ·æœ¬")
        return failed_samples
    
    def reevaluate_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """é‡æ–°è¯„ä¼°å•ä¸ªæ ·æœ¬"""
        try:
            # ä½¿ç”¨OpenAIè¯„ä¼°
            evaluation = self.evaluate_with_openai(
                sample['problem'],
                sample['model_response'],
                sample['correct_answer'],
                sample.get('standard_solution', '')
            )
            
            if isinstance(evaluation, dict) and 'error' not in evaluation:
                logger.info(f"âœ… é‡æ–°è¯„ä¼°æˆåŠŸ: {sample.get('id', 'unknown')} - æ€»åˆ†: {evaluation.get('overall_score', 0):.2f}")
                return evaluation
            else:
                logger.warning(f"âš ï¸ é‡æ–°è¯„ä¼°å¤±è´¥: {sample.get('id', 'unknown')} - {evaluation}")
                return evaluation
                
        except Exception as e:
            logger.error(f"âŒ é‡æ–°è¯„ä¼°å¼‚å¸¸: {sample.get('id', 'unknown')} - {e}")
            return {
                "error": f"Reevaluation failed: {e}",
                "error_type": "reevaluation_exception"
            }
    
    def evaluate_with_openai(self, problem: str, model_response: str, correct_answer: str, standard_solution: str = "") -> Dict[str, Any]:
        """ä½¿ç”¨OpenAIè¯„ä¼°æ¨¡å‹å›ç­”"""
        try:
            # æ„å»ºè¯„ä¼°æç¤º
            if standard_solution:
                evaluation_prompt = f"""
Please evaluate the quality of the answer to the following mathematical problem. You have access to the standard solution for reference.

Problem: {problem}

Correct Answer: {correct_answer}

Standard Solution: {standard_solution}

Model Response: {model_response}

Please evaluate from the following aspects and give a score from 1 to 10:

1. Answer Correctness (1-10 points): Whether the final answer is correct
2. Reasoning Logic (1-10 points): Whether the reasoning process is clear and logical, compared to the standard solution
3. Step Completeness (1-10 points): Whether all necessary solution steps are shown, considering what the standard solution covers
4. Mathematical Accuracy (1-10 points): Whether mathematical calculations and formulas are accurate
5. Expression Clarity (1-10 points): Whether the expression is clear and easy to understand

IMPORTANT: You must respond with ONLY a valid JSON object. Do not include any other text, explanations, or markdown formatting.

CRITICAL: In the "comments" field, avoid using backslashes (\) or special characters that could break JSON parsing. Use simple text only.

Please return the evaluation result in JSON format:
{{
    "answer_correctness": score,
    "reasoning_logic": score,
    "step_completeness": score,
    "mathematical_accuracy": score,
    "expression_clarity": score,
    "overall_score": total_score/5,
    "comments": "Detailed evaluation with reference to standard solution"
}}
"""
            else:
                evaluation_prompt = f"""
Please evaluate the quality of the answer to the following mathematical problem.

Problem: {problem}

Correct Answer: {correct_answer}

Model Response: {model_response}

Please evaluate from the following aspects and give a score from 1 to 10:

1. Answer Correctness (1-10 points): Whether the final answer is correct
2. Reasoning Logic (1-10 points): Whether the reasoning process is clear and logical
3. Step Completeness (1-10 points): Whether all solution steps are shown
4. Mathematical Accuracy (1-10 points): Whether mathematical calculations and formulas are accurate
5. Expression Clarity (1-10 points): Whether the expression is clear and easy to understand

IMPORTANT: You must respond with ONLY a valid JSON object. Do not include any other text, explanations, or markdown formatting.

CRITICAL: In the "comments" field, avoid using backslashes (\) or special characters that could break JSON parsing. Use simple text only.

Please return the evaluation result in JSON format:
{{
    "answer_correctness": score,
    "reasoning_logic": score,
    "step_completeness": score,
    "mathematical_accuracy": score,
    "expression_clarity": score,
    "overall_score": total_score/5,
    "comments": "Detailed evaluation"
}}
"""
            
            response = self.openai_client.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a professional mathematical education evaluator."},
                    {"role": "user", "content": evaluation_prompt}
                ],
                temperature=0.3
            )
            
            response_content = response.choices[0].message.content
            
            # è§£æJSONå“åº”
            import re
            try:
                evaluation = json.loads(response_content)
                return evaluation
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
                
                # å°è¯•æå–JSONéƒ¨åˆ†
                try:
                    json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group()
                        evaluation = json.loads(json_str)
                        logger.info(f"âœ… æˆåŠŸæå–JSONéƒ¨åˆ†")
                        return evaluation
                except:
                    pass
                
                # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                try:
                    cleaned_response = response_content.replace('```json', '').replace('```', '').strip()
                    evaluation = json.loads(cleaned_response)
                    logger.info(f"âœ… æˆåŠŸä¿®å¤JSONæ ¼å¼")
                    return evaluation
                except:
                    pass
                
                return {
                    "raw_response": response_content,
                    "error": "JSON parsing failed",
                    "parse_error": str(e),
                    "error_type": "json_decode_error"
                }
                
        except Exception as e:
            logger.error(f"âŒ OpenAI evaluation failed: {e}")
            return {
                "error": f"Evaluation failed: {e}",
                "error_type": "openai_api_error",
                "exception": str(e)
            }
    
    def update_results(self, all_results: List[Dict[str, Any]], failed_samples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ›´æ–°ç»“æœåˆ—è¡¨ï¼Œæ›¿æ¢å¤±è´¥çš„è¯„ä¼°"""
        updated_results = all_results.copy()
        
        for failed_info in failed_samples:
            index = failed_info['index']
            original_result = failed_info['result']
            
            # é‡æ–°è¯„ä¼°
            new_evaluation = self.reevaluate_sample(original_result)
            
            # æ›´æ–°ç»“æœ
            updated_results[index]['evaluation'] = new_evaluation
            
            # æ·»åŠ é‡æ–°è¯„ä¼°æ ‡è®°
            updated_results[index]['reevaluated'] = True
            updated_results[index]['reevaluation_time'] = time.strftime("%Y-%m-%d %H:%M:%S")
            
            logger.info(f"ğŸ”„ å·²æ›´æ–°æ ·æœ¬ {index}: {original_result.get('id', 'unknown')}")
        
        return updated_results
    
    def save_updated_results(self, updated_results: List[Dict[str, Any]], model_name: str, run_id: str = None) -> str:
        """ä¿å­˜æ›´æ–°åçš„ç»“æœ"""
        model_safe_name = model_name.replace('/', '_').replace('-', '_')
        base_path = f"data/intermediate/{model_safe_name}"
        
        if not run_id:
            # ä½¿ç”¨æœ€æ–°çš„è¿è¡ŒID
            run_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]
            run_id = sorted(run_dirs)[-1]
        
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        backup_dir = f"{base_path}/{run_id}_backup_{int(time.time())}"
        os.makedirs(backup_dir, exist_ok=True)
        
        # å¤‡ä»½åŸå§‹æ–‡ä»¶
        original_files = glob.glob(f"{base_path}/{run_id}/intermediate_results_*.json")
        for file_path in original_files:
            backup_path = os.path.join(backup_dir, os.path.basename(file_path))
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            with open(backup_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ åŸå§‹æ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_dir}")
        
        # é‡æ–°ç»„ç»‡ç»“æœåˆ°æ–‡ä»¶
        samples_per_file = 10
        file_count = 0
        
        for i in range(0, len(updated_results), samples_per_file):
            file_count += 1
            batch_results = updated_results[i:i + samples_per_file]
            
            # ä¿å­˜åˆ°åŸæ–‡ä»¶ä½ç½®
            file_path = f"{base_path}/{run_id}/intermediate_results_{i + len(batch_results)}.json"
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(batch_results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ ä¿å­˜æ›´æ–°æ–‡ä»¶: {os.path.basename(file_path)} ({len(batch_results)} ä¸ªæ ·æœ¬)")
        
        return backup_dir
    
    def run_reevaluation(self, model_name: str, run_id: str = None):
        """è¿è¡Œå®Œæ•´çš„é‡æ–°è¯„ä¼°æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹é‡æ–°è¯„ä¼°å¤±è´¥æ ·æœ¬")
        logger.info(f"ğŸ¤– æ¨¡å‹: {model_name}")
        if run_id:
            logger.info(f"ğŸ†” è¿è¡ŒID: {run_id}")
        
        try:
            # 1. æŸ¥æ‰¾ä¸­é—´ç»“æœæ–‡ä»¶
            files = self.find_intermediate_files(model_name, run_id)
            if not files:
                logger.error("âŒ æœªæ‰¾åˆ°ä¸­é—´ç»“æœæ–‡ä»¶")
                return
            
            # 2. åŠ è½½æ‰€æœ‰ç»“æœ
            all_results = self.load_all_results(files)
            if not all_results:
                logger.error("âŒ æœªåŠ è½½åˆ°ä»»ä½•ç»“æœ")
                return
            
            # 3. è¯†åˆ«å¤±è´¥æ ·æœ¬
            failed_samples = self.identify_failed_samples(all_results)
            if not failed_samples:
                logger.info("âœ… æ²¡æœ‰å‘ç°å¤±è´¥çš„æ ·æœ¬")
                return
            
            # 4. é‡æ–°è¯„ä¼°å¤±è´¥æ ·æœ¬
            logger.info(f"ğŸ”„ å¼€å§‹é‡æ–°è¯„ä¼° {len(failed_samples)} ä¸ªå¤±è´¥æ ·æœ¬...")
            for failed_info in tqdm(failed_samples, desc="é‡æ–°è¯„ä¼°è¿›åº¦"):
                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                time.sleep(1)
            
            # 5. æ›´æ–°ç»“æœ
            updated_results = self.update_results(all_results, failed_samples)
            
            # 6. ä¿å­˜æ›´æ–°åçš„ç»“æœ
            backup_dir = self.save_updated_results(updated_results, model_name, run_id)
            
            logger.info("ğŸ‰ é‡æ–°è¯„ä¼°å®Œæˆï¼")
            logger.info(f"ğŸ“Š å¤„ç†äº† {len(failed_samples)} ä¸ªå¤±è´¥æ ·æœ¬")
            logger.info(f"ğŸ’¾ åŸå§‹æ–‡ä»¶å¤‡ä»½åˆ°: {backup_dir}")
            
        except Exception as e:
            logger.error(f"âŒ é‡æ–°è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="é‡æ–°è¯„ä¼°å¤±è´¥æ ·æœ¬")
    parser.add_argument("--model", type=str, required=True, 
                       help="æ¨¡å‹åç§° (ä¾‹å¦‚: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)")
    parser.add_argument("--run-id", type=str, 
                       help="è¿è¡ŒID (å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°çš„)")
    parser.add_argument("--openai-key", type=str, 
                       help="OpenAI API Key (å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ç¯å¢ƒå˜é‡)")
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºé‡æ–°è¯„ä¼°å™¨
        reevaluator = FailedSampleReevaluator(args.openai_key)
        
        # è¿è¡Œé‡æ–°è¯„ä¼°
        reevaluator.run_reevaluation(args.model, args.run_id)
        
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 