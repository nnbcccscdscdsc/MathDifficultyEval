#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Hendrycks Mathæ•°æ®é›†æ•°å­¦è¯„ä¼°æ¡†æ¶
ä¸“é—¨é€‚é…Hendrycks Mathæ•°æ®é›†çš„å­—æ®µç»“æ„

æ•°æ®é›†å­—æ®µï¼š
- problem: æ•°å­¦é—®é¢˜
- level: éš¾åº¦ç­‰çº§ (Level 1-5)
- type: é—®é¢˜ç±»å‹ (Algebra, Geometry, Precalculusç­‰)
- solution: æ ‡å‡†è§£ç­”
- subset: å­é›†åˆ†ç±»
"""

import os
import sys
import json
import logging
import argparse
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Union, Any
from tqdm import tqdm
import openai
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from math_evaluation_framework import MathEvaluationFramework

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('hendrycks_math_evaluation.log'),
        logging.StreamHandler()
    ]
)

class HendrycksMathEvaluationFramework(MathEvaluationFramework):
    """
    Hendrycks Mathæ•°æ®é›†ä¸“ç”¨è¯„ä¼°æ¡†æ¶
    ç»§æ‰¿è‡ªåŸºç¡€è¯„ä¼°æ¡†æ¶ï¼Œé€‚é…Hendrycks Mathæ•°æ®é›†çš„ç‰¹æ®Šå­—æ®µ
    """
    
    def __init__(self, model_name: str, dataset_path: str, **kwargs):
        """
        åˆå§‹åŒ–Hendrycks Mathè¯„ä¼°æ¡†æ¶
        
        Args:
            model_name: æ¨¡å‹åç§°
            dataset_path: Hendrycks Mathæ•°æ®é›†è·¯å¾„
            **kwargs: å…¶ä»–å‚æ•°
        """
        # æ„å»ºæ¨¡å‹é…ç½®å­—å…¸ï¼Œä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹æ˜ å°„
        model_config = self._get_model_config(model_name)
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(model_config, **kwargs)
        
        # ä¿å­˜æ•°æ®é›†è·¯å¾„
        self.dataset_path = dataset_path
        
        # è®¾ç½®loggerå±æ€§
        self.logger = logging.getLogger(__name__)
        
        # ç”Ÿæˆå”¯ä¸€çš„è¿è¡ŒID
        self.run_id = f"hendrycks_math_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.logger.info(f"ğŸ¯ Hendrycks Mathè¯„ä¼°æ¡†æ¶åˆå§‹åŒ–å®Œæˆï¼Œè¿è¡ŒID: {self.run_id}")
        
        # éªŒè¯æ•°æ®é›†æ ¼å¼
        self._validate_hendrycks_math_dataset()
    
    def _get_model_config(self, model_name: str) -> Dict[str, Any]:
        """
        æ ¹æ®æ¨¡å‹åç§°è·å–å®Œæ•´çš„æ¨¡å‹é…ç½®
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            æ¨¡å‹é…ç½®å­—å…¸
        """
        # å‚è€ƒ math_evaluation_framework.py ä¸­çš„ MODEL_CONFIGS
        MODEL_CONFIGS = {
            # DeepSeek R1 ç³»åˆ—æ¨¡å‹
            "deepseek_r1_1.5b": {
                "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                "type": "1.5b_quantized",
                "max_new_tokens": 500,
                "description": "DeepSeek-R1 1.5B æ¨¡å‹"
            },
            "deepseek_r1_7b": {
                "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B", 
                "type": "7b_quantized",
                "max_new_tokens": 600,
                "description": "DeepSeek-R1 7B æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰"
            },
            "deepseek_r1_14b": {
                "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
                "type": "14b_quantized",
                "max_new_tokens": 700,
                "description": "DeepSeek-R1 14B æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰"
            },
            "deepseek_r1_32b": {
                "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                "type": "32b_quantized",
                "max_new_tokens": 800,
                "description": "DeepSeek-R1 32B æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰"
            },
            "deepseek_r1_70b": {
                "name": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
                "type": "70b_quantized",
                "max_new_tokens": 1000,
                "description": "DeepSeek-R1 70B æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰"
            },
            
            # Qwen2.5 ç³»åˆ—æ¨¡å‹
            "qwen25_0.5b": {
                "name": "Qwen/Qwen2.5-0.5B-Instruct",
                "type": "0.5b",
                "max_new_tokens": 400,
                "description": "Qwen2.5 0.5B Instructæ¨¡å‹"
            },
            "qwen25_1.5b": {
                "name": "Qwen/Qwen2.5-1.5B-Instruct",
                "type": "1.5b",
                "max_new_tokens": 500,
                "description": "Qwen2.5 1.5B Instructæ¨¡å‹"
            },
            "qwen25_3b": {
                "name": "Qwen/Qwen2.5-3B-Instruct",
                "type": "3b",
                "max_new_tokens": 600,
                "description": "Qwen2.5 3B Instructæ¨¡å‹"
            },
            "qwen25_7b": {
                "name": "Qwen/Qwen2.5-7B-Instruct",
                "type": "7b",
                "max_new_tokens": 700,
                "description": "Qwen2.5 7B Instructæ¨¡å‹"
            },
            "qwen25_14b": {
                "name": "Qwen/Qwen2.5-14B-Instruct",
                "type": "14b",
                "max_new_tokens": 800,
                "description": "Qwen2.5 14B Instructæ¨¡å‹"
            },
            "qwen25_32b": {
                "name": "Qwen/Qwen2.5-32B-Instruct",
                "type": "32b",
                "max_new_tokens": 900,
                "description": "Qwen2.5 32B Instructæ¨¡å‹"
            },
            "qwen25_72b": {
                "name": "Qwen/Qwen2.5-72B-Instruct",
                "type": "72b",
                "max_new_tokens": 1000,
                "description": "Qwen2.5 72B Instructæ¨¡å‹"
            }
        }
        
        if model_name in MODEL_CONFIGS:
            return MODEL_CONFIGS[model_name]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}ã€‚æ”¯æŒçš„æ¨¡å‹: {list(MODEL_CONFIGS.keys())}")
    
    def _validate_hendrycks_math_dataset(self):
        """
        éªŒè¯Hendrycks Mathæ•°æ®é›†æ ¼å¼
        """
        try:
            df = pd.read_csv(self.dataset_path)
            required_columns = ['problem', 'level', 'type', 'solution', 'subset']
            
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                raise ValueError(f"âŒ Hendrycks Mathæ•°æ®é›†ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}")
            
            self.logger.info(f"âœ… Hendrycks Mathæ•°æ®é›†éªŒè¯é€šè¿‡ï¼ŒåŒ…å« {len(df)} ä¸ªæ ·æœ¬")
            self.logger.info(f"ğŸ“Š æ•°æ®é›†åˆ—: {list(df.columns)}")
            
        except Exception as e:
            self.logger.error(f"âŒ Hendrycks Mathæ•°æ®é›†éªŒè¯å¤±è´¥: {e}")
            raise
    
    def load_dataset(self, problem_type: str = None, samples_per_level: int = None, use_train: bool = False) -> pd.DataFrame:
        """
        åŠ è½½Hendrycks Mathæ•°æ®é›†
        
        Args:
            problem_type: é—®é¢˜ç±»å‹ç­›é€‰ï¼ˆå¦‚"Counting & Probability"ï¼‰
            samples_per_level: æ¯ä¸ªéš¾åº¦ç­‰çº§çš„æ ·æœ¬æ•°é‡
            use_train: æ˜¯å¦ä½¿ç”¨train.csvè€Œä¸æ˜¯test.csv
            
        Returns:
            åŒ…å«Hendrycks Mathæ•°æ®çš„DataFrame
        """
        try:
            # é€‰æ‹©æ•°æ®æ–‡ä»¶
            if use_train:
                data_file = self.dataset_path.replace('test.csv', 'train.csv')
                self.logger.info(f"ğŸ“‚ ä½¿ç”¨è®­ç»ƒé›†: {data_file}")
            else:
                data_file = self.dataset_path
                self.logger.info(f"ğŸ“‚ ä½¿ç”¨æµ‹è¯•é›†: {data_file}")
            
            df = pd.read_csv(data_file)
            self.logger.info(f"ğŸ“‚ æˆåŠŸåŠ è½½Hendrycks Mathæ•°æ®é›†: {len(df)} ä¸ªæ ·æœ¬")
            
            # æŒ‰é—®é¢˜ç±»å‹ç­›é€‰
            if problem_type:
                original_count = len(df)
                df = df[df['type'] == problem_type]
                self.logger.info(f"ğŸ” æŒ‰ç±»å‹ '{problem_type}' ç­›é€‰å: {len(df)} ä¸ªæ ·æœ¬ (å‡å°‘ {original_count - len(df)} ä¸ª)")
            
            # æŒ‰éš¾åº¦ç­‰çº§é‡‡æ ·
            if samples_per_level:
                sampled_dfs = []
                for level in sorted(df['level'].unique()):
                    level_df = df[df['level'] == level]
                    if len(level_df) >= samples_per_level:
                        # éšæœºé‡‡æ ·æŒ‡å®šæ•°é‡
                        sampled_level_df = level_df.sample(n=samples_per_level, random_state=42)
                    else:
                        # å¦‚æœè¯¥éš¾åº¦ç­‰çº§çš„æ ·æœ¬ä¸è¶³ï¼Œåˆ™ä½¿ç”¨å…¨éƒ¨
                        sampled_level_df = level_df
                        self.logger.warning(f"âš ï¸ éš¾åº¦ç­‰çº§ {level} æ ·æœ¬ä¸è¶³ {samples_per_level}ï¼Œä½¿ç”¨å…¨éƒ¨ {len(level_df)} ä¸ªæ ·æœ¬")
                    
                    sampled_dfs.append(sampled_level_df)
                
                df = pd.concat(sampled_dfs, ignore_index=True)
                self.logger.info(f"ğŸ“Š æŒ‰éš¾åº¦ç­‰çº§é‡‡æ ·å: {len(df)} ä¸ªæ ·æœ¬")
            
            # æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
            self.logger.info(f"ğŸ“Š éš¾åº¦åˆ†å¸ƒ: {df['level'].value_counts().to_dict()}")
            self.logger.info(f"ğŸ“Š ç±»å‹åˆ†å¸ƒ: {df['type'].value_counts().to_dict()}")
            self.logger.info(f"ğŸ“Š å­é›†åˆ†å¸ƒ: {df['subset'].value_counts().to_dict()}")
            
            return df
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½Hendrycks Mathæ•°æ®é›†å¤±è´¥: {e}")
            raise
    
    def format_question_for_model(self, row: pd.Series) -> str:
        """
        æ ¼å¼åŒ–Hendrycks Mathé—®é¢˜ä¾›æ¨¡å‹ä½¿ç”¨
        
        Args:
            row: æ•°æ®é›†ä¸­çš„ä¸€è¡Œæ•°æ®
            
        Returns:
            æ ¼å¼åŒ–åçš„é—®é¢˜æ–‡æœ¬
        """
        problem = row['problem']
        level = row['level']
        problem_type = row['type']
        
        # æ„å»ºHendrycks Mathä¸“ç”¨æç¤º
        prompt = f"""è¯·è§£å†³ä»¥ä¸‹æ•°å­¦é—®é¢˜ï¼š

éš¾åº¦ç­‰çº§: {level}
é—®é¢˜ç±»å‹: {problem_type}

é—®é¢˜: {problem}

è¯·æä¾›è¯¦ç»†çš„è§£é¢˜æ­¥éª¤å’Œæœ€ç»ˆç­”æ¡ˆã€‚ç¡®ä¿ä½ çš„ç­”æ¡ˆå‡†ç¡®ä¸”å®Œæ•´ã€‚"""
        
        return prompt
    
    def evaluate_with_openai(self, question: str, model_answer: str, correct_answer: str, 
                           level: str, problem_type: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨OpenAIè¯„ä¼°Hendrycks Mathç­”æ¡ˆ
        
        Args:
            question: åŸå§‹é—®é¢˜
            model_answer: æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆ
            correct_answer: æ­£ç¡®ç­”æ¡ˆ
            level: éš¾åº¦ç­‰çº§ (Hendrycks Mathå­—æ®µ)
            problem_type: é—®é¢˜ç±»å‹ (Hendrycks Mathå­—æ®µ)
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        try:
            # æ„å»ºHendrycks Mathä¸“ç”¨çš„è¯„ä¼°æç¤ºï¼Œå‚è€ƒç»Ÿä¸€è¯„ä¼°æ¡†æ¶çš„æ ¼å¼
            prompt = f"""You are a professional mathematical education evaluator. Please evaluate the quality of the answer to the following mathematical problem.

Problem: {question}

Difficulty Level: {level}
Problem Type: {problem_type}

Correct Answer: {correct_answer}

Model Response: {model_answer}

Please evaluate from the following aspects and give a score from 1 to 10:

1. Answer Correctness (1-10 points): Whether the final answer is correct
2. Reasoning Logic (1-10 points): Whether the reasoning process is clear and logical
3. Step Completeness (1-10 points): Whether all necessary solution steps are shown
4. Mathematical Accuracy (1-10 points): Whether mathematical calculations and formulas are accurate
5. Expression Clarity (1-10 points): Whether the expression is clear and easy to understand

IMPORTANT: You must respond with ONLY a valid JSON object. Do not include any other text, explanations, or markdown formatting.

Required JSON format:
{{
    "answer_correctness": <score>,
    "reasoning_logic": <score>,
    "step_completeness": <score>,
    "mathematical_accuracy": <score>,
    "expression_clarity": <score>,
    "overall_score": <average_of_all_scores>,
    "comments": "<brief_evaluation_comments>"
}}

Example:
{{
    "answer_correctness": 8,
    "reasoning_logic": 7,
    "step_completeness": 9,
    "mathematical_accuracy": 8,
    "expression_clarity": 7,
    "overall_score": 7.8,
    "comments": "Good reasoning but could be more detailed in steps"
}}"""

            openai.api_key = os.getenv('OPENAI_API_KEY')
            
            # æ£€æŸ¥OpenAI APIå¯†é’¥æ˜¯å¦å­˜åœ¨
            if not openai.api_key:
                self.logger.warning("âš ï¸ æœªè®¾ç½®OpenAI APIå¯†é’¥ï¼Œè·³è¿‡è¯„ä¼°")
                return {
                    "answer_correctness": 0,
                    "reasoning_logic": 0,
                    "step_completeness": 0,
                    "mathematical_accuracy": 0,
                    "expression_clarity": 0,
                    "overall_score": 0,
                    "comments": "æœªè®¾ç½®OpenAI APIå¯†é’¥ï¼Œè·³è¿‡è¯„ä¼°",
                    "error": True,
                    "error_type": "no_openai_key"
                }
            
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a professional mathematical education evaluator."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3
            )
            
            response_content = response.choices[0].message.content
            
            # è§£æJSONå“åº”ï¼Œå‚è€ƒç»Ÿä¸€è¯„ä¼°æ¡†æ¶çš„é”™è¯¯å¤„ç†
            import json
            import re
            
            try:
                evaluation = json.loads(response_content)
                return evaluation
            except json.JSONDecodeError as e:
                self.logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
                self.logger.warning(f"åŸå§‹å“åº”: {response_content[:200]}...")  # åªæ˜¾ç¤ºå‰200å­—ç¬¦
                
                # å°è¯•æå–JSONéƒ¨åˆ†
                try:
                    # æŸ¥æ‰¾JSONå¯¹è±¡
                    json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group()
                        evaluation = json.loads(json_str)
                        self.logger.info(f"âœ… æˆåŠŸæå–JSONéƒ¨åˆ†")
                        return evaluation
                except:
                    pass
                
                # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                try:
                    # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                    cleaned_response = response_content.replace('```json', '').replace('```', '').strip()
                    evaluation = json.loads(cleaned_response)
                    self.logger.info(f"âœ… æˆåŠŸä¿®å¤JSONæ ¼å¼")
                    return evaluation
                except:
                    pass
                
                return {
                    "raw_response": response_content,
                    "error": "JSON parsing failed",
                    "parse_error": str(e),
                    "error_type": "json_decode_error"
                }
            
        except Exception as e:
            self.logger.error(f"âŒ OpenAI evaluation failed: {e}")
            return {
                "error": f"Evaluation failed: {e}",
                "error_type": "openai_api_error",
                "exception": str(e)
            }
    
    def _create_error_evaluation(self, error_message: str) -> Dict[str, Any]:
        """
        åˆ›å»ºé”™è¯¯è¯„ä¼°ç»“æœ
        
        Args:
            error_message: é”™è¯¯ä¿¡æ¯
            
        Returns:
            é”™è¯¯è¯„ä¼°å­—å…¸
        """
        return {
            'answer_correctness': 0,
            'reasoning_logic': 0,
            'step_completeness': 0,
            'mathematical_accuracy': 0,
            'expression_clarity': 0,
            'overall_score': 0,
            'comments': f"è¯„ä¼°å¤±è´¥: {error_message}",
            'error': True,
            'error_type': 'evaluation_failure'
        }
    
    def save_intermediate_results(self, results: List[Dict], sample_count: int):
        """
        ä¿å­˜Hendrycks Mathä¸­é—´ç»“æœ
        
        Args:
            results: è¯„ä¼°ç»“æœåˆ—è¡¨
            sample_count: å·²å¤„ç†çš„æ ·æœ¬æ•°é‡
        """
        try:
            # åˆ›å»ºHendrycks Mathä¸“ç”¨ç›®å½•ç»“æ„
            save_dir = Path(f"data/hendrycks_math_results/{self.model_name}/{self.run_id}")
            save_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            intermediate_file = save_dir / f"intermediate_results_{sample_count}.json"
            with open(intermediate_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ’¾ Hendrycks Mathä¸­é—´ç»“æœå·²ä¿å­˜: {intermediate_file}")
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜Hendrycks Mathä¸­é—´ç»“æœå¤±è´¥: {e}")
    
    def run_evaluation(self, max_samples: Optional[int] = None, 
                      problem_type: str = None, samples_per_level: int = None, use_train: bool = False) -> List[Dict]:
        """
        è¿è¡ŒHendrycks Mathè¯„ä¼°
        
        Args:
            max_samples: æœ€å¤§æ ·æœ¬æ•°é‡ï¼ˆå·²åºŸå¼ƒï¼Œä½¿ç”¨samples_per_levelæ›¿ä»£ï¼‰
            problem_type: é—®é¢˜ç±»å‹ç­›é€‰ï¼ˆå¦‚"Counting & Probability"ï¼‰
            samples_per_level: æ¯ä¸ªéš¾åº¦ç­‰çº§çš„æ ·æœ¬æ•°é‡
            use_train: æ˜¯å¦ä½¿ç”¨train.csvè€Œä¸æ˜¯test.csv
            
        Returns:
            è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        try:
            # åŠ è½½æ¨¡å‹ - å‚è€ƒç»Ÿä¸€è¯„ä¼°æ¡†æ¶çš„æ–¹å¼
            self.load_model()
            
            # åŠ è½½æ•°æ®é›†
            df = self.load_dataset(problem_type=problem_type, samples_per_level=samples_per_level, use_train=use_train)
            
            # å…¼å®¹æ—§çš„max_sampleså‚æ•°
            if max_samples and not samples_per_level:
                df = df.head(max_samples)
            
            self.logger.info(f"ğŸš€ å¼€å§‹Hendrycks Mathè¯„ä¼°ï¼Œå…± {len(df)} ä¸ªæ ·æœ¬")
            
            results = []
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰ç»“æœå¯ä»¥æ¢å¤
            existing_count = self._check_existing_results()
            if existing_count > 0:
                self.logger.info(f"ğŸ”„ å‘ç°ç°æœ‰ç»“æœï¼Œä»ç¬¬ {existing_count + 1} ä¸ªæ ·æœ¬å¼€å§‹")
                df = df.iloc[existing_count:]
            
            for idx, row in tqdm(df.iterrows(), total=len(df), desc="Hendrycks Mathè¯„ä¼°è¿›åº¦"):
                try:
                    # æ ¼å¼åŒ–é—®é¢˜
                    question = self.format_question_for_model(row)
                    
                    # ç”Ÿæˆç­”æ¡ˆ
                    model_answer = self.generate_response(question)
                    
                    if not model_answer or model_answer.strip() == "":
                        self.logger.warning(f"âš ï¸ æ ·æœ¬ {idx} ç”Ÿæˆå¤±è´¥")
                        evaluation = self._create_error_evaluation("ç”Ÿæˆå¤±è´¥ï¼šç©ºç­”æ¡ˆ")
                    else:
                        # è¯„ä¼°ç­”æ¡ˆ
                        evaluation = self.evaluate_with_openai(
                            question=row['problem'],
                            model_answer=model_answer,
                            correct_answer=row['solution'],
                            level=row['level'],
                            problem_type=row['type']
                        )
                        
                        # æ£€æŸ¥è¯„ä¼°ç»“æœï¼Œå‚è€ƒç»Ÿä¸€è¯„ä¼°æ¡†æ¶çš„å¤„ç†æ–¹å¼
                        if isinstance(evaluation, dict) and "error" not in evaluation:
                            self.logger.info(f"âœ… è¯„ä¼°å®Œæˆï¼Œæ€»åˆ†: {evaluation.get('overall_score', 0):.2f}")
                        else:
                            # ä¸ºè¯„ä¼°å¤±è´¥çš„æ ·æœ¬åˆ›å»ºç‰¹æ®Šè¯„ä¼°
                            failed_evaluation = {
                                "answer_correctness": 0,
                                "reasoning_logic": 0,
                                "step_completeness": 0,
                                "mathematical_accuracy": 0,
                                "expression_clarity": 0,
                                "overall_score": 0,
                                "comments": f"è¯„ä¼°å¤±è´¥: {str(evaluation)}",
                                "error": "evaluation_failed"
                            }
                            evaluation = failed_evaluation
                    
                    # æ„å»ºç»“æœ
                    result = {
                        'sample_id': idx,
                        'problem': row['problem'],
                        'correct_answer': row['solution'],
                        'model_answer': model_answer,
                        'level': row['level'],
                        'type': row['type'],
                        'subset': row['subset'],
                        'evaluation': evaluation,
                        'timestamp': datetime.now().isoformat()
                    }
                    
                    results.append(result)
                    
                    # æ¯10ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
                    if len(results) % 10 == 0:
                        self.save_intermediate_results(results, len(results))
                    
                except Exception as e:
                    self.logger.error(f"âŒ æ ·æœ¬ {idx} å¤„ç†å¤±è´¥: {e}")
                    error_result = {
                        'sample_id': idx,
                        'problem': row.get('problem', ''),
                        'correct_answer': row.get('solution', ''),
                        'model_answer': '',
                        'level': row.get('level', 'Unknown'),
                        'type': row.get('type', 'Unknown'),
                        'subset': row.get('subset', 'Unknown'),
                        'evaluation': self._create_error_evaluation(f"å¤„ç†å¤±è´¥: {str(e)}"),
                        'timestamp': datetime.now().isoformat()
                    }
                    results.append(error_result)
            
            # ä¿å­˜æœ€ç»ˆç»“æœ
            self.save_final_results(results)
            
            self.logger.info(f"âœ… Hendrycks Mathè¯„ä¼°å®Œæˆï¼Œå…±å¤„ç† {len(results)} ä¸ªæ ·æœ¬")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ Hendrycks Mathè¯„ä¼°å¤±è´¥: {e}")
            raise
    
    def save_final_results(self, results: List[Dict]):
        """
        ä¿å­˜Hendrycks Mathæœ€ç»ˆç»“æœ
        
        Args:
            results: è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        try:
            # åˆ›å»ºä¿å­˜ç›®å½•
            save_dir = Path(f"data/hendrycks_math_results/{self.model_name}/{self.run_id}")
            save_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜JSONæ ¼å¼
            json_file = save_dir / "final_results.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜CSVæ ¼å¼
            csv_file = save_dir / "final_results.csv"
            df_results = pd.DataFrame(results)
            df_results.to_csv(csv_file, index=False, encoding='utf-8')
            
            self.logger.info(f"ğŸ’¾ Hendrycks Mathæœ€ç»ˆç»“æœå·²ä¿å­˜:")
            self.logger.info(f"   JSON: {json_file}")
            self.logger.info(f"   CSV: {csv_file}")
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜Hendrycks Mathæœ€ç»ˆç»“æœå¤±è´¥: {e}")
    
    def _check_existing_results(self) -> int:
        """
        æ£€æŸ¥ç°æœ‰ç»“æœï¼Œè¿”å›å·²å¤„ç†çš„æ ·æœ¬æ•°é‡
        
        Returns:
            å·²å¤„ç†çš„æ ·æœ¬æ•°é‡
        """
        try:
            save_dir = Path(f"data/hendrycks_math_results/{self.model_name}/{self.run_id}")
            if not save_dir.exists():
                return 0
            
            # æŸ¥æ‰¾æœ€æ–°çš„ä¸­é—´ç»“æœæ–‡ä»¶
            intermediate_files = list(save_dir.glob("intermediate_results_*.json"))
            if not intermediate_files:
                return 0
            
            # æŒ‰æ–‡ä»¶åä¸­çš„æ•°å­—æ’åº
            latest_file = max(intermediate_files, key=lambda x: int(x.stem.split('_')[-1]))
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            return len(results)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ£€æŸ¥ç°æœ‰ç»“æœå¤±è´¥: {e}")
            return 0


def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description='Hendrycks Mathæ•°æ®é›†æ•°å­¦è¯„ä¼°æ¡†æ¶')
    parser.add_argument('-m', '--model', type=str, required=True,
                       help='æ¨¡å‹åç§° (deepseek_r1_1.5b, deepseek_r1_7b, deepseek_r1_14b, deepseek_r1_32b, deepseek_r1_70b, qwen25_0.5b, qwen25_1.5b, qwen25_3b, qwen25_7b, qwen25_14b, qwen25_32b, qwen25_72b)')
    parser.add_argument('-d', '--dataset', type=str, required=True,
                       help='Hendrycks Mathæ•°æ®é›†è·¯å¾„')
    parser.add_argument('-s', '--samples', type=int, default=None,
                       help='æœ€å¤§æ ·æœ¬æ•°é‡ (é»˜è®¤: å…¨éƒ¨)')
    parser.add_argument('-t', '--type', type=str, default=None,
                       help='é—®é¢˜ç±»å‹ç­›é€‰ (å¦‚: "Counting & Probability")')
    parser.add_argument('-l', '--samples_per_level', type=int, default=None,
                       help='æ¯ä¸ªéš¾åº¦ç­‰çº§çš„æ ·æœ¬æ•°é‡')
    parser.add_argument('--use_train', action='store_true',
                       help='ä½¿ç”¨train.csvè€Œä¸æ˜¯test.csv')
    parser.add_argument('--hf_token', type=str, default=None,
                       help='Hugging Face API Token (ç”¨äºè¿œç¨‹æ¨¡å‹)')
    
    args = parser.parse_args()
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    if args.hf_token:
        os.environ['HF_TOKEN'] = args.hf_token
    
    # éªŒè¯æ•°æ®é›†è·¯å¾„
    if not os.path.exists(args.dataset):
        print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {args.dataset}")
        sys.exit(1)
    
    try:
        # åˆ›å»ºè¯„ä¼°æ¡†æ¶
        framework = HendrycksMathEvaluationFramework(
            model_name=args.model,
            dataset_path=args.dataset
        )
        
        # è¿è¡Œè¯„ä¼°
        results = framework.run_evaluation(
            max_samples=args.samples,
            problem_type=args.type,
            samples_per_level=args.samples_per_level,
            use_train=args.use_train
        )
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š Hendrycks Mathè¯„ä¼°ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(results)}")
        
        if args.type:
            print(f"   é—®é¢˜ç±»å‹: {args.type}")
        
        if args.samples_per_level:
            print(f"   æ¯éš¾åº¦ç­‰çº§æ ·æœ¬æ•°: {args.samples_per_level}")
        
        if args.use_train:
            print(f"   æ•°æ®é›†: train.csv")
        else:
            print(f"   æ•°æ®é›†: test.csv")
        
        # è®¡ç®—æˆåŠŸç‡
        successful = sum(1 for r in results if not r.get('evaluation', {}).get('error', False))
        success_rate = successful / len(results) * 100
        print(f"   æˆåŠŸç‡: {success_rate:.1f}% ({successful}/{len(results)})")
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        scores = [r.get('evaluation', {}).get('overall_score', 0) for r in results 
                 if not r.get('evaluation', {}).get('error', False)]
        if scores:
            avg_score = np.mean(scores)
            print(f"   å¹³å‡åˆ†æ•°: {avg_score:.2f}/10")
        
        print(f"âœ… Hendrycks Mathè¯„ä¼°å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ Hendrycks Mathè¯„ä¼°å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 