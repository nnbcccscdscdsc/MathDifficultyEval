#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MATH-500æ•°æ®é›†æ•°å­¦è¯„ä¼°æ¡†æ¶
ä¸“é—¨é€‚é…MATH-500æ•°æ®é›†çš„å­—æ®µç»“æ„

æ•°æ®é›†å­—æ®µï¼š
- id: é—®é¢˜ID
- problem: æ•°å­¦é—®é¢˜
- solution: è§£å†³æ–¹æ¡ˆ
- answer: ç­”æ¡ˆ
- difficulty: éš¾åº¦ç­‰çº§
- topic: ä¸»é¢˜åˆ†ç±»
- difficulty_score: éš¾åº¦åˆ†æ•°
- source_dataset: æ•°æ®æ¥æº
"""

import os
import sys
import json
import logging
import argparse
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Union, Any
from tqdm import tqdm
import openai
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from math_evaluation_framework import MathEvaluationFramework

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('math500_evaluation.log'),
        logging.StreamHandler()
    ]
)

class Math500EvaluationFramework(MathEvaluationFramework):
    """
    MATH-500æ•°æ®é›†ä¸“ç”¨è¯„ä¼°æ¡†æ¶
    ç»§æ‰¿è‡ªåŸºç¡€è¯„ä¼°æ¡†æ¶ï¼Œé€‚é…MATH-500æ•°æ®é›†çš„ç‰¹æ®Šå­—æ®µ
    """
    
    def __init__(self, model_name: str, dataset_path: str, **kwargs):
        """
        åˆå§‹åŒ–MATH-500è¯„ä¼°æ¡†æ¶
        
        Args:
            model_name: æ¨¡å‹åç§°
            dataset_path: MATH-500æ•°æ®é›†è·¯å¾„
            **kwargs: å…¶ä»–å‚æ•°
        """
        # æ„å»ºæ¨¡å‹é…ç½®å­—å…¸ï¼Œä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹æ˜ å°„
        model_config = self._get_model_config(model_name)
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(model_config, **kwargs)
        
        # ä¿å­˜æ•°æ®é›†è·¯å¾„
        self.dataset_path = dataset_path
        
        # è®¾ç½®loggerå±æ€§
        self.logger = logging.getLogger(__name__)
        
        # ç”Ÿæˆå”¯ä¸€çš„è¿è¡ŒID
        self.run_id = f"math500_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.logger.info(f"ğŸ¯ MATH-500è¯„ä¼°æ¡†æ¶åˆå§‹åŒ–å®Œæˆï¼Œè¿è¡ŒID: {self.run_id}")
        
        # éªŒè¯æ•°æ®é›†æ ¼å¼
        self._validate_math500_dataset()
    
    def _get_model_config(self, model_name: str) -> Dict[str, Any]:
        """
        æ ¹æ®æ¨¡å‹åç§°è·å–å®Œæ•´çš„æ¨¡å‹é…ç½®
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            æ¨¡å‹é…ç½®å­—å…¸
        """
        # å‚è€ƒ math_evaluation_framework.py ä¸­çš„ MODEL_CONFIGS
        MODEL_CONFIGS = {
            # DeepSeek R1 ç³»åˆ—æ¨¡å‹
            "deepseek_r1_1.5b": {
                "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                "type": "1.5b_quantized",
                "max_new_tokens": 500,
                "description": "DeepSeek-R1 1.5B æ¨¡å‹"
            },
            "deepseek_r1_7b": {
                "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B", 
                "type": "7b_quantized",
                "max_new_tokens": 600,
                "description": "DeepSeek-R1 7B æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰"
            },
            "deepseek_r1_14b": {
                "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
                "type": "14b_quantized",
                "max_new_tokens": 700,
                "description": "DeepSeek-R1 14B æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰"
            },
            "deepseek_r1_32b": {
                "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                "type": "32b_quantized",
                "max_new_tokens": 800,
                "description": "DeepSeek-R1 32B æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰"
            },
            "deepseek_r1_70b": {
                "name": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
                "type": "70b_quantized",
                "max_new_tokens": 1000,
                "description": "DeepSeek-R1 70B æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰"
            },
            
            # Qwen2.5 ç³»åˆ—æ¨¡å‹
            "qwen25_0.5b": {
                "name": "Qwen/Qwen2.5-0.5B-Instruct",
                "type": "0.5b",
                "max_new_tokens": 400,
                "description": "Qwen2.5 0.5B Instructæ¨¡å‹"
            },
            "qwen25_1.5b": {
                "name": "Qwen/Qwen2.5-1.5B-Instruct",
                "type": "1.5b",
                "max_new_tokens": 500,
                "description": "Qwen2.5 1.5B Instructæ¨¡å‹"
            },
            "qwen25_3b": {
                "name": "Qwen/Qwen2.5-3B-Instruct",
                "type": "3b",
                "max_new_tokens": 600,
                "description": "Qwen2.5 3B Instructæ¨¡å‹"
            },
            "qwen25_7b": {
                "name": "Qwen/Qwen2.5-7B-Instruct",
                "type": "7b",
                "max_new_tokens": 700,
                "description": "Qwen2.5 7B Instructæ¨¡å‹"
            },
            "qwen25_14b": {
                "name": "Qwen/Qwen2.5-14B-Instruct",
                "type": "14b",
                "max_new_tokens": 800,
                "description": "Qwen2.5 14B Instructæ¨¡å‹"
            },
            "qwen25_32b": {
                "name": "Qwen/Qwen2.5-32B-Instruct",
                "type": "32b",
                "max_new_tokens": 900,
                "description": "Qwen2.5 32B Instructæ¨¡å‹"
            },
            "qwen25_72b": {
                "name": "Qwen/Qwen2.5-72B-Instruct",
                "type": "72b",
                "max_new_tokens": 1000,
                "description": "Qwen2.5 72B Instructæ¨¡å‹"
            }
        }
        
        if model_name in MODEL_CONFIGS:
            return MODEL_CONFIGS[model_name]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}ã€‚æ”¯æŒçš„æ¨¡å‹: {list(MODEL_CONFIGS.keys())}")
    
    def _validate_math500_dataset(self):
        """
        éªŒè¯MATH-500æ•°æ®é›†æ ¼å¼
        """
        try:
            df = pd.read_csv(self.dataset_path)
            required_columns = ['problem', 'solution', 'answer', 'subject', 'level', 'unique_id']
            
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                raise ValueError(f"âŒ MATH-500æ•°æ®é›†ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}")
            
            self.logger.info(f"âœ… MATH-500æ•°æ®é›†éªŒè¯é€šè¿‡ï¼ŒåŒ…å« {len(df)} ä¸ªæ ·æœ¬")
            self.logger.info(f"ğŸ“Š æ•°æ®é›†åˆ—: {list(df.columns)}")
            
        except Exception as e:
            self.logger.error(f"âŒ MATH-500æ•°æ®é›†éªŒè¯å¤±è´¥: {e}")
            raise
    
    def load_dataset(self) -> pd.DataFrame:
        """
        åŠ è½½MATH-500æ•°æ®é›†
        
        Returns:
            åŒ…å«MATH-500æ•°æ®çš„DataFrame
        """
        try:
            df = pd.read_csv(self.dataset_path)
            self.logger.info(f"ğŸ“‚ æˆåŠŸåŠ è½½MATH-500æ•°æ®é›†: {len(df)} ä¸ªæ ·æœ¬")
            
            # æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
            self.logger.info(f"ğŸ“Š éš¾åº¦åˆ†å¸ƒ: {df['level'].value_counts().to_dict()}")
            self.logger.info(f"ğŸ“Š ä¸»é¢˜åˆ†å¸ƒ: {df['subject'].value_counts().to_dict()}")
            self.logger.info(f"ğŸ“Š éš¾åº¦èŒƒå›´: {df['level'].min()} - {df['level'].max()}")
            
            return df
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½MATH-500æ•°æ®é›†å¤±è´¥: {e}")
            raise
    
    def format_question_for_model(self, row: pd.Series) -> str:
        """
        æ ¼å¼åŒ–MATH-500é—®é¢˜ä¾›æ¨¡å‹ä½¿ç”¨
        
        Args:
            row: æ•°æ®é›†ä¸­çš„ä¸€è¡Œæ•°æ®
            
        Returns:
            æ ¼å¼åŒ–åçš„é—®é¢˜æ–‡æœ¬
        """
        problem = row['problem']
        subject = row['subject']
        level = row['level']
        
        # æ„å»ºMATH-500ä¸“ç”¨æç¤º
        prompt = f"""è¯·è§£å†³ä»¥ä¸‹æ•°å­¦é—®é¢˜ï¼š

ä¸»é¢˜: {subject}
éš¾åº¦ç­‰çº§: {level}

é—®é¢˜: {problem}

è¯·æä¾›è¯¦ç»†çš„è§£é¢˜æ­¥éª¤å’Œæœ€ç»ˆç­”æ¡ˆã€‚ç¡®ä¿ä½ çš„ç­”æ¡ˆå‡†ç¡®ä¸”å®Œæ•´ã€‚"""
        
        return prompt
    
    def evaluate_with_openai(self, question: str, model_answer: str, correct_answer: str, 
                           subject: str, level: int) -> Dict[str, Any]:
        """
        ä½¿ç”¨OpenAIè¯„ä¼°MATH-500ç­”æ¡ˆ
        
        Args:
            question: åŸå§‹é—®é¢˜
            model_answer: æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆ
            correct_answer: æ­£ç¡®ç­”æ¡ˆ
            subject: ä¸»é¢˜åˆ†ç±» (MATH-500å­—æ®µ)
            level: éš¾åº¦ç­‰çº§ (MATH-500å­—æ®µ)
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        try:
            # æ„å»ºMATH-500ä¸“ç”¨çš„è¯„ä¼°æç¤ºï¼Œå‚è€ƒç»Ÿä¸€è¯„ä¼°æ¡†æ¶çš„æ ¼å¼
            prompt = f"""You are a professional mathematical education evaluator. Please evaluate the quality of the answer to the following mathematical problem.

Problem: {question}

Subject: {subject}
Difficulty Level: {level}

Correct Answer: {correct_answer}

Model Response: {model_answer}

Please evaluate from the following aspects and give a score from 1 to 10:

1. Answer Correctness (1-10 points): Whether the final answer is correct
2. Reasoning Logic (1-10 points): Whether the reasoning process is clear and logical
3. Step Completeness (1-10 points): Whether all necessary solution steps are shown
4. Mathematical Accuracy (1-10 points): Whether mathematical calculations and formulas are accurate
5. Expression Clarity (1-10 points): Whether the expression is clear and easy to understand

IMPORTANT: You must respond with ONLY a valid JSON object. Do not include any other text, explanations, or markdown formatting.

Required JSON format:
{{
    "answer_correctness": <score>,
    "reasoning_logic": <score>,
    "step_completeness": <score>,
    "mathematical_accuracy": <score>,
    "expression_clarity": <score>,
    "overall_score": <average_of_all_scores>,
    "comments": "<brief_evaluation_comments>"
}}

Example:
{{
    "answer_correctness": 8,
    "reasoning_logic": 7,
    "step_completeness": 9,
    "mathematical_accuracy": 8,
    "expression_clarity": 7,
    "overall_score": 7.8,
    "comments": "Good reasoning but could be more detailed in steps"
}}"""

            openai.api_key = os.getenv('OPENAI_API_KEY')
            
            # æ£€æŸ¥OpenAI APIå¯†é’¥æ˜¯å¦å­˜åœ¨
            if not openai.api_key:
                self.logger.warning("âš ï¸ æœªè®¾ç½®OpenAI APIå¯†é’¥ï¼Œè·³è¿‡è¯„ä¼°")
                return {
                    "answer_correctness": 0,
                    "reasoning_logic": 0,
                    "step_completeness": 0,
                    "mathematical_accuracy": 0,
                    "expression_clarity": 0,
                    "overall_score": 0,
                    "comments": "æœªè®¾ç½®OpenAI APIå¯†é’¥ï¼Œè·³è¿‡è¯„ä¼°",
                    "error": True,
                    "error_type": "no_openai_key"
                }
            
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a professional mathematical education evaluator."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3
            )
            
            response_content = response.choices[0].message.content
            
            # è§£æJSONå“åº”ï¼Œå‚è€ƒç»Ÿä¸€è¯„ä¼°æ¡†æ¶çš„é”™è¯¯å¤„ç†
            import json
            import re
            
            try:
                evaluation = json.loads(response_content)
                return evaluation
            except json.JSONDecodeError as e:
                self.logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
                self.logger.warning(f"åŸå§‹å“åº”: {response_content[:200]}...")  # åªæ˜¾ç¤ºå‰200å­—ç¬¦
                
                # å°è¯•æå–JSONéƒ¨åˆ†
                try:
                    # æŸ¥æ‰¾JSONå¯¹è±¡
                    json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group()
                        evaluation = json.loads(json_str)
                        self.logger.info(f"âœ… æˆåŠŸæå–JSONéƒ¨åˆ†")
                        return evaluation
                except:
                    pass
                
                # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                try:
                    # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                    cleaned_response = response_content.replace('```json', '').replace('```', '').strip()
                    evaluation = json.loads(cleaned_response)
                    self.logger.info(f"âœ… æˆåŠŸä¿®å¤JSONæ ¼å¼")
                    return evaluation
                except:
                    pass
                
                return {
                    "raw_response": response_content,
                    "error": "JSON parsing failed",
                    "parse_error": str(e),
                    "error_type": "json_decode_error"
                }
            
        except Exception as e:
            self.logger.error(f"âŒ OpenAI evaluation failed: {e}")
            return {
                "error": f"Evaluation failed: {e}",
                "error_type": "openai_api_error",
                "exception": str(e)
            }
    
    def _create_error_evaluation(self, error_message: str) -> Dict[str, Any]:
        """
        åˆ›å»ºé”™è¯¯è¯„ä¼°ç»“æœ
        
        Args:
            error_message: é”™è¯¯ä¿¡æ¯
            
        Returns:
            é”™è¯¯è¯„ä¼°å­—å…¸
        """
        return {
            'answer_correctness': 0,
            'reasoning_logic': 0,
            'step_completeness': 0,
            'mathematical_accuracy': 0,
            'expression_clarity': 0,
            'overall_score': 0,
            'comments': f"è¯„ä¼°å¤±è´¥: {error_message}",
            'error': True,
            'error_type': 'evaluation_failure'
        }
    
    def save_intermediate_results(self, results: List[Dict], sample_count: int):
        """
        ä¿å­˜MATH-500ä¸­é—´ç»“æœ
        
        Args:
            results: è¯„ä¼°ç»“æœåˆ—è¡¨
            sample_count: å·²å¤„ç†çš„æ ·æœ¬æ•°é‡
        """
        try:
            # åˆ›å»ºMATH-500ä¸“ç”¨ç›®å½•ç»“æ„
            save_dir = Path(f"data/math500_results/{self.model_name}/{self.run_id}")
            save_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            intermediate_file = save_dir / f"intermediate_results_{sample_count}.json"
            with open(intermediate_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ’¾ MATH-500ä¸­é—´ç»“æœå·²ä¿å­˜: {intermediate_file}")
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜MATH-500ä¸­é—´ç»“æœå¤±è´¥: {e}")
    
    def run_evaluation(self, max_samples: Optional[int] = None) -> List[Dict]:
        """
        è¿è¡ŒMATH-500è¯„ä¼°
        
        Args:
            max_samples: æœ€å¤§æ ·æœ¬æ•°é‡
            
        Returns:
            è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        try:
            # åŠ è½½æ¨¡å‹ - å‚è€ƒç»Ÿä¸€è¯„ä¼°æ¡†æ¶çš„æ–¹å¼
            self.load_model()
            
            # åŠ è½½æ•°æ®é›†
            df = self.load_dataset()
            
            if max_samples:
                df = df.head(max_samples)
            
            self.logger.info(f"ğŸš€ å¼€å§‹MATH-500è¯„ä¼°ï¼Œå…± {len(df)} ä¸ªæ ·æœ¬")
            
            results = []
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰ç»“æœå¯ä»¥æ¢å¤
            existing_count = self._check_existing_results()
            if existing_count > 0:
                self.logger.info(f"ğŸ”„ å‘ç°ç°æœ‰ç»“æœï¼Œä»ç¬¬ {existing_count + 1} ä¸ªæ ·æœ¬å¼€å§‹")
                df = df.iloc[existing_count:]
            
            for idx, row in tqdm(df.iterrows(), total=len(df), desc="MATH-500è¯„ä¼°è¿›åº¦"):
                try:
                    # æ ¼å¼åŒ–é—®é¢˜
                    question = self.format_question_for_model(row)
                    
                    # ç”Ÿæˆç­”æ¡ˆ
                    model_answer = self.generate_response(question)
                    
                    if not model_answer or model_answer.strip() == "":
                        self.logger.warning(f"âš ï¸ æ ·æœ¬ {idx} ç”Ÿæˆå¤±è´¥")
                        evaluation = self._create_error_evaluation("ç”Ÿæˆå¤±è´¥ï¼šç©ºç­”æ¡ˆ")
                    else:
                        # è¯„ä¼°ç­”æ¡ˆ
                        evaluation = self.evaluate_with_openai(
                            question=row['problem'],
                            model_answer=model_answer,
                            correct_answer=row['answer'],
                            subject=row['subject'],
                            level=row['level']
                        )
                        
                        # æ£€æŸ¥è¯„ä¼°ç»“æœï¼Œå‚è€ƒç»Ÿä¸€è¯„ä¼°æ¡†æ¶çš„å¤„ç†æ–¹å¼
                        if isinstance(evaluation, dict) and "error" not in evaluation:
                            self.logger.info(f"âœ… è¯„ä¼°å®Œæˆï¼Œæ€»åˆ†: {evaluation.get('overall_score', 0):.2f}")
                        else:
                            # ä¸ºè¯„ä¼°å¤±è´¥çš„æ ·æœ¬åˆ›å»ºç‰¹æ®Šè¯„ä¼°
                            failed_evaluation = {
                                "answer_correctness": 0,
                                "reasoning_logic": 0,
                                "step_completeness": 0,
                                "mathematical_accuracy": 0,
                                "expression_clarity": 0,
                                "overall_score": 0,
                                "comments": f"è¯„ä¼°å¤±è´¥: {str(evaluation)}",
                                "error": "evaluation_failed"
                            }
                            evaluation = failed_evaluation
                    
                    # æ„å»ºç»“æœ
                    result = {
                        'sample_id': idx,
                        'unique_id': row['unique_id'],
                        'problem': row['problem'],
                        'correct_answer': row['answer'],
                        'model_answer': model_answer,
                        'subject': row['subject'],
                        'level': row['level'],
                        'evaluation': evaluation,
                        'timestamp': datetime.now().isoformat()
                    }
                    
                    results.append(result)
                    
                    # æ¯10ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
                    if len(results) % 10 == 0:
                        self.save_intermediate_results(results, len(results))
                    
                except Exception as e:
                    self.logger.error(f"âŒ æ ·æœ¬ {idx} å¤„ç†å¤±è´¥: {e}")
                    error_result = {
                        'sample_id': idx,
                        'unique_id': row.get('unique_id', f'error_{idx}'),
                        'problem': row.get('problem', ''),
                        'correct_answer': row.get('answer', ''),
                        'model_answer': '',
                        'subject': row.get('subject', 'Unknown'),
                        'level': row.get('level', 0),
                        'evaluation': self._create_error_evaluation(f"å¤„ç†å¤±è´¥: {str(e)}"),
                        'timestamp': datetime.now().isoformat()
                    }
                    results.append(error_result)
            
            # ä¿å­˜æœ€ç»ˆç»“æœ
            self.save_final_results(results)
            
            self.logger.info(f"âœ… MATH-500è¯„ä¼°å®Œæˆï¼Œå…±å¤„ç† {len(results)} ä¸ªæ ·æœ¬")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ MATH-500è¯„ä¼°å¤±è´¥: {e}")
            raise
    
    def save_final_results(self, results: List[Dict]):
        """
        ä¿å­˜MATH-500æœ€ç»ˆç»“æœ
        
        Args:
            results: è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        try:
            # åˆ›å»ºä¿å­˜ç›®å½•
            save_dir = Path(f"data/math500_results/{self.model_name}/{self.run_id}")
            save_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜JSONæ ¼å¼
            json_file = save_dir / "final_results.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜CSVæ ¼å¼
            csv_file = save_dir / "final_results.csv"
            df_results = pd.DataFrame(results)
            df_results.to_csv(csv_file, index=False, encoding='utf-8')
            
            self.logger.info(f"ğŸ’¾ MATH-500æœ€ç»ˆç»“æœå·²ä¿å­˜:")
            self.logger.info(f"   JSON: {json_file}")
            self.logger.info(f"   CSV: {csv_file}")
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜MATH-500æœ€ç»ˆç»“æœå¤±è´¥: {e}")
    
    def _check_existing_results(self) -> int:
        """
        æ£€æŸ¥ç°æœ‰ç»“æœï¼Œè¿”å›å·²å¤„ç†çš„æ ·æœ¬æ•°é‡
        
        Returns:
            å·²å¤„ç†çš„æ ·æœ¬æ•°é‡
        """
        try:
            save_dir = Path(f"data/math500_results/{self.model_name}/{self.run_id}")
            if not save_dir.exists():
                return 0
            
            # æŸ¥æ‰¾æœ€æ–°çš„ä¸­é—´ç»“æœæ–‡ä»¶
            intermediate_files = list(save_dir.glob("intermediate_results_*.json"))
            if not intermediate_files:
                return 0
            
            # æŒ‰æ–‡ä»¶åä¸­çš„æ•°å­—æ’åº
            latest_file = max(intermediate_files, key=lambda x: int(x.stem.split('_')[-1]))
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            return len(results)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ£€æŸ¥ç°æœ‰ç»“æœå¤±è´¥: {e}")
            return 0


def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description='MATH-500æ•°æ®é›†æ•°å­¦è¯„ä¼°æ¡†æ¶')
    parser.add_argument('-m', '--model', type=str, required=True,
                       help='æ¨¡å‹åç§° (deepseek_r1_1.5b, deepseek_r1_7b, deepseek_r1_14b, deepseek_r1_32b, deepseek_r1_70b, qwen25_0.5b, qwen25_1.5b, qwen25_3b, qwen25_7b, qwen25_14b, qwen25_32b, qwen25_72b)')
    parser.add_argument('-d', '--dataset', type=str, required=True,
                       help='MATH-500æ•°æ®é›†è·¯å¾„')
    parser.add_argument('-s', '--samples', type=int, default=None,
                       help='æœ€å¤§æ ·æœ¬æ•°é‡ (é»˜è®¤: å…¨éƒ¨)')
    parser.add_argument('--hf_token', type=str, default=None,
                       help='Hugging Face API Token (ç”¨äºè¿œç¨‹æ¨¡å‹)')
    
    args = parser.parse_args()
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    if args.hf_token:
        os.environ['HF_TOKEN'] = args.hf_token
    
    # éªŒè¯æ•°æ®é›†è·¯å¾„
    if not os.path.exists(args.dataset):
        print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {args.dataset}")
        sys.exit(1)
    
    try:
        # åˆ›å»ºè¯„ä¼°æ¡†æ¶
        framework = Math500EvaluationFramework(
            model_name=args.model,
            dataset_path=args.dataset
        )
        
        # è¿è¡Œè¯„ä¼°
        results = framework.run_evaluation(max_samples=args.samples)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š MATH-500è¯„ä¼°ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(results)}")
        
        # è®¡ç®—æˆåŠŸç‡
        successful = sum(1 for r in results if not r.get('evaluation', {}).get('error', False))
        success_rate = successful / len(results) * 100
        print(f"   æˆåŠŸç‡: {success_rate:.1f}% ({successful}/{len(results)})")
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        scores = [r.get('evaluation', {}).get('overall_score', 0) for r in results 
                 if not r.get('evaluation', {}).get('error', False)]
        if scores:
            avg_score = np.mean(scores)
            print(f"   å¹³å‡åˆ†æ•°: {avg_score:.2f}/10")
        
        print(f"âœ… MATH-500è¯„ä¼°å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ MATH-500è¯„ä¼°å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 