#!/usr/bin/env python3
"""
æ¨¡å‹è¯„ä¼°è„šæœ¬ï¼šè¯„ä¼°ä¸åŒå‚æ•°çš„Llamaæ¨¡å‹åœ¨æ•°å­¦é¢˜ä¸Šçš„è¡¨ç°
"""

import os
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    BitsAndBytesConfig,
    pipeline
)
import yaml
from tqdm import tqdm
import sys
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from scripts.utils import ConfigLoader, setup_logging, calculate_metrics
from scripts.openai_scorer import OpenAIScorer

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.config = ConfigLoader.load_config(config_path)
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # è®¾å¤‡é…ç½®
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        self.model_name = None
        
        # åˆå§‹åŒ–OpenAIè¯„åˆ†å™¨
        self.openai_scorer = None
        if self.config.get('openai_scoring', {}).get('enabled', False):
            try:
                self.openai_scorer = OpenAIScorer(config_path)
                self.logger.info("OpenAIè¯„åˆ†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"OpenAIè¯„åˆ†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.openai_scorer = None
    
    def load_model(self, model_name: str, quantization: str = "4bit", num_gpus: int = None):
        """åŠ è½½æ¨¡å‹"""
        # å¯¼å…¥é…ç½®ç®¡ç†å™¨
        from scripts.model_config_manager import ModelConfigManager
        
        # åˆ›å»ºé…ç½®ç®¡ç†å™¨
        config_manager = ModelConfigManager()
        
        try:
            # è·å–æ¨¡å‹é…ç½®
            model_config = config_manager.get_model_config(model_name)
            gpu_config = config_manager.get_gpu_config(model_name)
            quant_config = config_manager.get_quantization_config(model_name, quantization)
            generation_config = config_manager.get_generation_config(model_name)
            model_specific_config = config_manager.get_model_specific_config(model_name)
            
            # ç¡®å®šGPUæ•°é‡
            if num_gpus is None:
                num_gpus = gpu_config.get('num_gpus', 1)
            
            # ä¿å­˜æ¨¡å‹åç§°
            self.model_name = model_name
            
            self.logger.info(f"åŠ è½½æ¨¡å‹: {model_name}")
            self.logger.info(f"æ˜¾ç¤ºåç§°: {model_config['model']['display_name']}")
            self.logger.info(f"GPUæ•°é‡: {num_gpus}, é‡åŒ–: {quantization}")
            
            # æ£€æŸ¥GPUæ•°é‡
            if torch.cuda.is_available():
                available_gpus = torch.cuda.device_count()
                if num_gpus > available_gpus:
                    self.logger.warning(f"è¯·æ±‚çš„GPUæ•°é‡({num_gpus})è¶…è¿‡å¯ç”¨æ•°é‡({available_gpus})ï¼Œä½¿ç”¨å¯ç”¨GPUæ•°é‡")
                    num_gpus = available_gpus
            else:
                num_gpus = 0
                self.logger.warning("CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # é…ç½®é‡åŒ–å‚æ•°
            quantization_config = None
            if quantization != "none" and quant_config:
                quantization_config = BitsAndBytesConfig(**quant_config)
            
            # é…ç½®è®¾å¤‡æ˜ å°„
            device_map = gpu_config.get('device_map', 'auto')
            if num_gpus > 1:
                # å¤šGPUé…ç½®
                max_memory = gpu_config.get('max_memory', {})
                self.logger.info(f"ä½¿ç”¨å¤šGPUé…ç½®ï¼Œè®¾å¤‡æ˜ å°„: {device_map}")
                self.logger.info(f"å†…å­˜é…ç½®: {max_memory}")
            else:
                # å•GPUé…ç½®
                device_map = device_map if self.device == "cuda" else None
            
            # åŠ è½½æ¨¡å‹
            model_kwargs = {
                'quantization_config': quantization_config,
                'device_map': device_map,
                'torch_dtype': torch.float16 if self.device == "cuda" else torch.float32,
                'trust_remote_code': model_specific_config.get('trust_remote_code', True),
                'low_cpu_mem_usage': model_specific_config.get('low_cpu_mem_usage', True)
            }
            
            # æ·»åŠ å¤šGPUå†…å­˜é…ç½®
            if num_gpus > 1 and 'max_memory' in gpu_config:
                model_kwargs['max_memory'] = gpu_config['max_memory']
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                **model_kwargs
            )
            
            # å‡†å¤‡pipelineå‚æ•°
            pipeline_kwargs = {
                'model': self.model,
                'tokenizer': self.tokenizer,
                'max_new_tokens': generation_config.get('max_new_tokens', 128),
                'do_sample': generation_config.get('do_sample', True),
                'temperature': generation_config.get('temperature', 0.7),
                'top_p': generation_config.get('top_p', 0.9),
                'top_k': generation_config.get('top_k', 50),
                'repetition_penalty': generation_config.get('repetition_penalty', 1.1),
                'pad_token_id': self.tokenizer.eos_token_id,
                'eos_token_id': self.tokenizer.eos_token_id,
                'return_full_text': generation_config.get('return_full_text', False)
            }
            
            # åˆ›å»ºpipeline - ä½¿ç”¨æ›´ç®€å•çš„æ–¹å¼
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device_map="auto" if num_gpus > 1 else None
            )
            
            self.logger.info(f"æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ (GPUæ•°é‡: {num_gpus})")
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def load_dataset(self, dataset_name: str) -> pd.DataFrame:
        """åŠ è½½æ•°æ®é›†"""
        self.logger.info(f"åŠ è½½æ•°æ®é›†: {dataset_name}")
        
        data_path = Path("data/processed") / f"{dataset_name}.csv"
        if not data_path.exists():
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        
        df = pd.read_csv(data_path)
        self.logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(df)} ä¸ªæ ·æœ¬")
        return df
    
    def generate_answer(self, problem: str, prompt_template: str = None) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        try:
            # å¦‚æœæ²¡æœ‰æä¾›æç¤ºæ¨¡æ¿ï¼Œä½¿ç”¨æ¨¡å‹é…ç½®ä¸­çš„æ¨¡æ¿
            if prompt_template is None:
                from scripts.model_config_manager import ModelConfigManager
                config_manager = ModelConfigManager()
                prompt_template = config_manager.get_prompt_template(self.model_name)
            
            # æ„å»ºæç¤º
            prompt = prompt_template.format(problem=problem)
            
            # ä½¿ç”¨æ›´ç¨³å®šçš„ç”Ÿæˆå‚æ•°ï¼ˆå‚è€ƒCacheGené¡¹ç›®ï¼‰
            generation_kwargs = {
                'max_new_tokens': 128,
                'do_sample': False,  # ä½¿ç”¨ç¡®å®šæ€§ç”Ÿæˆ
                'num_beams': 1,      # ä½¿ç”¨è´ªå©ªæœç´¢
                'pad_token_id': self.tokenizer.eos_token_id,
                'eos_token_id': self.tokenizer.eos_token_id
            }
            
            # ç”Ÿæˆå›ç­”
            response = self.pipeline(prompt, **generation_kwargs)
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            if isinstance(response, list) and len(response) > 0:
                generated_text = response[0].get('generated_text', '')
                if generated_text:
                    # ç§»é™¤åŸå§‹æç¤ºï¼Œåªä¿ç•™æ–°ç”Ÿæˆçš„éƒ¨åˆ†
                    if prompt in generated_text:
                        answer = generated_text[len(prompt):].strip()
                    else:
                        answer = generated_text.strip()
                else:
                    answer = "ç”Ÿæˆå¤±è´¥ï¼šæ— è¾“å‡º"
            else:
                answer = "ç”Ÿæˆå¤±è´¥ï¼šå“åº”æ ¼å¼é”™è¯¯"
            
            return answer
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def evaluate_dataset(self, dataset_name: str, max_samples: Optional[int] = None) -> Dict[str, Any]:
        """è¯„ä¼°æ•°æ®é›†"""
        self.logger.info(f"å¼€å§‹è¯„ä¼°æ•°æ®é›†: {dataset_name}")
        
        # åŠ è½½æ•°æ®é›†
        df = self.load_dataset(dataset_name)
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if max_samples and len(df) > max_samples:
            df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)
            self.logger.info(f"é™åˆ¶æ ·æœ¬æ•°é‡ä¸º: {max_samples}")
        
        # è·å–æ•°æ®é›†é…ç½®
        dataset_config = self.config['datasets'].get(dataset_name, {})
        prompt_template = dataset_config.get('prompt_template', "Problem: {problem}\n\nSolution:")
        
        results = []
        
        # æŒ‰éš¾åº¦åˆ†ç»„è¯„ä¼°
        for difficulty in ['elementary', 'middle', 'college']:
            difficulty_df = df[df['difficulty'] == difficulty]
            
            if len(difficulty_df) == 0:
                self.logger.warning(f"éš¾åº¦ç­‰çº§ {difficulty} æ²¡æœ‰æ•°æ®")
                continue
            
            self.logger.info(f"è¯„ä¼°éš¾åº¦ç­‰çº§: {difficulty}, æ ·æœ¬æ•°: {len(difficulty_df)}")
            
            difficulty_results = []
            
            for idx, row in tqdm(difficulty_df.iterrows(), total=len(difficulty_df), desc=f"è¯„ä¼° {difficulty}"):
                problem = row['problem']
                expected_answer = row.get('solution', '')
                
                # ç”Ÿæˆç­”æ¡ˆ
                start_time = time.time()
                generated_answer = self.generate_answer(problem, prompt_template)
                generation_time = time.time() - start_time
                
                # è®¡ç®—åŸºç¡€æŒ‡æ ‡
                metrics = calculate_metrics(generated_answer, expected_answer)
                
                # æ·»åŠ OpenAIè¯„åˆ†
                if self.openai_scorer:
                    try:
                        openai_result = self.openai_scorer.score_answer(
                            problem=problem,
                            reference_answer=expected_answer,
                            student_answer=generated_answer
                        )
                        metrics['openai_score'] = openai_result['openai_score']
                        metrics['openai_score_text'] = openai_result.get('score_text', '')
                    except Exception as e:
                        self.logger.warning(f"OpenAIè¯„åˆ†å¤±è´¥: {e}")
                        metrics['openai_score'] = 50.0
                        metrics['openai_score_text'] = f"è¯„åˆ†å¤±è´¥: {str(e)}"
                else:
                    metrics['openai_score'] = 50.0
                    metrics['openai_score_text'] = "OpenAIè¯„åˆ†æœªå¯ç”¨"
                
                result = {
                    'id': row.get('id', idx),
                    'problem': problem,
                    'expected_answer': expected_answer,
                    'generated_answer': generated_answer,
                    'difficulty': difficulty,
                    'generation_time': generation_time,
                    **metrics
                }
                
                difficulty_results.append(result)
            
            # è®¡ç®—è¯¥éš¾åº¦ç­‰çº§çš„å¹³å‡æŒ‡æ ‡
            if difficulty_results:
                avg_metrics = {}
                for key in ['accuracy', 'exact_match', 'rouge_score', 'bleu_score']:
                    if key in difficulty_results[0]:
                        values = [r[key] for r in difficulty_results if key in r]
                        avg_metrics[f'avg_{key}'] = sum(values) / len(values) if values else 0
                
                self.logger.info(f"éš¾åº¦ {difficulty} å¹³å‡æŒ‡æ ‡: {avg_metrics}")
                results.extend(difficulty_results)
        
        return results
    
    def save_results(self, results: List[Dict], model_name: str, dataset_name: str):
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = self.results_dir / f"{model_name}_{dataset_name}_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜CSVæ ¼å¼
        df_results = pd.DataFrame(results)
        csv_file = self.results_dir / f"{model_name}_{dataset_name}_{timestamp}.csv"
        df_results.to_csv(csv_file, index=False, encoding='utf-8')
        
        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        summary = self.generate_summary(results, model_name, dataset_name)
        summary_file = self.results_dir / f"{model_name}_{dataset_name}_{timestamp}_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {self.results_dir}")
        return summary
    
    def generate_summary(self, results: List[Dict], model_name: str, dataset_name: str) -> Dict[str, Any]:
        """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
        if not results:
            return {}
        
        df = pd.DataFrame(results)
        
        summary = {
            'model_name': model_name,
            'dataset_name': dataset_name,
            'total_samples': len(results),
            'evaluation_time': datetime.now().isoformat(),
            'overall_metrics': {},
            'difficulty_metrics': {}
        }
        
        # æ€»ä½“æŒ‡æ ‡
        for metric in ['accuracy', 'exact_match', 'rouge_score', 'bleu_score']:
            if metric in df.columns:
                summary['overall_metrics'][metric] = df[metric].mean()
        
        # æŒ‰éš¾åº¦åˆ†ç»„çš„æŒ‡æ ‡
        for difficulty in df['difficulty'].unique():
            difficulty_df = df[df['difficulty'] == difficulty]
            summary['difficulty_metrics'][difficulty] = {
                'sample_count': len(difficulty_df),
                'avg_generation_time': difficulty_df['generation_time'].mean()
            }
            
            for metric in ['accuracy', 'exact_match', 'rouge_score', 'bleu_score']:
                if metric in difficulty_df.columns:
                    summary['difficulty_metrics'][difficulty][f'avg_{metric}'] = difficulty_df[metric].mean()
        
        return summary

def main():
    parser = argparse.ArgumentParser(description="æ¨¡å‹è¯„ä¼°è„šæœ¬")
    parser.add_argument("--model", type=str, default="mistral-7b",
                       choices=["mistral-7b", "longalpaca-7b"],
                       help="è¦è¯„ä¼°çš„æ¨¡å‹ï¼ˆå¿«é€Ÿæµ‹è¯•ç‰ˆæœ¬ï¼‰")
    parser.add_argument("--quantization", type=str, default="4bit",
                       choices=["none", "4bit", "8bit"],
                       help="é‡åŒ–æ–¹å¼")
    parser.add_argument("--dataset", type=str, default="sample",
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--max-samples", type=int, default=None,
                       help="æœ€å¤§æ ·æœ¬æ•°é‡")
    parser.add_argument("--config", type=str, default="configs/config.yaml",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = ModelEvaluator(args.config)
    
    try:
        # åŠ è½½æ¨¡å‹
        evaluator.load_model(args.model, args.quantization)
        
        # è¯„ä¼°æ•°æ®é›†
        results = evaluator.evaluate_dataset(args.dataset, args.max_samples)
        
        # ä¿å­˜ç»“æœ
        summary = evaluator.save_results(results, args.model, args.dataset)
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
        print("="*60)
        print(f"æ¨¡å‹: {summary['model_name']}")
        print(f"æ•°æ®é›†: {summary['dataset_name']}")
        print(f"æ€»æ ·æœ¬æ•°: {summary['total_samples']}")
        
        print("\nğŸ“ˆ æ€»ä½“æŒ‡æ ‡:")
        for metric, value in summary['overall_metrics'].items():
            print(f"  {metric}: {value:.4f}")
        
        print("\nğŸ¯ å„éš¾åº¦ç­‰çº§æŒ‡æ ‡:")
        for difficulty, metrics in summary['difficulty_metrics'].items():
            print(f"\n  {difficulty.upper()}:")
            print(f"    æ ·æœ¬æ•°: {metrics['sample_count']}")
            print(f"    å¹³å‡ç”Ÿæˆæ—¶é—´: {metrics['avg_generation_time']:.2f}ç§’")
            for key, value in metrics.items():
                if key.startswith('avg_') and key != 'avg_generation_time':
                    print(f"    {key}: {value:.4f}")
        
        print("\nâœ… è¯„ä¼°å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 