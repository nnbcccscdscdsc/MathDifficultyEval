#!/usr/bin/env python3
"""
ç»Ÿä¸€æ¨¡å‹è¯„ä¼°è°ƒåº¦è„šæœ¬

ä½¿ç”¨æ–¹æ³•ï¼š
python scripts/evaluate_all_models.py --models mistral-community/Mistral-7B-v0.2 lmsys/longchat-7b-16k --dataset deepmath_evaluation_dataset --max-samples 100
"""

import os
import json
import argparse
import logging
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
import time
from datetime import datetime
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from scripts.utils import ConfigLoader, setup_logging

class MultiModelEvaluator:
    """å¤šæ¨¡å‹è¯„ä¼°è°ƒåº¦å™¨"""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        """åˆå§‹åŒ–è°ƒåº¦å™¨"""
        self.config = ConfigLoader.load_config(config_path)
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ï¼ˆæ›´æ–°ä¸ºå®é™…è¦æµ‹è¯•çš„æ¨¡å‹ï¼‰
        self.supported_models = [
            "mistral-community/Mistral-7B-v0.2",
            "lmsys/longchat-7b-16k", 
            "Yukang/LongAlpaca-13B-16k",
            "Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-hf",
            "Yukang/LongAlpaca-70B-16k"
        ]
        
        # æ¨¡å‹GPUé…ç½®
        self.model_gpu_config = {
            "mistral-community/Mistral-7B-v0.2": 1,
            "lmsys/longchat-7b-16k": 1,
            "Yukang/LongAlpaca-13B-16k": 2,
            "Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-hf": 4,
            "Yukang/LongAlpaca-70B-16k": 4
        }
    
    def evaluate_models_sequential(self, models: List[str], dataset: str, 
                                 quantization: str = "4bit", max_samples: Optional[int] = None):
        """ä¸²è¡Œè¯„ä¼°å¤šä¸ªæ¨¡å‹"""
        self.logger.info(f"å¼€å§‹ä¸²è¡Œè¯„ä¼°æ¨¡å‹: {models}")
        
        results = []
        failed_models = []
        
        for i, model_name in enumerate(models, 1):
            self.logger.info(f"è¯„ä¼°è¿›åº¦: {i}/{len(models)} - {model_name}")
            
            try:
                # è°ƒç”¨å•ä¸ªæ¨¡å‹è¯„ä¼°è„šæœ¬
                result = self.run_single_model_evaluation(
                    model_name, dataset, quantization, max_samples
                )
                
                if result:
                    results.append(result)
                    self.logger.info(f"âœ… æ¨¡å‹ {model_name} è¯„ä¼°æˆåŠŸ")
                else:
                    failed_models.append(model_name)
                    self.logger.error(f"âŒ æ¨¡å‹ {model_name} è¯„ä¼°å¤±è´¥")
                
                # ç­‰å¾…ä¸€ä¸‹ï¼Œç¡®ä¿GPUèµ„æºé‡Šæ”¾
                time.sleep(10)  # å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿å¤§æ¨¡å‹å®Œå…¨é‡Šæ”¾
                
            except Exception as e:
                self.logger.error(f"è¯„ä¼°æ¨¡å‹ {model_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                failed_models.append(model_name)
                continue
        
        return results, failed_models
    
    def evaluate_models_parallel(self, models: List[str], dataset: str,
                               quantization: str = "4bit", max_samples: Optional[int] = None):
        """å¹¶è¡Œè¯„ä¼°å¤šä¸ªæ¨¡å‹ï¼ˆéœ€è¦å¤šä¸ªGPUï¼‰"""
        self.logger.info(f"å¼€å§‹å¹¶è¡Œè¯„ä¼°æ¨¡å‹: {models}")
        
        # æ£€æŸ¥GPUæ•°é‡
        import torch
        gpu_count = torch.cuda.device_count()
        
        # è®¡ç®—éœ€è¦çš„æ€»GPUæ•°é‡
        total_gpus_needed = sum(self.model_gpu_config.get(model, 1) for model in models)
        
        if gpu_count < total_gpus_needed:
            self.logger.warning(f"GPUæ•°é‡({gpu_count})å°‘äºæ‰€éœ€æ•°é‡({total_gpus_needed})ï¼Œå°†ä¸²è¡Œè¯„ä¼°")
            return self.evaluate_models_sequential(models, dataset, quantization, max_samples)
        
        # å¯åŠ¨å¹¶è¡Œè¿›ç¨‹
        processes = []
        current_gpu = 0
        
        for model_name in models:
            num_gpus = self.model_gpu_config.get(model_name, 1)
            
            cmd = [
                sys.executable, "scripts/evaluate_single_model.py",
                "--model", model_name,
                "--dataset", dataset,
                "--quantization", quantization,
                "--num-gpus", str(num_gpus)
            ]
            
            if max_samples:
                cmd.extend(["--max-samples", str(max_samples)])
            
            # è®¾ç½®ç¯å¢ƒå˜é‡æŒ‡å®šGPU
            env = os.environ.copy()
            gpu_list = ",".join(str(i) for i in range(current_gpu, current_gpu + num_gpus))
            env["CUDA_VISIBLE_DEVICES"] = gpu_list
            
            process = subprocess.Popen(cmd, env=env)
            processes.append((model_name, process))
            
            current_gpu += num_gpus
        
        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
        results = []
        failed_models = []
        
        for model_name, process in processes:
            try:
                process.wait()
                if process.returncode == 0:
                    self.logger.info(f"âœ… æ¨¡å‹ {model_name} è¯„ä¼°æˆåŠŸ")
                    # è¿™é‡Œå¯ä»¥è¯»å–ç»“æœæ–‡ä»¶
                else:
                    self.logger.error(f"âŒ æ¨¡å‹ {model_name} è¯„ä¼°å¤±è´¥")
                    failed_models.append(model_name)
            except Exception as e:
                self.logger.error(f"ç­‰å¾…æ¨¡å‹ {model_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                failed_models.append(model_name)
        
        return results, failed_models
    
    def run_single_model_evaluation(self, model_name: str, dataset: str,
                                  quantization: str, max_samples: Optional[int] = None):
        """è¿è¡Œå•ä¸ªæ¨¡å‹è¯„ä¼°"""
        num_gpus = self.model_gpu_config.get(model_name, 1)
        
        cmd = [
            sys.executable, "scripts/evaluate_single_model.py",
            "--model", model_name,
            "--dataset", dataset,
            "--quantization", quantization,
            "--num-gpus", str(num_gpus)
        ]
        
        if max_samples:
            cmd.extend(["--max-samples", str(max_samples)])
        
        try:
            # è¿è¡Œå­è¿›ç¨‹
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=7200)  # 2å°æ—¶è¶…æ—¶
            
            if result.returncode == 0:
                self.logger.info(f"æ¨¡å‹ {model_name} è¯„ä¼°å®Œæˆ")
                return True
            else:
                self.logger.error(f"æ¨¡å‹ {model_name} è¯„ä¼°å¤±è´¥: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            self.logger.error(f"æ¨¡å‹ {model_name} è¯„ä¼°è¶…æ—¶")
            return False
        except Exception as e:
            self.logger.error(f"è¿è¡Œæ¨¡å‹ {model_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def generate_summary_report(self, results: List[Dict], failed_models: List[str]):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ”¶é›†æ‰€æœ‰ç»“æœæ–‡ä»¶
        all_results = []
        for result in results:
            if result:
                all_results.append(result)
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        report = f"""
# å¤šæ¨¡å‹è¯„ä¼°æ±‡æ€»æŠ¥å‘Š

## è¯„ä¼°æ¦‚è§ˆ
- è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- æˆåŠŸè¯„ä¼°æ¨¡å‹: {len(all_results)} ä¸ª
- å¤±è´¥æ¨¡å‹: {len(failed_models)} ä¸ª
- å¤±è´¥æ¨¡å‹åˆ—è¡¨: {', '.join(failed_models) if failed_models else 'æ— '}

## å„æ¨¡å‹æ€§èƒ½å¯¹æ¯”
"""
        
        if all_results:
            # æŒ‰OpenAIè¯„åˆ†æ’åº
            all_results.sort(key=lambda x: x.get('avg_openai_score', 0), reverse=True)
            
            for i, result in enumerate(all_results, 1):
                report += f"""
### {i}. {result['model_name']}
- æ ·æœ¬æ•°: {result['total_samples']}
- GPUæ•°é‡: {result['num_gpus']}
- å¹³å‡OpenAIè¯„åˆ†: {result['avg_openai_score']:.2f}
- å¹³å‡å‡†ç¡®ç‡: {result['avg_accuracy']:.4f}
- è¯„ä¼°æ—¶é—´: {result['timestamp']}
- ç»“æœæ–‡ä»¶: {result['results_file']}
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.results_dir / f"multi_model_summary_{timestamp}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.logger.info(f"æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return str(report_file)
    
    def run_evaluation(self, models: List[str], dataset: str, 
                      quantization: str = "4bit", max_samples: Optional[int] = None,
                      parallel: bool = False):
        """è¿è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
        self.logger.info("å¼€å§‹å¤šæ¨¡å‹è¯„ä¼°æµç¨‹")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒ
        unsupported_models = [m for m in models if m not in self.supported_models]
        if unsupported_models:
            self.logger.error(f"ä¸æ”¯æŒçš„æ¨¡å‹: {unsupported_models}")
            return
        
        # é€‰æ‹©è¯„ä¼°æ–¹å¼
        if parallel:
            results, failed_models = self.evaluate_models_parallel(
                models, dataset, quantization, max_samples
            )
        else:
            results, failed_models = self.evaluate_models_sequential(
                models, dataset, quantization, max_samples
            )
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        report_file = self.generate_summary_report(results, failed_models)
        
        # æ‰“å°æœ€ç»ˆæ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ‰ å¤šæ¨¡å‹è¯„ä¼°å®Œæˆï¼")
        print("="*60)
        print(f"æˆåŠŸè¯„ä¼°: {len(results)} ä¸ªæ¨¡å‹")
        print(f"å¤±è´¥æ¨¡å‹: {len(failed_models)} ä¸ª")
        
        if failed_models:
            print(f"å¤±è´¥æ¨¡å‹: {', '.join(failed_models)}")
        
        print(f"æ±‡æ€»æŠ¥å‘Š: {report_file}")
        print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¤šæ¨¡å‹è¯„ä¼°è°ƒåº¦è„šæœ¬")
    parser.add_argument("--models", nargs="+", required=True,
                       choices=[
                           "mistral-community/Mistral-7B-v0.2",
                           "lmsys/longchat-7b-16k", 
                           "Yukang/LongAlpaca-13B-16k",
                           "Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-hf",
                           "Yukang/LongAlpaca-70B-16k"
                       ],
                       help="è¦è¯„ä¼°çš„æ¨¡å‹åˆ—è¡¨")
    parser.add_argument("--dataset", type=str, default="deepmath_evaluation_dataset",
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--quantization", type=str, default="4bit",
                       choices=["none", "4bit", "8bit"],
                       help="é‡åŒ–æ–¹å¼")
    parser.add_argument("--max-samples", type=int, default=None,
                       help="æ¯ä¸ªæ¨¡å‹çš„æœ€å¤§æ ·æœ¬æ•°é‡")
    parser.add_argument("--parallel", action="store_true",
                       help="å¹¶è¡Œè¯„ä¼°ï¼ˆéœ€è¦å¤šä¸ªGPUï¼‰")
    parser.add_argument("--config", type=str, default="configs/config.yaml",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè°ƒåº¦å™¨
    evaluator = MultiModelEvaluator(args.config)
    
    try:
        # è¿è¡Œè¯„ä¼°
        evaluator.run_evaluation(
            models=args.models,
            dataset=args.dataset,
            quantization=args.quantization,
            max_samples=args.max_samples,
            parallel=args.parallel
        )
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 