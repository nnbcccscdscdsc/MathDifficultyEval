#!/usr/bin/env python3
"""
MATH-500æ•°æ®é›†ä¸“ç”¨è¯„ä¼°æ¡†æ¶
ä¸“é—¨å¤„ç†MATH-500æ•°æ®é›†çš„æ•°å­¦è¯„ä¼°ï¼ŒåŒ…å«æœ¬åœ°å’Œè¿œç¨‹æ¨¡å‹æ”¯æŒ
"""

import os
import sys
import json
import logging
import torch
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import openai

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Math500EvaluationFramework:
    """MATH-500æ•°æ®é›†ä¸“ç”¨è¯„ä¼°æ¡†æ¶"""
    
    def __init__(self, model_config: Dict[str, Any], openai_api_key: str = None, max_samples: int = 200):
        """
        åˆå§‹åŒ–MATH-500è¯„ä¼°æ¡†æ¶
        
        Args:
            model_config: æ¨¡å‹é…ç½®å­—å…¸
            openai_api_key: OpenAI APIå¯†é’¥
            max_samples: æœ€å¤§æ ·æœ¬æ•°é‡
        """
        self.model_config = model_config
        self.model_name = model_config['name']
        self.model_type = model_config.get('type', 'unknown')
        self.max_samples = max_samples
        self.openai_api_key = openai_api_key
        
        # ç”Ÿæˆè¿è¡Œæ ‡è¯†ç¬¦
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        import random
        random_suffix = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=4))
        self.run_id = f"{timestamp}_{random_suffix}"
        logger.info(f"ğŸ†” æœ¬æ¬¡è¿è¡ŒID: {self.run_id}")
        
        # åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer
        self.model = None
        self.tokenizer = None
        
        # è®¾ç½®OpenAIå®¢æˆ·ç«¯
        if openai_api_key:
            openai.api_key = openai_api_key
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        logger.info(f"ğŸ§® åŠ è½½æ¨¡å‹: {self.model_name}")
        
        try:
            # æ£€æŸ¥æ¨¡å‹ç¼“å­˜çŠ¶æ€
            if self._check_model_cache():
                logger.info("ğŸ“¦ ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹...")
                cache_path = self._load_model_from_cache()
                if cache_path:
                    self._load_model_from_path(cache_path, use_cache=True)
                else:
                    raise ValueError("æ— æ³•è·å–ç¼“å­˜è·¯å¾„")
            else:
                logger.info("ğŸ“¥ ä»Hugging Faceä¸‹è½½æ¨¡å‹...")
                self._load_model_from_path(self.model_name, use_cache=False)
            
            logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
            self._log_gpu_memory()
                
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _load_model_from_path(self, model_path: str, use_cache: bool = False):
        """ä»æŒ‡å®šè·¯å¾„åŠ è½½æ¨¡å‹"""
        # åŠ è½½tokenizer
        logger.info("åŠ è½½tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            local_files_only=use_cache
        )
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹ - æ ¹æ®æ¨¡å‹ç±»å‹ä½¿ç”¨ä¸åŒé…ç½®
        logger.info("åŠ è½½æ¨¡å‹æƒé‡...")
        
        # è®¡ç®—GPUå†…å­˜åˆ†é…ç­–ç•¥
        total_gpus = torch.cuda.device_count()
        logger.info(f"ğŸ–¥ï¸ æ£€æµ‹åˆ° {total_gpus} ä¸ªGPU")
        
        # ä¸ºä¸åŒå¤§å°çš„æ¨¡å‹ä½¿ç”¨æœ€ä¼˜çš„å¹¶è¡Œç­–ç•¥
        if self.model_type in ["32b_quantized", "70b_quantized"]:
            # å¤§æ¨¡å‹ï¼šä½¿ç”¨balanced_low_0ç­–ç•¥å®ç°çœŸæ­£çš„å¤šGPUå¹¶è¡Œ
            device_map = "balanced_low_0"
            logger.info(f"ğŸ“Š å¤§æ¨¡å‹ä½¿ç”¨balanced_low_0ç­–ç•¥å®ç°å¤šGPUå¹¶è¡Œ")
        else:
            # å°æ¨¡å‹ï¼šä½¿ç”¨autoç­–ç•¥
            device_map = "auto"
            logger.info("ğŸ“Š å°æ¨¡å‹ä½¿ç”¨autoç­–ç•¥")
        
        model_kwargs = {
            "torch_dtype": torch.float16,
            "device_map": device_map,
            "trust_remote_code": True,
            "low_cpu_mem_usage": True,
            "local_files_only": use_cache
        }
        
        # æ ¹æ®æ¨¡å‹ç±»å‹æ·»åŠ é‡åŒ–é…ç½®
        if self.model_type in ["7b_quantized", "14b_quantized", "32b_quantized", "70b_quantized"]:
            # ä½¿ç”¨4bité‡åŒ–
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16
            )
            model_kwargs["quantization_config"] = quantization_config
        
        self.model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
    
    def _check_model_cache(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¼“å­˜"""
        try:
            cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
            model_cache_path = os.path.join(cache_dir, f"models--{self.model_name.replace('/', '--')}")
            
            if os.path.exists(model_cache_path):
                # é¦–å…ˆæ£€æŸ¥ä¸»æ¨¡å‹ç›®å½•ï¼ˆæ¨¡å‹æ–‡ä»¶é€šå¸¸åœ¨è¿™é‡Œï¼‰
                if os.path.exists(os.path.join(model_cache_path, "model-00001-of-000017.safetensors")):
                    # æ£€æŸ¥åŸºæœ¬æ–‡ä»¶
                    basic_files = ["config.json", "tokenizer.json", "model.safetensors.index.json"]
                    missing_basic = []
                    for f in basic_files:
                        file_path = os.path.join(model_cache_path, f)
                        if not os.path.exists(file_path):
                            missing_basic.append(f)
                    
                    if not missing_basic:
                        logger.info(f"âœ… æ¨¡å‹å·²ç¼“å­˜: {self.model_name}")
                        return True
                    else:
                        logger.info(f"âš ï¸ æ¨¡å‹ç¼“å­˜ä¸å®Œæ•´ï¼Œç¼ºå°‘æ–‡ä»¶: {missing_basic}")
                        return False
                
                # å¦‚æœä¸»ç›®å½•æ²¡æœ‰ï¼Œå†æ£€æŸ¥snapshotsç›®å½•
                snapshots_dir = os.path.join(model_cache_path, "snapshots")
                if os.path.exists(snapshots_dir):
                    for snapshot in os.listdir(snapshots_dir):
                        snapshot_path = os.path.join(snapshots_dir, snapshot)
                        if os.path.isdir(snapshot_path):
                            # æ£€æŸ¥åŸºæœ¬æ–‡ä»¶ï¼ˆåŒ…æ‹¬ç¬¦å·é“¾æ¥ï¼‰
                            basic_files = ["config.json", "tokenizer.json"]
                            missing_basic = []
                            for f in basic_files:
                                file_path = os.path.join(snapshot_path, f)
                                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆåŒ…æ‹¬ç¬¦å·é“¾æ¥ï¼‰
                                if not (os.path.exists(file_path) or os.path.lexists(file_path)):
                                    missing_basic.append(f)
                            
                            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
                            model_files = ["model.safetensors", "model.safetensors.index.json"]
                            has_model_file = False
                            for f in model_files:
                                file_path = os.path.join(snapshot_path, f)
                                if os.path.exists(file_path) or os.path.lexists(file_path):
                                    has_model_file = True
                                    break
                            
                            if not missing_basic and has_model_file:
                                logger.info(f"âœ… æ¨¡å‹å·²ç¼“å­˜: {self.model_name}")
                                return True
                            else:
                                missing_files = missing_basic + ([] if has_model_file else ["model files"])
                                logger.info(f"âš ï¸ æ¨¡å‹ç¼“å­˜ä¸å®Œæ•´ï¼Œç¼ºå°‘æ–‡ä»¶: {missing_files}")
                                return False
            
            logger.info(f"âŒ æ¨¡å‹æœªç¼“å­˜: {self.model_name}")
            return False
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ£€æŸ¥ç¼“å­˜æ—¶å‡ºé”™: {e}")
            return False
    
    def _load_model_from_cache(self):
        """ç›´æ¥ä»ç¼“å­˜åŠ è½½æ¨¡å‹ï¼Œä¸è¿›è¡Œç½‘ç»œè¿æ¥"""
        try:
            cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
            model_cache_path = os.path.join(cache_dir, f"models--{self.model_name.replace('/', '--')}")
            
            if not os.path.exists(model_cache_path):
                raise ValueError(f"æ¨¡å‹ç¼“å­˜ä¸å­˜åœ¨: {model_cache_path}")
            
            # é¦–å…ˆå°è¯•ä»ä¸»æ¨¡å‹ç›®å½•åŠ è½½ï¼ˆæ¨¡å‹æ–‡ä»¶é€šå¸¸åœ¨è¿™é‡Œï¼‰
            if os.path.exists(os.path.join(model_cache_path, "model-00001-of-000017.safetensors")):
                logger.info(f"ğŸ“¦ ä»ä¸»æ¨¡å‹ç›®å½•åŠ è½½: {model_cache_path}")
                return model_cache_path
            
            # å¦‚æœä¸»ç›®å½•æ²¡æœ‰ï¼Œå†å°è¯•snapshotsç›®å½•
            snapshots_dir = os.path.join(model_cache_path, "snapshots")
            if not os.path.exists(snapshots_dir):
                raise ValueError(f"æ¨¡å‹å¿«ç…§ç›®å½•ä¸å­˜åœ¨: {snapshots_dir}")
            
            snapshots = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
            if not snapshots:
                raise ValueError(f"æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹å¿«ç…§")
            
            latest_snapshot = snapshots[-1]
            snapshot_path = os.path.join(snapshots_dir, latest_snapshot)
            
            logger.info(f"ğŸ“¦ ä»ç¼“å­˜è·¯å¾„åŠ è½½: {snapshot_path}")
            return snapshot_path
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç¼“å­˜è·¯å¾„å¤±è´¥: {e}")
            return None
    
    def _log_gpu_memory(self):
        """æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
                memory_reserved = torch.cuda.memory_reserved(i) / 1024**3
                logger.info(f"GPU {i}: å·²åˆ†é… {memory_allocated:.2f}GB, å·²ä¿ç•™ {memory_reserved:.2f}GB")
        else:
            logger.info("æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè¿è¡Œ")
    
    def load_dataset(self, dataset_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½MATH-500æ•°æ®é›†"""
        logger.info(f"ğŸ“Š åŠ è½½MATH-500æ•°æ®é›†: {dataset_path}")
        
        try:
            df = pd.read_csv(dataset_path)
            
            # éªŒè¯MATH-500æ•°æ®é›†æ ¼å¼
            required_columns = ['problem', 'solution', 'answer', 'subject', 'level', 'unique_id']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                raise ValueError(f"âŒ ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}")
            
            # æ£€æŸ¥éš¾åº¦èŒƒå›´ (MATH-500åªæœ‰1-5çº§)
            level_range = df['level'].dropna()
            if not all(level_range.isin([1, 2, 3, 4, 5])):
                invalid_levels = level_range[~level_range.isin([1, 2, 3, 4, 5])].unique()
                raise ValueError(f"âŒ å‘ç°æ— æ•ˆéš¾åº¦ç­‰çº§: {invalid_levels}ï¼ŒMATH-500åªæ”¯æŒ1-5çº§")
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            dataset = []
            for _, row in df.iterrows():
                sample = {
                    'id': row['unique_id'],  # ä½¿ç”¨unique_idä½œä¸ºid
                    'problem': row['problem'],
                    'solution': row['solution'],
                    'answer': row['answer'],
                    'difficulty': row['level'],  # ä½¿ç”¨levelä½œä¸ºdifficulty
                    'topic': row['subject'],     # ä½¿ç”¨subjectä½œä¸ºtopic
                    'difficulty_score': float(row['level'])  # ä½¿ç”¨levelä½œä¸ºdifficulty_score
                }
                dataset.append(sample)
            
            # é™åˆ¶æ ·æœ¬æ•°é‡
            dataset = dataset[:self.max_samples]
            logger.info(f"âœ… ä»MATH-500æ•°æ®é›†åŠ è½½ {len(dataset)} ä¸ªæ ·æœ¬")
            return dataset
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
            raise
    
    def generate_response(self, problem: str) -> str:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆå›ç­”"""
        try:
            # DeepSeek-R1æ¨èçš„æç¤ºæ ¼å¼
            prompt = f"<think>\nPlease reason step by step, and put your final answer within \\boxed{{}}.\n</think>\n\n{problem}\n\n<think>\n"
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
            
            # ç§»åŠ¨åˆ°GPU
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.model_config.get('max_new_tokens', 500),
                    temperature=0.1,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç è¾“å‡º
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥æç¤ºï¼‰
            generated_text = response.replace(prompt, "").strip()
            
            return generated_text
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def evaluate_with_openai(self, problem: str, model_response: str, correct_answer: str, standard_solution: str = "") -> Dict[str, Any]:
        """ä½¿ç”¨OpenAIè¯„ä¼°æ¨¡å‹å›ç­”"""
        if not self.openai_api_key:
            return {"error": "OpenAI APIå¯†é’¥æœªè®¾ç½®"}
        
        try:
            evaluation_prompt = f"""
You are a professional mathematical education evaluator. Please evaluate the quality of the answer to the following mathematical problem.

Problem: {problem}

Correct Answer: {correct_answer}

Standard Solution: {standard_solution}

Model Response: {model_response}

Please evaluate from the following aspects and give a score from 1 to 10:

1. Answer Correctness (1-10 points): Whether the final answer is correct
2. Reasoning Logic (1-10 points): Whether the reasoning process is clear and logical, compared to the standard solution
3. Step Completeness (1-10 points): Whether all necessary solution steps are shown, considering what the standard solution covers
4. Mathematical Accuracy (1-10 points): Whether mathematical calculations and formulas are accurate
5. Expression Clarity (1-10 points): Whether the expression is clear and easy to understand

IMPORTANT: You must respond with ONLY a valid JSON object. Do not include any other text, explanations, or markdown formatting.

Required JSON format:
{{
    "answer_correctness": <score>,
    "reasoning_logic": <score>,
    "step_completeness": <score>,
    "mathematical_accuracy": <score>,
    "expression_clarity": <score>,
    "overall_score": <average_of_all_scores>,
    "comments": "<brief_evaluation_comments>"
}}
"""
            
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a professional mathematical education evaluator."},
                    {"role": "user", "content": evaluation_prompt}
                ],
                temperature=0.3
            )
            
            response_content = response.choices[0].message.content
            
            # è§£æJSONå“åº”
            import re
            
            try:
                evaluation = json.loads(response_content)
                return evaluation
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
                
                # å°è¯•æå–JSONéƒ¨åˆ†
                try:
                    json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group()
                        evaluation = json.loads(json_str)
                        logger.info(f"âœ… æˆåŠŸæå–JSONéƒ¨åˆ†")
                        return evaluation
                except:
                    pass
                
                # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                try:
                    cleaned_response = response_content.replace('```json', '').replace('```', '').strip()
                    evaluation = json.loads(cleaned_response)
                    logger.info(f"âœ… æˆåŠŸä¿®å¤JSONæ ¼å¼")
                    return evaluation
                except:
                    pass
                
                return {
                    "raw_response": response_content,
                    "error": "JSON parsing failed",
                    "parse_error": str(e),
                    "error_type": "json_decode_error"
                }
                
        except Exception as e:
            logger.error(f"âŒ OpenAI evaluation failed: {e}")
            return {
                "error": f"Evaluation failed: {e}",
                "error_type": "openai_api_error",
                "exception": str(e)
            }
    
    def run_evaluation(self, dataset_path: str) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´MATH-500è¯„ä¼°æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹MATH-500è¯„ä¼°æµç¨‹")
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
        
        # åŠ è½½æ•°æ®é›†
        dataset = self.load_dataset(dataset_path)
        
        # æ£€æŸ¥å·²æœ‰ç»“æœï¼Œå®ç°æ–­ç‚¹ç»­ä¼ 
        start_index = self._check_existing_results()
        if start_index > 0:
            logger.info(f"ğŸ”„ å‘ç°å·²æœ‰ç»“æœï¼Œä»ç¬¬ {start_index + 1} ä¸ªæ ·æœ¬å¼€å§‹ç»§ç»­è¯„ä¼°")
            dataset = dataset[start_index:]
        else:
            logger.info(f"ğŸ†• å¼€å§‹å…¨æ–°è¯„ä¼°")
        
        results = []
        successful_generations = 0
        successful_evaluations = 0
        
        logger.info(f"ğŸ“ å¼€å§‹è¯„ä¼° {len(dataset)} ä¸ªæ ·æœ¬...")
        
        from tqdm import tqdm
        for i, sample in enumerate(tqdm(dataset, desc="MATH-500è¯„ä¼°è¿›åº¦")):
            global_index = start_index + i + 1
            logger.info(f"\n--- æ ·æœ¬ {global_index}/{len(dataset)}: {sample['id']} ---")
            
            # ç”Ÿæˆæ¨¡å‹å›ç­”
            model_response = self.generate_response(sample['problem'])
            
            if model_response and not model_response.startswith("ç”Ÿæˆå¤±è´¥"):
                successful_generations += 1
                
                # OpenAIè¯„ä¼°
                evaluation = self.evaluate_with_openai(
                    sample['problem'], 
                    model_response, 
                    sample['answer'],
                    sample['solution']
                )
                
                if isinstance(evaluation, dict) and "error" not in evaluation:
                    successful_evaluations += 1
                    logger.info(f"âœ… è¯„ä¼°å®Œæˆï¼Œæ€»åˆ†: {evaluation.get('overall_score', 0):.2f}")
                else:
                    # ä¸ºè¯„ä¼°å¤±è´¥çš„æ ·æœ¬åˆ›å»ºç‰¹æ®Šè¯„ä¼°
                    failed_evaluation = {
                        "answer_correctness": 0,
                        "reasoning_logic": 0,
                        "step_completeness": 0,
                        "mathematical_accuracy": 0,
                        "expression_clarity": 0,
                        "overall_score": 0,
                        "comments": f"è¯„ä¼°å¤±è´¥: {str(evaluation)}",
                        "error": "evaluation_failed"
                    }
                    evaluation = failed_evaluation
                
                # ä¿å­˜ç»“æœ
                result = {
                    "id": sample['id'],
                    "problem": sample['problem'],
                    "correct_answer": sample['answer'],
                    "standard_solution": sample['solution'],
                    "model_response": model_response,
                    "difficulty": sample['difficulty'],
                    "topic": sample['topic'],
                    "evaluation": evaluation,
                    "generation_status": "success"
                }
                results.append(result)
            else:
                logger.warning(f"âŒ ç”Ÿæˆå¤±è´¥: {model_response}")
                
                # ä¸ºç”Ÿæˆå¤±è´¥çš„æ ·æœ¬åˆ›å»ºç‰¹æ®Šè¯„ä¼°
                failed_evaluation = {
                    "answer_correctness": 0,
                    "reasoning_logic": 0,
                    "step_completeness": 0,
                    "mathematical_accuracy": 0,
                    "expression_clarity": 0,
                    "overall_score": 0,
                    "comments": f"ç”Ÿæˆå¤±è´¥: {model_response}",
                    "error": "generation_failed"
                }
                
                result = {
                    "id": sample['id'],
                    "problem": sample['problem'],
                    "correct_answer": sample['answer'],
                    "standard_solution": sample['solution'],
                    "model_response": model_response,
                    "difficulty": sample['difficulty'],
                    "topic": sample['topic'],
                    "evaluation": failed_evaluation,
                    "generation_status": "failed"
                }
                results.append(result)
            
            # æ¯10ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
            if (i + 1) % 10 == 0:
                global_count = start_index + i + 1
                self.save_intermediate_results(results, global_count)
                logger.info(f"ğŸ’¾ å·²å¤„ç† {global_count} ä¸ªæ ·æœ¬ï¼Œä¸­é—´ç»“æœå·²ä¿å­˜")
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        if results:
            scores = [r['evaluation'].get('overall_score', 0) for r in results if r.get('generation_status') == 'success' and 'overall_score' in r['evaluation']]
            avg_score = sum(scores) / len(scores) if scores else 0
            
            final_results = {
                "results": results,
                "summary": {
                    "total_evaluated": len(results),
                    "successful_generations": successful_generations,
                    "failed_generations": len(results) - successful_generations,
                    "generation_success_rate": successful_generations / len(results) if results else 0,
                    "evaluation_success_rate": successful_evaluations / len(results) if results else 0,
                    "average_overall_score": avg_score
                }
            }
            
            logger.info(f"\nğŸ“‹ MATH-500è¯„ä¼°æ‘˜è¦:")
            logger.info(f"æ€»è¯„ä¼°æ ·æœ¬: {len(results)}")
            logger.info(f"ç”ŸæˆæˆåŠŸç‡: {successful_generations / len(results) * 100:.1f}%")
            logger.info(f"å¹³å‡æ€»åˆ†: {avg_score:.2f}")
            
            # ä¿å­˜æœ€ç»ˆç»“æœ
            self.save_final_results(final_results)
            
            return final_results
        else:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆç»“æœ"}
    
    def save_intermediate_results(self, results: List[Dict], count: int):
        """ä¿å­˜MATH-500ä¸­é—´ç»“æœ"""
        # åˆ›å»ºMATH-500ä¸“ç”¨ç›®å½•
        model_safe_name = self.model_name.replace('/', '_').replace('-', '_')
        model_dir = f"data/math500_results/{model_safe_name}"
        os.makedirs(model_dir, exist_ok=True)
        
        # ä½¿ç”¨è¿è¡ŒIDåˆ›å»ºå­ç›®å½•
        run_dir = f"{model_dir}/{self.run_id}"
        os.makedirs(run_dir, exist_ok=True)
        
        filename = f"{run_dir}/intermediate_results_{count}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ MATH-500ä¸­é—´ç»“æœå·²ä¿å­˜: {filename}")
    
    def save_final_results(self, final_results: Dict[str, Any]):
        """ä¿å­˜MATH-500æœ€ç»ˆç»“æœ"""
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs("data/math500_results", exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_safe_name = self.model_name.replace('/', '_').replace('-', '_')
        filename = f"data/math500_results/final_math500_evaluation_{model_safe_name}_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ MATH-500æœ€ç»ˆç»“æœå·²ä¿å­˜: {filename}")
        
        # æ‰“å°æ‘˜è¦
        summary = final_results.get("summary", {})
        logger.info(f"\nğŸ“‹ MATH-500è¯„ä¼°æ‘˜è¦:")
        logger.info(f"æ€»è¯„ä¼°æ ·æœ¬: {summary.get('total_evaluated', 0)}")
        logger.info(f"ç”ŸæˆæˆåŠŸç‡: {summary.get('generation_success_rate', 0):.2%}")
        logger.info(f"å¹³å‡æ€»åˆ†: {summary.get('average_overall_score', 0):.2f}")
    
    def _check_existing_results(self):
        """æ£€æŸ¥å·²æœ‰ç»“æœï¼Œè¿”å›åº”è¯¥å¼€å§‹çš„æ ·æœ¬ç´¢å¼•"""
        # åˆ›å»ºMATH-500ä¸“ç”¨ç›®å½•
        model_safe_name = self.model_name.replace('/', '_').replace('-', '_')
        model_dir = f"data/math500_results/{model_safe_name}"
        
        if not os.path.exists(model_dir):
            return 0
        
        # æŸ¥æ‰¾æ‰€æœ‰è¿è¡Œç›®å½•
        import glob
        run_dirs = glob.glob(f"{model_dir}/*")
        if not run_dirs:
            return 0
        
        # æ‰¾åˆ°æœ€æ–°çš„è¿è¡Œç›®å½•
        latest_run_dir = max(run_dirs, key=os.path.getctime)
        
        # æŸ¥æ‰¾è¯¥è¿è¡Œç›®å½•ä¸‹çš„æ‰€æœ‰ä¸­é—´ç»“æœæ–‡ä»¶
        result_files = glob.glob(f"{latest_run_dir}/intermediate_results_*.json")
        if not result_files:
            return 0
        
        # æ‰¾åˆ°æœ€å¤§çš„æ ·æœ¬æ•°é‡
        max_count = 0
        for file_path in result_files:
            try:
                filename = os.path.basename(file_path)
                # æå–æ–‡ä»¶åä¸­çš„æ•°å­—ï¼Œå¦‚ intermediate_results_30.json -> 30
                count_str = filename.replace('intermediate_results_', '').replace('.json', '')
                count = int(count_str)
                max_count = max(max_count, count)
            except:
                continue
        
        logger.info(f"ğŸ“ å‘ç°å·²æœ‰ç»“æœç›®å½•: {latest_run_dir}")
        logger.info(f"ğŸ“Š å·²å¤„ç†æ ·æœ¬æ•°é‡: {max_count}")
        
        return max_count

# é¢„å®šä¹‰çš„æ¨¡å‹é…ç½®
MATH500_MODEL_CONFIGS = {
    "deepseek_r1_1.5b": {
        "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "type": "1.5b",
        "max_new_tokens": 500,
        "description": "DeepSeek-R1 1.5B æ¨¡å‹"
    },
    "deepseek_r1_7b": {
        "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B", 
        "type": "7b_quantized",
        "max_new_tokens": 600,
        "description": "DeepSeek-R1 7B æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰"
    },
    "deepseek_r1_14b": {
        "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        "type": "14b_quantized",
        "max_new_tokens": 700,
        "description": "DeepSeek-R1 14B æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰"
    },
    "deepseek_r1_32b": {
        "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
        "type": "32b_quantized",
        "max_new_tokens": 800,
        "description": "DeepSeek-R1 32B æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰"
    },
    "deepseek_r1_70b": {
        "name": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
        "type": "70b_quantized",
        "max_new_tokens": 1000,
        "description": "DeepSeek-R1 70B æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰"
    }
} 