#!/usr/bin/env python3
"""
ç»“æœåˆ†æè„šæœ¬ï¼šåˆ†æè¯„ä¼°ç»“æœå¹¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
"""

import os
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from datetime import datetime
import sys
import warnings

# ç¦ç”¨matplotlibå­—ä½“è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from scripts.utils import ConfigLoader, setup_logging

class ResultsAnalyzer:
    """ç»“æœåˆ†æå™¨"""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.config = ConfigLoader.load_config(config_path)
        self.results_dir = Path("results")
        self.plots_dir = self.results_dir / "plots"
        self.plots_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'Noto Sans CJK SC']
        plt.rcParams['axes.unicode_minus'] = False
        
        # è®¾ç½®seabornæ ·å¼
        sns.set_style("whitegrid")
        sns.set_palette("husl")
    
    def load_results(self, results_file: str) -> pd.DataFrame:
        """åŠ è½½ç»“æœæ–‡ä»¶"""
        file_path = self.results_dir / results_file
        
        if file_path.suffix == '.csv':
            df = pd.read_csv(file_path)
        elif file_path.suffix == '.json':
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            df = pd.DataFrame(data)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
        
        self.logger.info(f"åŠ è½½ç»“æœæ–‡ä»¶: {file_path}, å…± {len(df)} ä¸ªæ ·æœ¬")
        return df
    
    def analyze_accuracy_by_difficulty(self, df: pd.DataFrame, model_name: str):
        """åˆ†æä¸åŒéš¾åº¦ç­‰çº§çš„å‡†ç¡®ç‡"""
        self.logger.info("åˆ†æä¸åŒéš¾åº¦ç­‰çº§çš„å‡†ç¡®ç‡")
        
        # æŒ‰éš¾åº¦åˆ†ç»„è®¡ç®—å¹³å‡æŒ‡æ ‡
        difficulty_metrics = df.groupby('difficulty').agg({
            'accuracy': 'mean',
            'exact_match': 'mean',
            'rouge_score': 'mean',
            'bleu_score': 'mean',
            'openai_score': 'mean',
            'generation_time': 'mean'
        }).reset_index()
        
        # åˆ›å»ºæŸ±çŠ¶å›¾
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'{model_name} - ä¸åŒéš¾åº¦ç­‰çº§æ€§èƒ½åˆ†æ', fontsize=16, fontweight='bold')
        
        # å‡†ç¡®ç‡
        sns.barplot(data=difficulty_metrics, x='difficulty', y='accuracy', ax=axes[0,0])
        axes[0,0].set_title('Accuracy')
        axes[0,0].set_ylabel('Accuracy')
        axes[0,0].set_ylim(0, 1)
        
        # ç²¾ç¡®åŒ¹é…
        sns.barplot(data=difficulty_metrics, x='difficulty', y='exact_match', ax=axes[0,1])
        axes[0,1].set_title('Exact Match')
        axes[0,1].set_ylabel('Exact Match Rate')
        axes[0,1].set_ylim(0, 1)
        
        # OpenAIè¯„åˆ†
        sns.barplot(data=difficulty_metrics, x='difficulty', y='openai_score', ax=axes[0,2])
        axes[0,2].set_title('OpenAI Score')
        axes[0,2].set_ylabel('Score (0-100)')
        axes[0,2].set_ylim(0, 100)
        
        # ROUGEåˆ†æ•°
        sns.barplot(data=difficulty_metrics, x='difficulty', y='rouge_score', ax=axes[1,0])
        axes[1,0].set_title('ROUGEåˆ†æ•°')
        axes[1,0].set_ylabel('ROUGEåˆ†æ•°')
        axes[1,0].set_ylim(0, 1)
        
        # BLEUåˆ†æ•°
        sns.barplot(data=difficulty_metrics, x='difficulty', y='bleu_score', ax=axes[1,1])
        axes[1,1].set_title('BLEUåˆ†æ•°')
        axes[1,1].set_ylabel('BLEUåˆ†æ•°')
        axes[1,1].set_ylim(0, 1)
        
        # ç”Ÿæˆæ—¶é—´
        sns.barplot(data=difficulty_metrics, x='difficulty', y='generation_time', ax=axes[1,2])
        axes[1,2].set_title('å¹³å‡ç”Ÿæˆæ—¶é—´')
        axes[1,2].set_ylabel('æ—¶é—´ (ç§’)')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plot_file = self.plots_dir / f"{model_name}_difficulty_analysis.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"éš¾åº¦åˆ†æå›¾å·²ä¿å­˜: {plot_file}")
        return difficulty_metrics
    
    def plot_model_parameter_curves(self, model_results: Dict[str, pd.DataFrame]):
        """ç»˜åˆ¶æ¨¡å‹å‚æ•°ä¸æ€§èƒ½çš„å…³ç³»æ›²çº¿ï¼ˆç±»ä¼¼æ‰‹ç»˜å›¾ï¼‰"""
        self.logger.info("ç»˜åˆ¶æ¨¡å‹å‚æ•°ä¸æ€§èƒ½å…³ç³»æ›²çº¿")
        
        # æ¨¡å‹å‚æ•°æ˜ å°„
        model_params = {
            'llama-7b': 7,
            'llama-13b': 13,
            'llama-70b': 70
        }
        
        # æ”¶é›†æ•°æ®
        curve_data = []
        for model_name, df in model_results.items():
            if model_name in model_params:
                params = model_params[model_name]
                
                # è®¡ç®—å¹³å‡OpenAIè¯„åˆ†
                avg_openai_score = df['openai_score'].mean() if 'openai_score' in df.columns else 50.0
                
                # æŒ‰éš¾åº¦åˆ†ç»„
                for difficulty in ['elementary', 'middle', 'college']:
                    difficulty_df = df[df['difficulty'] == difficulty]
                    if len(difficulty_df) > 0:
                        difficulty_score = difficulty_df['openai_score'].mean() if 'openai_score' in difficulty_df.columns else 50.0
                        curve_data.append({
                            'model': model_name,
                            'parameters': params,
                            'difficulty': difficulty,
                            'score': difficulty_score,
                            'avg_score': avg_openai_score
                        })
        
        if not curve_data:
            self.logger.warning("æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç»˜åˆ¶æ›²çº¿")
            return
        
        curve_df = pd.DataFrame(curve_data)
        
        # åˆ›å»ºæ›²çº¿å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        
        # å›¾1ï¼šæ€»ä½“æ€§èƒ½æ›²çº¿
        ax1.set_xlabel('æ¨¡å‹å‚æ•° (Billion)', fontsize=12)
        ax1.set_ylabel('OpenAIè¯„åˆ†', fontsize=12)
        ax1.set_title('æ¨¡å‹å‚æ•°ä¸æ€§èƒ½å…³ç³»', fontsize=14, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        
        # ç»˜åˆ¶æ€»ä½“å¹³å‡åˆ†æ•°æ›²çº¿
        avg_scores = curve_df.groupby('parameters')['avg_score'].mean().reset_index()
        ax1.plot(avg_scores['parameters'], avg_scores['avg_score'], 
                'o-', linewidth=3, markersize=8, label='æ€»ä½“å¹³å‡', color='blue')
        
        # æ·»åŠ æ•°æ®ç‚¹æ ‡ç­¾
        for _, row in avg_scores.iterrows():
            ax1.annotate(f"{row['avg_score']:.1f}", 
                        (row['parameters'], row['avg_score']),
                        textcoords="offset points", xytext=(0,10), ha='center')
        
        # å›¾2ï¼šä¸åŒéš¾åº¦ç­‰çº§çš„æ›²çº¿
        ax2.set_xlabel('æ¨¡å‹å‚æ•° (Billion)', fontsize=12)
        ax2.set_ylabel('OpenAIè¯„åˆ†', fontsize=12)
        ax2.set_title('ä¸åŒéš¾åº¦ç­‰çº§çš„æ€§èƒ½æ›²çº¿', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
        difficulties = ['elementary', 'middle', 'college']
        
        for i, difficulty in enumerate(difficulties):
            diff_data = curve_df[curve_df['difficulty'] == difficulty]
            if len(diff_data) > 0:
                diff_scores = diff_data.groupby('parameters')['score'].mean().reset_index()
                ax2.plot(diff_scores['parameters'], diff_scores['score'], 
                        'o-', linewidth=3, markersize=8, 
                        label=difficulty.capitalize(), color=colors[i])
                
                # æ·»åŠ æ•°æ®ç‚¹æ ‡ç­¾
                for _, row in diff_scores.iterrows():
                    ax2.annotate(f"{row['score']:.1f}", 
                                (row['parameters'], row['score']),
                                textcoords="offset points", xytext=(0,10), ha='center')
        
        ax2.legend()
        
        # è®¾ç½®åæ ‡è½´èŒƒå›´
        ax1.set_xlim(0, 80)
        ax1.set_ylim(0, 100)
        ax2.set_xlim(0, 80)
        ax2.set_ylim(0, 100)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plot_file = self.plots_dir / "model_parameter_curves.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"æ¨¡å‹å‚æ•°æ›²çº¿å›¾å·²ä¿å­˜: {plot_file}")
        
        # åˆ›å»ºäº¤äº’å¼ç‰ˆæœ¬
        fig = go.Figure()
        
        # æ·»åŠ æ€»ä½“å¹³å‡æ›²çº¿
        fig.add_trace(go.Scatter(
            x=avg_scores['parameters'],
            y=avg_scores['avg_score'],
            mode='lines+markers',
            name='æ€»ä½“å¹³å‡',
            line=dict(width=3),
            marker=dict(size=8)
        ))
        
        # æ·»åŠ ä¸åŒéš¾åº¦ç­‰çº§çš„æ›²çº¿
        for difficulty in difficulties:
            diff_data = curve_df[curve_df['difficulty'] == difficulty]
            if len(diff_data) > 0:
                diff_scores = diff_data.groupby('parameters')['score'].mean().reset_index()
                fig.add_trace(go.Scatter(
                    x=diff_scores['parameters'],
                    y=diff_scores['score'],
                    mode='lines+markers',
                    name=difficulty.capitalize(),
                    line=dict(width=3)
                ))
        
        fig.update_layout(
            title='æ¨¡å‹å‚æ•°ä¸æ€§èƒ½å…³ç³»æ›²çº¿',
            xaxis_title='æ¨¡å‹å‚æ•° (Billion)',
            yaxis_title='OpenAIè¯„åˆ†',
            xaxis=dict(range=[0, 80]),
            yaxis=dict(range=[0, 100]),
            height=600
        )
        
        # ä¿å­˜äº¤äº’å¼å›¾è¡¨
        interactive_file = self.plots_dir / "model_parameter_curves_interactive.html"
        fig.write_html(interactive_file)
        
        self.logger.info(f"äº¤äº’å¼æ¨¡å‹å‚æ•°æ›²çº¿å›¾å·²ä¿å­˜: {interactive_file}")
        
        return curve_df
    
    def analyze_error_patterns(self, df: pd.DataFrame, model_name: str):
        """åˆ†æé”™è¯¯æ¨¡å¼"""
        self.logger.info("åˆ†æé”™è¯¯æ¨¡å¼")
        
        # æ‰¾å‡ºé”™è¯¯çš„æ ·æœ¬
        error_df = df[df['accuracy'] < 0.8]  # å‡†ç¡®ç‡ä½äº80%çš„æ ·æœ¬
        
        if len(error_df) == 0:
            self.logger.info("æ²¡æœ‰å‘ç°æ˜æ˜¾çš„é”™è¯¯æ¨¡å¼")
            return
        
        # æŒ‰éš¾åº¦åˆ†ç»„ç»Ÿè®¡é”™è¯¯æ•°é‡
        error_counts = error_df['difficulty'].value_counts()
        
        # åˆ›å»ºé”™è¯¯åˆ†å¸ƒå›¾
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # é”™è¯¯æ•°é‡åˆ†å¸ƒ
        axes[0].pie(error_counts.values, labels=error_counts.index, autopct='%1.1f%%')
        axes[0].set_title('é”™è¯¯æ ·æœ¬éš¾åº¦åˆ†å¸ƒ')
        
        # é”™è¯¯ç‡åˆ†å¸ƒ
        error_rates = error_df.groupby('difficulty').size() / df.groupby('difficulty').size()
        error_rates.plot(kind='bar', ax=axes[1])
        axes[1].set_title('å„éš¾åº¦ç­‰çº§é”™è¯¯ç‡')
        axes[1].set_ylabel('é”™è¯¯ç‡')
        axes[1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plot_file = self.plots_dir / f"{model_name}_error_patterns.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"é”™è¯¯æ¨¡å¼åˆ†æå›¾å·²ä¿å­˜: {plot_file}")
        
        # ä¿å­˜é”™è¯¯æ ·æœ¬è¯¦æƒ…
        error_file = self.results_dir / f"{model_name}_error_samples.csv"
        error_df.to_csv(error_file, index=False, encoding='utf-8')
        self.logger.info(f"é”™è¯¯æ ·æœ¬è¯¦æƒ…å·²ä¿å­˜: {error_file}")
    
    def create_interactive_plots(self, df: pd.DataFrame, model_name: str):
        """åˆ›å»ºäº¤äº’å¼å›¾è¡¨"""
        self.logger.info("åˆ›å»ºäº¤äº’å¼å›¾è¡¨")
        
        # åˆ›å»ºå­å›¾
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('å‡†ç¡®ç‡åˆ†å¸ƒ', 'ç”Ÿæˆæ—¶é—´åˆ†å¸ƒ', 'ROUGEåˆ†æ•°åˆ†å¸ƒ', 'BLEUåˆ†æ•°åˆ†å¸ƒ'),
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )
        
        # å‡†ç¡®ç‡åˆ†å¸ƒ
        fig.add_trace(
            go.Histogram(x=df['accuracy'], name='å‡†ç¡®ç‡', nbinsx=20),
            row=1, col=1
        )
        
        # ç”Ÿæˆæ—¶é—´åˆ†å¸ƒ
        fig.add_trace(
            go.Histogram(x=df['generation_time'], name='ç”Ÿæˆæ—¶é—´', nbinsx=20),
            row=1, col=2
        )
        
        # ROUGEåˆ†æ•°åˆ†å¸ƒ
        fig.add_trace(
            go.Histogram(x=df['rouge_score'], name='ROUGEåˆ†æ•°', nbinsx=20),
            row=2, col=1
        )
        
        # BLEUåˆ†æ•°åˆ†å¸ƒ
        fig.add_trace(
            go.Histogram(x=df['bleu_score'], name='BLEUåˆ†æ•°', nbinsx=20),
            row=2, col=2
        )
        
        # æ›´æ–°å¸ƒå±€
        fig.update_layout(
            title=f'{model_name} - è¯„ä¼°æŒ‡æ ‡åˆ†å¸ƒ',
            height=800,
            showlegend=False
        )
        
        # ä¿å­˜äº¤äº’å¼å›¾è¡¨
        plot_file = self.plots_dir / f"{model_name}_interactive_plots.html"
        fig.write_html(plot_file)
        
        self.logger.info(f"äº¤äº’å¼å›¾è¡¨å·²ä¿å­˜: {plot_file}")
    
    def compare_models(self, model_results: Dict[str, pd.DataFrame]):
        """æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½"""
        self.logger.info("æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½")
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„å¹³å‡æŒ‡æ ‡
        comparison_data = []
        
        for model_name, df in model_results.items():
            overall_metrics = df.agg({
                'accuracy': 'mean',
                'exact_match': 'mean',
                'rouge_score': 'mean',
                'bleu_score': 'mean',
                'openai_score': 'mean',
                'generation_time': 'mean'
            })
            
            comparison_data.append({
                'model': model_name,
                **overall_metrics.to_dict()
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        
        # åˆ›å»ºæ¯”è¾ƒå›¾
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('ä¸åŒæ¨¡å‹æ€§èƒ½æ¯”è¾ƒ', fontsize=16, fontweight='bold')
        
        # å‡†ç¡®ç‡æ¯”è¾ƒ
        sns.barplot(data=comparison_df, x='model', y='accuracy', ax=axes[0,0])
        axes[0,0].set_title('å‡†ç¡®ç‡æ¯”è¾ƒ')
        axes[0,0].set_ylabel('å‡†ç¡®ç‡')
        axes[0,0].set_ylim(0, 1)
        axes[0,0].tick_params(axis='x', rotation=45)
        
        # ç²¾ç¡®åŒ¹é…æ¯”è¾ƒ
        sns.barplot(data=comparison_df, x='model', y='exact_match', ax=axes[0,1])
        axes[0,1].set_title('ç²¾ç¡®åŒ¹é…æ¯”è¾ƒ')
        axes[0,1].set_ylabel('ç²¾ç¡®åŒ¹é…ç‡')
        axes[0,1].set_ylim(0, 1)
        axes[0,1].tick_params(axis='x', rotation=45)
        
        # OpenAIè¯„åˆ†æ¯”è¾ƒ
        sns.barplot(data=comparison_df, x='model', y='openai_score', ax=axes[0,2])
        axes[0,2].set_title('OpenAIè¯„åˆ†æ¯”è¾ƒ')
        axes[0,2].set_ylabel('è¯„åˆ† (0-100)')
        axes[0,2].set_ylim(0, 100)
        axes[0,2].tick_params(axis='x', rotation=45)
        
        # ROUGEåˆ†æ•°æ¯”è¾ƒ
        sns.barplot(data=comparison_df, x='model', y='rouge_score', ax=axes[1,0])
        axes[1,0].set_title('ROUGEåˆ†æ•°æ¯”è¾ƒ')
        axes[1,0].set_ylabel('ROUGEåˆ†æ•°')
        axes[1,0].set_ylim(0, 1)
        axes[1,0].tick_params(axis='x', rotation=45)
        
        # BLEUåˆ†æ•°æ¯”è¾ƒ
        sns.barplot(data=comparison_df, x='model', y='bleu_score', ax=axes[1,1])
        axes[1,1].set_title('BLEUåˆ†æ•°æ¯”è¾ƒ')
        axes[1,1].set_ylabel('BLEUåˆ†æ•°')
        axes[1,1].set_ylim(0, 1)
        axes[1,1].tick_params(axis='x', rotation=45)
        
        # ç”Ÿæˆæ—¶é—´æ¯”è¾ƒ
        sns.barplot(data=comparison_df, x='model', y='generation_time', ax=axes[1,2])
        axes[1,2].set_title('ç”Ÿæˆæ—¶é—´æ¯”è¾ƒ')
        axes[1,2].set_ylabel('æ—¶é—´ (ç§’)')
        axes[1,2].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plot_file = self.plots_dir / "model_comparison.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"æ¨¡å‹æ¯”è¾ƒå›¾å·²ä¿å­˜: {plot_file}")
        
        # ä¿å­˜æ¯”è¾ƒæ•°æ®
        comparison_file = self.results_dir / "model_comparison.csv"
        comparison_df.to_csv(comparison_file, index=False, encoding='utf-8')
        self.logger.info(f"æ¨¡å‹æ¯”è¾ƒæ•°æ®å·²ä¿å­˜: {comparison_file}")
        
        return comparison_df
    
    def generate_report(self, df: pd.DataFrame, model_name: str, analysis_results: Dict):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        self.logger.info("ç”Ÿæˆåˆ†ææŠ¥å‘Š")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_samples = len(df)
        avg_accuracy = df['accuracy'].mean()
        avg_generation_time = df['generation_time'].mean()
        
        # æŒ‰éš¾åº¦åˆ†ç»„çš„ç»Ÿè®¡
        difficulty_stats = df.groupby('difficulty').agg({
            'accuracy': ['mean', 'std', 'count'],
            'generation_time': 'mean'
        }).round(4)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""
# {model_name} è¯„ä¼°åˆ†ææŠ¥å‘Š

## æ€»ä½“ç»Ÿè®¡
- æ€»æ ·æœ¬æ•°: {total_samples}
- å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}
- å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_generation_time:.2f}ç§’

## å„éš¾åº¦ç­‰çº§ç»Ÿè®¡
{difficulty_stats.to_string()}

## åˆ†æç»“æœ
- æœ€ä½³è¡¨ç°éš¾åº¦ç­‰çº§: {analysis_results.get('best_difficulty', 'N/A')}
- æœ€å·®è¡¨ç°éš¾åº¦ç­‰çº§: {analysis_results.get('worst_difficulty', 'N/A')}
- æ€§èƒ½å·®å¼‚: {analysis_results.get('performance_gap', 'N/A')}

## å»ºè®®
1. é’ˆå¯¹è¡¨ç°è¾ƒå·®çš„éš¾åº¦ç­‰çº§è¿›è¡Œæ¨¡å‹ä¼˜åŒ–
2. è€ƒè™‘å¢åŠ è®­ç»ƒæ•°æ®æˆ–è°ƒæ•´æ¨¡å‹å‚æ•°
3. åˆ†æé”™è¯¯æ¨¡å¼ï¼Œæ”¹è¿›æç¤ºå·¥ç¨‹

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        """
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.results_dir / f"{model_name}_analysis_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.logger.info(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report

def main():
    parser = argparse.ArgumentParser(description="ç»“æœåˆ†æè„šæœ¬")
    parser.add_argument("--results-file", type=str, required=True,
                       help="ç»“æœæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model-name", type=str, required=True,
                       help="æ¨¡å‹åç§°")
    parser.add_argument("--config", type=str, default="configs/config.yaml",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = ResultsAnalyzer(args.config)
    
    try:
        # åŠ è½½ç»“æœ
        df = analyzer.load_results(args.results_file)
        
        # åˆ†æä¸åŒéš¾åº¦ç­‰çº§çš„å‡†ç¡®ç‡
        difficulty_metrics = analyzer.analyze_accuracy_by_difficulty(df, args.model_name)
        
        # åˆ†æé”™è¯¯æ¨¡å¼
        analyzer.analyze_error_patterns(df, args.model_name)
        
        # åˆ›å»ºäº¤äº’å¼å›¾è¡¨
        analyzer.create_interactive_plots(df, args.model_name)
        
        # ç”Ÿæˆåˆ†æç»“æœ
        analysis_results = {
            'best_difficulty': difficulty_metrics.loc[difficulty_metrics['accuracy'].idxmax(), 'difficulty'],
            'worst_difficulty': difficulty_metrics.loc[difficulty_metrics['accuracy'].idxmin(), 'difficulty'],
            'performance_gap': difficulty_metrics['accuracy'].max() - difficulty_metrics['accuracy'].min()
        }
        
        # å¦‚æœæœ‰OpenAIè¯„åˆ†ï¼Œç»˜åˆ¶æ¨¡å‹å‚æ•°æ›²çº¿
        if 'openai_score' in df.columns:
            # åˆ›å»ºå•æ¨¡å‹ç»“æœå­—å…¸
            single_model_results = {args.model_name: df}
            analyzer.plot_model_parameter_curves(single_model_results)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = analyzer.generate_report(df, args.model_name, analysis_results)
        
        print("\n" + "="*60)
        print("ğŸ“Š åˆ†æå®Œæˆï¼")
        print("="*60)
        print(f"æ¨¡å‹: {args.model_name}")
        print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
        print(f"å¹³å‡å‡†ç¡®ç‡: {df['accuracy'].mean():.4f}")
        print(f"æœ€ä½³éš¾åº¦ç­‰çº§: {analysis_results['best_difficulty']}")
        print(f"æœ€å·®éš¾åº¦ç­‰çº§: {analysis_results['worst_difficulty']}")
        print(f"æ€§èƒ½å·®å¼‚: {analysis_results['performance_gap']:.4f}")
        print(f"\nğŸ“ ç»“æœæ–‡ä»¶ä½ç½®: {analyzer.results_dir}")
        print(f"ğŸ“ˆ å›¾è¡¨æ–‡ä»¶ä½ç½®: {analyzer.plots_dir}")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 