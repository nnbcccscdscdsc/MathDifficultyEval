#!/usr/bin/env python3
"""
é€šç”¨æ•°å­¦è¯„ä¼°æ¡†æ¶ - æ”¯æŒå¤šä¸ªæ¨¡å‹
åŒ…æ‹¬ï¼šæ•°æ®é›†å¤„ç† -> æ¨¡å‹æ¨ç† -> OpenAIæ‰“åˆ† -> ç»“æœåˆ†æ
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import logging
import json
import pandas as pd
import os
import time
from datetime import datetime
from typing import List, Dict, Any, Optional
from tqdm import tqdm
import openai
import matplotlib.pyplot as plt
import matplotlib
# ä½¿ç”¨è‹±æ–‡æ ‡ç­¾ï¼Œé¿å…ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
matplotlib.rcParams['font.family'] = 'DejaVu Sans'

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å…³é—­OpenAIåº“çš„è°ƒè¯•æ—¥å¿—
logging.getLogger("openai").setLevel(logging.ERROR)
logging.getLogger("httpx").setLevel(logging.ERROR)
logging.getLogger("urllib3").setLevel(logging.ERROR)

class MathEvaluationFramework:
    """é€šç”¨æ•°å­¦è¯„ä¼°æ¡†æ¶"""
    
    def __init__(self, model_config: Dict[str, Any], openai_api_key: str = None, max_samples: int = 200):
        """
        åˆå§‹åŒ–è¯„ä¼°æ¡†æ¶
        
        Args:
            model_config: æ¨¡å‹é…ç½®å­—å…¸
            openai_api_key: OpenAI APIå¯†é’¥
            max_samples: æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°
        """
        self.model_config = model_config
        self.model_name = model_config['name']
        self.model_type = model_config.get('type', 'default')
        self.max_samples = max_samples
        self.model = None
        self.tokenizer = None
        
        # ç”Ÿæˆè¿è¡Œæ ‡è¯†ç¬¦
        import random
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        random_suffix = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=4))
        self.run_id = f"{timestamp}_{random_suffix}"
        logger.info(f"ğŸ†” æœ¬æ¬¡è¿è¡ŒID: {self.run_id}")
        
        # è®¾ç½®OpenAI
        if openai_api_key:
            openai.api_key = openai_api_key
            # ä½¿ç”¨æ—§ç‰ˆæœ¬OpenAIåº“ (0.28.0)
            self.openai_client = openai
        else:
            self.openai_client = None
            logger.warning("æœªæä¾›OpenAI APIå¯†é’¥ï¼Œå°†è·³è¿‡OpenAIæ‰“åˆ†æ­¥éª¤")
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        logger.info(f"ğŸ§® åŠ è½½æ¨¡å‹: {self.model_name}")
        
        try:
            # æ£€æŸ¥æ¨¡å‹ç¼“å­˜çŠ¶æ€
            if self._check_model_cache():
                logger.info("ğŸ“¦ ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹...")
                cache_path = self._load_model_from_cache()
                if cache_path:
                    self._load_model_from_path(cache_path, use_cache=True)
                else:
                    raise ValueError("æ— æ³•è·å–ç¼“å­˜è·¯å¾„")
            else:
                logger.info("ğŸ“¥ ä»Hugging Faceä¸‹è½½æ¨¡å‹...")
                self._load_model_from_path(self.model_name, use_cache=False)
            
            logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
            self._log_gpu_memory()
                
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _load_model_from_path(self, model_path: str, use_cache: bool = False):
        """ä»æŒ‡å®šè·¯å¾„åŠ è½½æ¨¡å‹"""
        # åŠ è½½tokenizer
        logger.info("åŠ è½½tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            local_files_only=use_cache
        )
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹ - æ ¹æ®æ¨¡å‹ç±»å‹ä½¿ç”¨ä¸åŒé…ç½®
        logger.info("åŠ è½½æ¨¡å‹æƒé‡...")
        
        # è®¡ç®—GPUå†…å­˜åˆ†é…ç­–ç•¥
        total_gpus = torch.cuda.device_count()
        logger.info(f"ğŸ–¥ï¸ æ£€æµ‹åˆ° {total_gpus} ä¸ªGPU")
        
        # ä¸ºä¸åŒå¤§å°çš„æ¨¡å‹ä½¿ç”¨æœ€ä¼˜çš„å¹¶è¡Œç­–ç•¥
        if self.model_type in ["32b_quantized", "70b_quantized"]:
            # å¤§æ¨¡å‹ï¼šä½¿ç”¨balanced_low_0ç­–ç•¥å®ç°çœŸæ­£çš„å¤šGPUå¹¶è¡Œ
            device_map = "balanced_low_0"
            logger.info(f"ğŸ“Š å¤§æ¨¡å‹ä½¿ç”¨balanced_low_0ç­–ç•¥å®ç°å¤šGPUå¹¶è¡Œ")
        elif self.model_type in ["3b", "7b"]:
            # 3Bã€7Bæ¨¡å‹ï¼šè·³è¿‡GPU 0ï¼Œä½¿ç”¨å…¶ä»–GPU
            device_map = "balanced_low_0"
            logger.info(f"ğŸ“Š {self.model_type}æ¨¡å‹ä½¿ç”¨balanced_low_0ç­–ç•¥ï¼Œè·³è¿‡GPU 0")
        elif self.model_type in ["14b", "32b", "72b"]:
            # 14Bã€32Bã€72Bæ¨¡å‹ï¼šä½¿ç”¨GPUå¹¶è¡Œç­–ç•¥
            device_map = "balanced_low_0"
            logger.info(f"ğŸ“Š {self.model_type}æ¨¡å‹ä½¿ç”¨balanced_low_0ç­–ç•¥å®ç°å¤šGPUå¹¶è¡Œ")
        else:
            # å°æ¨¡å‹ï¼šä½¿ç”¨autoç­–ç•¥
            device_map = "auto"
            logger.info("ğŸ“Š å°æ¨¡å‹ä½¿ç”¨autoç­–ç•¥")
        
        model_kwargs = {
            "torch_dtype": torch.float16,
            "device_map": device_map,  # å…³é”®ï¼šä½¿ç”¨æœ€ä¼˜çš„device_mapç­–ç•¥
            "trust_remote_code": True,
            "low_cpu_mem_usage": True,
            "local_files_only": use_cache
        }
        
        # æ ¹æ®æ¨¡å‹ç±»å‹æ·»åŠ ç‰¹æ®Šé…ç½®
        if self.model_type == "7b_quantized":
            # 7Bæ¨¡å‹ä½¿ç”¨4bité‡åŒ–
            model_kwargs.update({
                "load_in_4bit": True,
                "bnb_4bit_compute_dtype": torch.float16
            })
        elif self.model_type == "14b_quantized":
            # 14Bæ¨¡å‹ä½¿ç”¨4bité‡åŒ–
            model_kwargs.update({
                "load_in_4bit": True,
                "bnb_4bit_compute_dtype": torch.float16
            })
        elif self.model_type == "32b_quantized":
            # 32Bæ¨¡å‹ä½¿ç”¨4bité‡åŒ–
            model_kwargs.update({
                "load_in_4bit": True,
                "bnb_4bit_compute_dtype": torch.float16
            })
        elif self.model_type == "70b_quantized":
            # 70Bæ¨¡å‹ä½¿ç”¨4bité‡åŒ–
            model_kwargs.update({
                "load_in_4bit": True,
                "bnb_4bit_compute_dtype": torch.float16
            })
        elif self.model_type in ["1.5b", "3b", "7b"]:
            # 1.5Bã€3Bã€7Bæ¨¡å‹ä½¿ç”¨æ ‡å‡†é…ç½®
            pass
        elif self.model_type in ["14b", "32b", "72b"]:
            # 14Bã€32Bã€72Bæ¨¡å‹ä½¿ç”¨æ ‡å‡†é…ç½®ï¼Œæ”¯æŒå¤šGPUå¹¶è¡Œ
            pass
        
        self.model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
    
    def _check_model_cache(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¼“å­˜"""
        try:
            cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
            model_cache_path = os.path.join(cache_dir, f"models--{self.model_name.replace('/', '--')}")
            
            if os.path.exists(model_cache_path):
                # æ£€æŸ¥ä¸»æ¨¡å‹ç›®å½•ï¼ˆæ¨¡å‹æ–‡ä»¶é€šå¸¸åœ¨è¿™é‡Œï¼‰
                # åŠ¨æ€æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ï¼Œæ”¯æŒåˆ†ç‰‡æ–‡ä»¶å’Œå•ä¸ªæ–‡ä»¶
                model_files = [f for f in os.listdir(model_cache_path) if f.startswith("model-") and f.endswith(".safetensors")]
                single_model_file = os.path.join(model_cache_path, "model.safetensors")
                
                if model_files or os.path.exists(single_model_file):
                    # æ£€æŸ¥åŸºæœ¬æ–‡ä»¶
                    basic_files = ["config.json", "tokenizer.json"]
                    missing_basic = []
                    for f in basic_files:
                        file_path = os.path.join(model_cache_path, f)
                        if not os.path.exists(file_path):
                            missing_basic.append(f)
                    
                    # æ£€æŸ¥æ¨¡å‹ç´¢å¼•æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œå°æ¨¡å‹å¯èƒ½æ²¡æœ‰ï¼‰
                    index_file = os.path.join(model_cache_path, "model.safetensors.index.json")
                    if not os.path.exists(index_file):
                        logger.debug(f"âš ï¸ æ¨¡å‹ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆå°æ¨¡å‹å¯èƒ½ä¸éœ€è¦ï¼‰: {index_file}")
                    
                    if not missing_basic:
                        logger.info(f"âœ… æ¨¡å‹å·²ç¼“å­˜: {self.model_name}")
                        return True
                    else:
                        logger.info(f"âš ï¸ æ¨¡å‹ç¼“å­˜ä¸å®Œæ•´ï¼Œç¼ºå°‘æ–‡ä»¶: {missing_basic}")
                        return False
                
                # å¦‚æœä¸»ç›®å½•æ²¡æœ‰ï¼Œå†æ£€æŸ¥snapshotsç›®å½•
                snapshots_dir = os.path.join(model_cache_path, "snapshots")
                if os.path.exists(snapshots_dir):
                    for snapshot in os.listdir(snapshots_dir):
                        snapshot_path = os.path.join(snapshots_dir, snapshot)
                        if os.path.isdir(snapshot_path):
                            # æ£€æŸ¥åŸºæœ¬æ–‡ä»¶ï¼ˆåŒ…æ‹¬ç¬¦å·é“¾æ¥ï¼‰
                            basic_files = ["config.json", "tokenizer.json"]
                            missing_basic = []
                            for f in basic_files:
                                file_path = os.path.join(snapshot_path, f)
                                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆåŒ…æ‹¬ç¬¦å·é“¾æ¥ï¼‰
                                if not (os.path.exists(file_path) or os.path.lexists(file_path)):
                                    missing_basic.append(f)
                            
                            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
                            # æ”¯æŒåˆ†ç‰‡æ–‡ä»¶å’Œå•ä¸ªæ–‡ä»¶
                            model_files = [f for f in os.listdir(snapshot_path) if f.startswith("model-") and f.endswith(".safetensors")]
                            single_model_file = os.path.join(snapshot_path, "model.safetensors")
                            
                            has_model_file = bool(model_files) or os.path.exists(single_model_file)
                            
                            if not missing_basic and has_model_file:
                                logger.info(f"âœ… æ¨¡å‹å·²ç¼“å­˜: {self.model_name}")
                                return True
                            else:
                                missing_files = missing_basic + ([] if has_model_file else ["model files"])
                                logger.info(f"âš ï¸ æ¨¡å‹ç¼“å­˜ä¸å®Œæ•´ï¼Œç¼ºå°‘æ–‡ä»¶: {missing_files}")
                                return False
            
            logger.info(f"âŒ æ¨¡å‹æœªç¼“å­˜: {self.model_name}")
            return False
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ£€æŸ¥ç¼“å­˜æ—¶å‡ºé”™: {e}")
            return False
    
    def _load_model_from_cache(self):
        """ç›´æ¥ä»ç¼“å­˜åŠ è½½æ¨¡å‹ï¼Œä¸è¿›è¡Œç½‘ç»œè¿æ¥"""
        try:
            cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
            model_cache_path = os.path.join(cache_dir, f"models--{self.model_name.replace('/', '--')}")
            
            if not os.path.exists(model_cache_path):
                raise ValueError(f"æ¨¡å‹ç¼“å­˜ä¸å­˜åœ¨: {model_cache_path}")
            
            # é¦–å…ˆå°è¯•ä»ä¸»æ¨¡å‹ç›®å½•åŠ è½½ï¼ˆæ¨¡å‹æ–‡ä»¶é€šå¸¸åœ¨è¿™é‡Œï¼‰
            # åŠ¨æ€æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ï¼Œæ”¯æŒåˆ†ç‰‡æ–‡ä»¶å’Œå•ä¸ªæ–‡ä»¶
            model_files = [f for f in os.listdir(model_cache_path) if f.startswith("model-") and f.endswith(".safetensors")]
            single_model_file = os.path.join(model_cache_path, "model.safetensors")
            
            if model_files or os.path.exists(single_model_file):
                logger.info(f"ğŸ“¦ ä»ä¸»æ¨¡å‹ç›®å½•åŠ è½½: {model_cache_path}")
                return model_cache_path
            
            # å¦‚æœä¸»ç›®å½•æ²¡æœ‰ï¼Œå†å°è¯•snapshotsç›®å½•
            snapshots_dir = os.path.join(model_cache_path, "snapshots")
            if not os.path.exists(snapshots_dir):
                raise ValueError(f"æ¨¡å‹å¿«ç…§ç›®å½•ä¸å­˜åœ¨: {snapshots_dir}")
            
            snapshots = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
            if not snapshots:
                raise ValueError(f"æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹å¿«ç…§")
            
            latest_snapshot = snapshots[-1]
            snapshot_path = os.path.join(snapshots_dir, latest_snapshot)
            
            logger.info(f"ğŸ“¦ ä»ç¼“å­˜è·¯å¾„åŠ è½½: {snapshot_path}")
            return snapshot_path
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç¼“å­˜è·¯å¾„å¤±è´¥: {e}")
            return None
    
    def _log_gpu_memory(self):
        """æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
                memory_reserved = torch.cuda.memory_reserved(i) / 1024**3
                logger.info(f"GPU {i}: å·²åˆ†é… {memory_allocated:.2f}GB, å·²ä¿ç•™ {memory_reserved:.2f}GB")
        else:
            logger.info("æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè¿è¡Œ")
    
    def load_dataset(self, dataset_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½æ•°æ®é›†"""
        logger.info(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {dataset_path}")
        
        # ä¼˜å…ˆä½¿ç”¨æŒ‡å®šçš„æ•°æ®é›†è·¯å¾„
        if os.path.exists(dataset_path):
            logger.info(f"ğŸ“ ä½¿ç”¨æŒ‡å®šæ•°æ®é›†: {dataset_path}")
            try:
                df = pd.read_csv(dataset_path)
                logger.info(f"âœ… ä»æŒ‡å®šæ–‡ä»¶åŠ è½½ {len(df)} ä¸ªæ ·æœ¬")
                return self._convert_df_to_dataset(df)
            except Exception as e:
                logger.error(f"âŒ æŒ‡å®šæ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ•°æ®é›†ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å›ºå®šæ ·æœ¬æ–‡ä»¶
        model_safe_name = self.model_name.replace('/', '_').replace('-', '_')
        fixed_samples_path = f"data/processed/fixed_{self.max_samples}_samples_{model_safe_name}.csv"
        
        if os.path.exists(fixed_samples_path):
            logger.info(f"ğŸ“ å‘ç°å›ºå®šæ ·æœ¬æ–‡ä»¶: {fixed_samples_path}")
            try:
                df = pd.read_csv(fixed_samples_path)
                logger.info(f"âœ… ä»å›ºå®šæ–‡ä»¶åŠ è½½ {len(df)} ä¸ªæ ·æœ¬")
                return self._convert_df_to_dataset(df)
            except Exception as e:
                logger.error(f"âŒ å›ºå®šæ ·æœ¬æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
                logger.info("ğŸ”„ é‡æ–°ç”Ÿæˆå›ºå®šæ ·æœ¬...")
        
        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œä»åŸå§‹æ•°æ®é›†åˆ›å»ºåˆ†å±‚é‡‡æ ·çš„æ ·æœ¬
        logger.info("ğŸ”„ åˆ›å»ºåˆ†å±‚é‡‡æ ·çš„å›ºå®šæ ·æœ¬...")
        try:
            original_dataset = "data/processed/deepmath_evaluation_dataset.csv"
            df = pd.read_csv(original_dataset)
            logger.info(f"åŸå§‹æ•°æ®é›†åŒ…å« {len(df)} ä¸ªæ ·æœ¬")
            
            # åˆ†å±‚é‡‡æ ·ï¼šç¡®ä¿åŒ…å«æ‰€æœ‰éš¾åº¦ç­‰çº§
            stratified_samples = self._create_stratified_samples(df, self.max_samples)
            
            # ä¿å­˜åˆ°å›ºå®šä½ç½®
            stratified_samples.to_csv(fixed_samples_path, index=False)
            logger.info(f"ğŸ’¾ å›ºå®šæ ·æœ¬å·²ä¿å­˜åˆ°: {fixed_samples_path}")
            
            return self._convert_df_to_dataset(stratified_samples)
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _convert_df_to_dataset(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """å°†DataFrameè½¬æ¢ä¸ºæ•°æ®é›†æ ¼å¼"""
        dataset = []
        for _, row in df.iterrows():
            sample = {
                'id': row['id'],
                'problem': row['problem'],
                'solution': row['solution'],
                'answer': row['answer'],
                'difficulty': row['difficulty'],
                'topic': row['topic'],
                'difficulty_score': row['difficulty_score']
            }
            dataset.append(sample)
        return dataset
    
    def _create_stratified_samples(self, df: pd.DataFrame, target_samples: int) -> pd.DataFrame:
        """åˆ›å»ºåˆ†å±‚é‡‡æ ·çš„æ ·æœ¬"""
        logger.info("ğŸ“Š å¼€å§‹åˆ†å±‚é‡‡æ ·...")
        
        # è·å–æ‰€æœ‰éš¾åº¦ç­‰çº§
        difficulty_levels = sorted(df['difficulty'].unique())
        logger.info(f"å‘ç°éš¾åº¦ç­‰çº§: {difficulty_levels}")
        
        # è®¡ç®—æ¯ä¸ªéš¾åº¦ç­‰çº§çš„æ ·æœ¬æ•°
        total_levels = len(difficulty_levels)
        base_samples_per_level = target_samples // total_levels
        remaining_samples = target_samples % total_levels
        
        logger.info(f"æ¯ä¸ªéš¾åº¦ç­‰çº§åŸºç¡€æ ·æœ¬æ•°: {base_samples_per_level}")
        logger.info(f"å‰©ä½™æ ·æœ¬æ•°: {remaining_samples}")
        
        stratified_samples = []
        
        for i, difficulty in enumerate(difficulty_levels):
            # è·å–å½“å‰éš¾åº¦ç­‰çº§çš„æ‰€æœ‰æ ·æœ¬
            level_df = df[df['difficulty'] == difficulty]
            level_count = len(level_df)
            
            # è®¡ç®—å½“å‰ç­‰çº§åº”é‡‡æ ·çš„æ•°é‡
            if i < remaining_samples:
                samples_needed = base_samples_per_level + 1
            else:
                samples_needed = base_samples_per_level
            
            # å¦‚æœå½“å‰ç­‰çº§çš„æ ·æœ¬æ•°ä¸è¶³ï¼Œå…¨éƒ¨ä½¿ç”¨
            if level_count <= samples_needed:
                samples_needed = level_count
                logger.info(f"éš¾åº¦ {difficulty}: ä½¿ç”¨å…¨éƒ¨ {level_count} ä¸ªæ ·æœ¬")
            else:
                logger.info(f"éš¾åº¦ {difficulty}: éšæœºé€‰æ‹© {samples_needed} ä¸ªæ ·æœ¬ï¼ˆå…± {level_count} ä¸ªï¼‰")
            
            # éšæœºé‡‡æ ·
            if level_count > 0:
                sampled = level_df.sample(n=samples_needed, random_state=42)
                stratified_samples.append(sampled)
        
        # åˆå¹¶æ‰€æœ‰é‡‡æ ·çš„æ ·æœ¬
        if stratified_samples:
            result_df = pd.concat(stratified_samples, ignore_index=True)
            logger.info(f"âœ… åˆ†å±‚é‡‡æ ·å®Œæˆï¼Œå…± {len(result_df)} ä¸ªæ ·æœ¬")
            
            # æ˜¾ç¤ºæ¯ä¸ªéš¾åº¦ç­‰çº§çš„æ ·æœ¬æ•°
            difficulty_counts = result_df['difficulty'].value_counts().sort_index()
            logger.info("ğŸ“Š å„éš¾åº¦ç­‰çº§æ ·æœ¬åˆ†å¸ƒ:")
            for difficulty, count in difficulty_counts.items():
                logger.info(f"  éš¾åº¦ {difficulty}: {count} ä¸ªæ ·æœ¬")
            
            return result_df
        else:
            raise ValueError("åˆ†å±‚é‡‡æ ·å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆä»»ä½•æ ·æœ¬")
    
    def generate_response(self, problem: str) -> str:
        """ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                # DeepSeek-R1æ¨èçš„æç¤ºæ ¼å¼
                prompt = f"<think>\nPlease reason step by step, and put your final answer within \\boxed{{}}.\n</think>\n\n{problem}\n\n<think>\n"
                
                logger.debug(f"è¾“å…¥æç¤ºé•¿åº¦: {len(prompt)} å­—ç¬¦")
            
                # ç¼–ç è¾“å…¥
                inputs = self.tokenizer(prompt, return_tensors="pt")
                input_ids = inputs.input_ids.to(self.model.device)
                attention_mask = inputs.attention_mask.to(self.model.device)
            
                logger.debug(f"è¾“å…¥tokenæ•°é‡: {input_ids.shape[1]}")
                
                # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒæ•´ç”Ÿæˆå‚æ•° - å‡å°‘tokenæ•°é‡ä»¥èŠ‚çœå†…å­˜
                max_new_tokens = min(self.model_config.get('max_new_tokens', 500), 300)
                
                # è®°å½•ç”Ÿæˆå‰çš„å†…å­˜çŠ¶æ€
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    gpu_memory_before = torch.cuda.memory_allocated() / 1024**3
                    logger.debug(f"ç”Ÿæˆå‰GPUå†…å­˜: {gpu_memory_before:.2f} GB")
            
                # ç”Ÿæˆå›ç­” - é’ˆå¯¹32Bæ¨¡å‹ä¼˜åŒ–å‚æ•°
                with torch.no_grad():
                    outputs = self.model.generate(
                        input_ids,
                        attention_mask=attention_mask,
                        max_new_tokens=max_new_tokens,
                        do_sample=False,  # æ”¹ä¸ºFalseé¿å…æ¦‚ç‡åˆ†å¸ƒé—®é¢˜
                        temperature=1.0,  # ä½¿ç”¨é»˜è®¤æ¸©åº¦
                        top_p=1.0,        # ä½¿ç”¨é»˜è®¤top_p
                        pad_token_id=self.tokenizer.eos_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                        repetition_penalty=1.0,  # é™ä½é‡å¤æƒ©ç½š
                        use_cache=True,   # å¯ç”¨ç¼“å­˜
                        return_dict_in_generate=False  # é¿å…å¤æ‚è¿”å›æ ¼å¼
                    )
            
                logger.debug(f"ç”Ÿæˆtokenæ•°é‡: {outputs.shape[1] - input_ids.shape[1]}")
                
                # è§£ç å®Œæ•´è¾“å‡º
                try:
                    full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                    logger.debug(f"å®Œæ•´å“åº”é•¿åº¦: {len(full_response)} å­—ç¬¦")
                except Exception as decode_error:
                    logger.error(f"âŒ Tokenè§£ç å¤±è´¥: {decode_error}")
                    # å°è¯•ä¸è·³è¿‡ç‰¹æ®Štoken
                    try:
                        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=False)
                        logger.warning("ä½¿ç”¨skip_special_tokens=Falseé‡æ–°è§£ç ")
                    except Exception as decode_error2:
                        logger.error(f"âŒ é‡æ–°è§£ç ä¹Ÿå¤±è´¥: {decode_error2}")
                        return f"ç”Ÿæˆå¤±è´¥: tokenè§£ç é”™è¯¯ - {decode_error}"
            
                # æå–æ¨¡å‹çš„å›ç­”éƒ¨åˆ†ï¼ˆç§»é™¤æç¤ºï¼‰
                model_response = full_response.replace(prompt, "").strip()
            
                logger.debug(f"æ¨¡å‹å›ç­”é•¿åº¦: {len(model_response)} å­—ç¬¦")
                logger.debug(f"æ¨¡å‹å›ç­”å‰50å­—ç¬¦: {model_response[:50]}")
                
                # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸ºç©ºæˆ–å¤ªçŸ­
                if not model_response or len(model_response) < 10:
                    logger.warning(f"âš ï¸ ç”Ÿæˆè¾“å‡ºè¿‡çŸ­æˆ–ä¸ºç©ºï¼Œå°è¯•é‡è¯• ({attempt + 1}/{max_retries})")
                    logger.warning(f"åŸå§‹å®Œæ•´å“åº”: {full_response}")
                    if attempt < max_retries - 1:
                        time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                        continue
                    else:
                        return f"ç”Ÿæˆå¤±è´¥: è¾“å‡ºä¸ºç©ºæˆ–è¿‡çŸ­"
                
                # è®°å½•ç”Ÿæˆåçš„å†…å­˜çŠ¶æ€
                if torch.cuda.is_available():
                    gpu_memory_after = torch.cuda.memory_allocated() / 1024**3
                    logger.debug(f"ç”ŸæˆåGPUå†…å­˜: {gpu_memory_after:.2f} GB")
                
                return model_response
                
            except torch.cuda.OutOfMemoryError as e:
                logger.error(f"âŒ GPUå†…å­˜ä¸è¶³ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                if attempt < max_retries - 1:
                    time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                    continue
                else:
                    return f"ç”Ÿæˆå¤±è´¥: GPUå†…å­˜ä¸è¶³"
            
            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
                logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
                
                # è®°å½•æ›´å¤šè°ƒè¯•ä¿¡æ¯
                if hasattr(e, '__traceback__'):
                    import traceback
                    logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                
                if attempt < max_retries - 1:
                    time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                    continue
                else:
                    return f"ç”Ÿæˆå¤±è´¥: {type(e).__name__} - {str(e)}"
        
        return f"ç”Ÿæˆå¤±è´¥: é‡è¯•{max_retries}æ¬¡åä»ç„¶å¤±è´¥"
    
    def evaluate_with_openai(self, problem: str, model_response: str, correct_answer: str, standard_solution: str = "") -> Dict[str, Any]:
        """ä½¿ç”¨OpenAIè¯„ä¼°æ¨¡å‹å›ç­”ï¼ŒåŒ…å«æ ‡å‡†è§£æ³•å‚è€ƒ"""
        if not self.openai_client:
            return {"error": "OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"}
        
        try:
            # æ„å»ºè¯„ä¼°æç¤º
            if standard_solution:
                evaluation_prompt = f"""
Please evaluate the quality of the answer to the following mathematical problem. You have access to the standard solution for reference.

Problem: {problem}

Correct Answer: {correct_answer}

Standard Solution: {standard_solution}

Model Response: {model_response}

Please evaluate from the following aspects and give a score from 1 to 10:

1. Answer Correctness (1-10 points): Whether the final answer is correct
2. Reasoning Logic (1-10 points): Whether the reasoning process is clear and logical, compared to the standard solution
3. Step Completeness (1-10 points): Whether all necessary solution steps are shown, considering what the standard solution covers
4. Mathematical Accuracy (1-10 points): Whether mathematical calculations and formulas are accurate
5. Expression Clarity (1-10 points): Whether the expression is clear and easy to understand

IMPORTANT: You must respond with ONLY a valid JSON object. Do not include any other text, explanations, or markdown formatting.

CRITICAL: In the "comments" field, avoid using backslashes (\) or special characters that could break JSON parsing. Use simple text only.

Please return the evaluation result in JSON format:
{{
    "answer_correctness": score,
    "reasoning_logic": score,
    "step_completeness": score,
    "mathematical_accuracy": score,
    "expression_clarity": score,
    "overall_score": total_score/5,
    "comments": "Detailed evaluation with reference to standard solution"
}}
"""
            else:
                evaluation_prompt = f"""
Please evaluate the quality of the answer to the following mathematical problem.

Problem: {problem}

Correct Answer: {correct_answer}

Model Response: {model_response}

Please evaluate from the following aspects and give a score from 1 to 10:

1. Answer Correctness (1-10 points): Whether the final answer is correct
2. Reasoning Logic (1-10 points): Whether the reasoning process is clear and logical
3. Step Completeness (1-10 points): Whether all solution steps are shown
4. Mathematical Accuracy (1-10 points): Whether mathematical calculations and formulas are accurate
5. Expression Clarity (1-10 points): Whether the expression is clear and easy to understand

IMPORTANT: You must respond with ONLY a valid JSON object. Do not include any other text, explanations, or markdown formatting.

CRITICAL: In the "comments" field, avoid using backslashes (\) or special characters that could break JSON parsing. Use simple text only.

Please return the evaluation result in JSON format:
{{
    "answer_correctness": score,
    "reasoning_logic": score,
    "step_completeness": score,
    "mathematical_accuracy": score,
    "expression_clarity": score,
    "overall_score": total_score/5,
    "comments": "Detailed evaluation"
}}
"""
            
            # ä½¿ç”¨æ—§ç‰ˆæœ¬OpenAIåº“ (0.28.0)
            response = self.openai_client.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a professional mathematical education evaluator."},
                    {"role": "user", "content": evaluation_prompt}
                ],
                temperature=0.3
            )
            response_content = response.choices[0].message.content
            
            # è§£æJSONå“åº”
            import re
            try:
                evaluation = json.loads(response_content)
                return evaluation
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
                logger.warning(f"åŸå§‹å“åº”: {response_content[:200]}...")  # åªæ˜¾ç¤ºå‰200å­—ç¬¦
                
                # å°è¯•æå–JSONéƒ¨åˆ†
                try:
                    # æŸ¥æ‰¾JSONå¯¹è±¡
                    json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group()
                        evaluation = json.loads(json_str)
                        logger.info(f"âœ… æˆåŠŸæå–JSONéƒ¨åˆ†")
                        return evaluation
                except:
                    pass
                
                # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                try:
                    # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                    cleaned_response = response_content.replace('```json', '').replace('```', '').strip()
                    evaluation = json.loads(cleaned_response)
                    logger.info(f"âœ… æˆåŠŸä¿®å¤JSONæ ¼å¼")
                    return evaluation
                except:
                    pass
                
                # å°è¯•ä¿®å¤è½¬ä¹‰å­—ç¬¦é—®é¢˜
                try:
                    # ä¿®å¤å¸¸è§çš„è½¬ä¹‰å­—ç¬¦é—®é¢˜
                    fixed_response = response_content
                    # ä¿®å¤ \boxed{} æ ¼å¼
                    fixed_response = re.sub(r'\\boxed\{([^}]*)\}', r'\\boxed{\1}', fixed_response)
                    # ä¿®å¤å…¶ä»–å¯èƒ½çš„è½¬ä¹‰é—®é¢˜
                    fixed_response = fixed_response.replace('\\n', '\\\\n')
                    fixed_response = fixed_response.replace('\\t', '\\\\t')
                    evaluation = json.loads(fixed_response)
                    logger.info(f"âœ… æˆåŠŸä¿®å¤è½¬ä¹‰å­—ç¬¦é—®é¢˜")
                    return evaluation
                except:
                    pass
                
                return {
                    "raw_response": response_content,
                    "error": "JSON parsing failed",
                    "parse_error": str(e),
                    "error_type": "json_decode_error"
                }
                
        except Exception as e:
            logger.error(f"âŒ OpenAI evaluation failed: {e}")
            return {
                "error": f"Evaluation failed: {e}",
                "error_type": "openai_api_error",
                "exception": str(e)
            }
    
    def run_evaluation(self, dataset_path: str) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´è¯„ä¼°æµç¨‹")
        
        # 1. åŠ è½½æ¨¡å‹
        self.load_model()
        
        # 2. åŠ è½½æ•°æ®é›†
        dataset = self.load_dataset(dataset_path)
        
        # 3. è¿è¡Œè¯„ä¼°
        results = []
        evaluation_stats = {
            "total_samples": len(dataset),
            "successful_generations": 0,
            "successful_evaluations": 0,
            "average_scores": {},
            "difficulty_analysis": {}
        }
        
        logger.info(f"ğŸ“ å¼€å§‹è¯„ä¼° {len(dataset)} ä¸ªæ ·æœ¬...")
        logger.info(f"â±ï¸ é¢„è®¡éœ€è¦æ—¶é—´: {len(dataset) * 30 / 60:.1f} åˆ†é’Ÿï¼ˆå‡è®¾æ¯ä¸ªæ ·æœ¬30ç§’ï¼‰")
        
        start_time = time.time()
        for i, sample in enumerate(tqdm(dataset, desc="è¯„ä¼°è¿›åº¦")):
            logger.info(f"\n--- æ ·æœ¬ {i+1}/{len(dataset)}: {sample['id']} ---")
            
            # ç”Ÿæˆæ¨¡å‹å›ç­”
            model_response = self.generate_response(sample['problem'])
            
            # è®°å½•æ‰€æœ‰æ ·æœ¬ï¼ŒåŒ…æ‹¬ç”Ÿæˆå¤±è´¥çš„
            if model_response and not model_response.startswith("ç”Ÿæˆå¤±è´¥"):
                evaluation_stats["successful_generations"] += 1
                
                # OpenAIè¯„ä¼°
                evaluation = self.evaluate_with_openai(
                    sample['problem'], 
                    model_response, 
                    sample['answer'],
                    sample.get('solution', '')  # ä¼ å…¥æ ‡å‡†è§£æ³•ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸ºç©ºå­—ç¬¦ä¸²
                )
                
                if isinstance(evaluation, dict) and "error" not in evaluation:
                    evaluation_stats["successful_evaluations"] += 1
                    logger.info(f"âœ… è¯„ä¼°å®Œæˆï¼Œæ€»åˆ†: {evaluation.get('overall_score', 0):.2f}")
                else:
                    # è¯¦ç»†é”™è¯¯ä¿¡æ¯å¤„ç†
                    if isinstance(evaluation, dict):
                        error_msg = evaluation.get('error', 'æœªçŸ¥é”™è¯¯')
                        parse_error = evaluation.get('parse_error', '')
                        raw_response = evaluation.get('raw_response', '')[:200]  # åªæ˜¾ç¤ºå‰200å­—ç¬¦
                        
                        if parse_error:
                            logger.warning(f"âš ï¸ è¯„ä¼°å¤±è´¥: {error_msg}")
                            logger.warning(f"è§£æé”™è¯¯: {parse_error}")
                            if raw_response:
                                logger.warning(f"åŸå§‹å“åº”: {raw_response}...")
                        else:
                            logger.warning(f"âš ï¸ è¯„ä¼°å¤±è´¥: {error_msg}")
                    else:
                        logger.warning(f"âš ï¸ è¯„ä¼°å¤±è´¥: {str(evaluation)}")
                    
                    # ä¸ºè¯„ä¼°å¤±è´¥çš„æ ·æœ¬åˆ›å»ºç‰¹æ®Šè¯„ä¼°
                    failed_evaluation = {
                        "answer_correctness": 0,
                        "reasoning_logic": 0,
                        "step_completeness": 0,
                        "mathematical_accuracy": 0,
                        "expression_clarity": 0,
                        "overall_score": 0,
                        "comments": f"è¯„ä¼°å¤±è´¥: {str(evaluation)}",
                        "error": "evaluation_failed"
                    }
                    evaluation = failed_evaluation
                
                # ä¿å­˜ç»“æœ
                result = {
                    "id": sample['id'],
                    "problem": sample['problem'],
                    "correct_answer": sample['answer'],
                    "standard_solution": sample.get('solution', ''),  # æ·»åŠ åŸå§‹è§£æ³•
                    "model_response": model_response,
                    "difficulty": sample['difficulty'],
                    "topic": sample['topic'],
                    "evaluation": evaluation,
                    "generation_status": "success"
                }
                results.append(result)
            
            else:
                # ç”Ÿæˆå¤±è´¥ - è®°å½•å¤±è´¥ä¿¡æ¯
                logger.warning(f"âŒ ç”Ÿæˆå¤±è´¥: {model_response}")
                
                # ä¸ºç”Ÿæˆå¤±è´¥çš„æ ·æœ¬åˆ›å»ºç‰¹æ®Šè¯„ä¼°
                failed_evaluation = {
                    "answer_correctness": 0,
                    "reasoning_logic": 0,
                    "step_completeness": 0,
                    "mathematical_accuracy": 0,
                    "expression_clarity": 0,
                    "overall_score": 0,
                    "comments": f"ç”Ÿæˆå¤±è´¥: {model_response}",
                    "error": "generation_failed"
                }
                
                # ä¿å­˜ç”Ÿæˆå¤±è´¥çš„ç»“æœ
                result = {
                    "id": sample['id'],
                    "problem": sample['problem'],
                    "correct_answer": sample['answer'],
                    "standard_solution": sample.get('solution', ''),  # æ·»åŠ åŸå§‹è§£æ³•
                    "model_response": model_response,
                    "difficulty": sample['difficulty'],
                    "topic": sample['topic'],
                    "evaluation": failed_evaluation,
                    "generation_status": "failed"
                }
                results.append(result)
            
            # æ¯10ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
            if (i + 1) % 10 == 0:
                self.save_intermediate_results(results, i + 1)
                logger.info(f"ğŸ’¾ å·²å¤„ç† {i + 1}/{len(dataset)} ä¸ªæ ·æœ¬ï¼Œä¸­é—´ç»“æœå·²ä¿å­˜")
        
        # 4. åˆ†æç»“æœ
        elapsed_time = time.time() - start_time
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {elapsed_time / 60:.1f} åˆ†é’Ÿ")
        logger.info(f"ğŸ“Š å¹³å‡æ¯ä¸ªæ ·æœ¬: {elapsed_time / len(dataset):.1f} ç§’")
        
        final_results = self.analyze_results(results, evaluation_stats)
        
        # 5. ç”Ÿæˆéš¾åº¦-è¯„åˆ†æ›²çº¿å›¾
        if results:
            try:
                plot_path = self.generate_difficulty_score_plot(results)
                final_results["plot_path"] = plot_path
                logger.info(f"ğŸ“ˆ éš¾åº¦-è¯„åˆ†æ›²çº¿å›¾å·²ç”Ÿæˆ: {plot_path}")
            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆæ›²çº¿å›¾å¤±è´¥: {e}")
        
        # 6. ä¿å­˜æœ€ç»ˆç»“æœ
        self.save_final_results(final_results)
        
        return final_results
    
    def analyze_results(self, results: List[Dict], stats: Dict) -> Dict[str, Any]:
        """åˆ†æè¯„ä¼°ç»“æœ"""
        logger.info("ğŸ“Š åˆ†æè¯„ä¼°ç»“æœ...")
        
        if not results:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆç»“æœå¯åˆ†æ"}
        
        # ç»Ÿè®¡ç”ŸæˆçŠ¶æ€
        successful_generations = sum(1 for r in results if r.get('generation_status') == 'success')
        failed_generations = sum(1 for r in results if r.get('generation_status') == 'failed')
        total_generations = len(results)
        
        logger.info(f"ğŸ“ˆ ç”Ÿæˆç»Ÿè®¡:")
        logger.info(f"  - æˆåŠŸç”Ÿæˆ: {successful_generations}/{total_generations} ({successful_generations/total_generations*100:.1f}%)")
        logger.info(f"  - ç”Ÿæˆå¤±è´¥: {failed_generations}/{total_generations} ({failed_generations/total_generations*100:.1f}%)")
        
        # è®¡ç®—å¹³å‡åˆ†æ•°ï¼ˆåªè€ƒè™‘æˆåŠŸç”Ÿæˆçš„æ ·æœ¬ï¼‰
        scores = []
        difficulty_scores = {}
        difficulty_failure_rates = {}
        
        for result in results:
            evaluation = result.get('evaluation', {})
            difficulty = result.get('difficulty', 'unknown')
            generation_status = result.get('generation_status', 'unknown')
            
            # ç»Ÿè®¡æ¯ä¸ªéš¾åº¦çš„å¤±è´¥ç‡
            if difficulty not in difficulty_failure_rates:
                difficulty_failure_rates[difficulty] = {'total': 0, 'failed': 0}
            difficulty_failure_rates[difficulty]['total'] += 1
            if generation_status == 'failed':
                difficulty_failure_rates[difficulty]['failed'] += 1
            
            # åªè®¡ç®—æˆåŠŸç”Ÿæˆçš„æ ·æœ¬åˆ†æ•°
            if generation_status == 'success' and 'overall_score' in evaluation:
                score = evaluation['overall_score']
                scores.append(score)
                
                # æŒ‰éš¾åº¦åˆ†ç»„
                if difficulty not in difficulty_scores:
                    difficulty_scores[difficulty] = []
                difficulty_scores[difficulty].append(score)
        
        if scores:
            stats["average_scores"] = {
                "overall": sum(scores) / len(scores),
                "min": min(scores),
                "max": max(scores),
                "std": (sum((x - sum(scores)/len(scores))**2 for x in scores) / len(scores))**0.5
            }
        
        # éš¾åº¦åˆ†æ
        for difficulty, diff_scores in difficulty_scores.items():
            if diff_scores:
                stats["difficulty_analysis"][difficulty] = {
                    "count": len(diff_scores),
                    "average": sum(diff_scores) / len(diff_scores),
                    "min": min(diff_scores),
                    "max": max(diff_scores)
                }
        
        # æ·»åŠ å¤±è´¥ç‡ç»Ÿè®¡
        stats["generation_failure_analysis"] = {
            "overall_failure_rate": failed_generations / total_generations if total_generations > 0 else 0,
            "difficulty_failure_rates": {
                diff: {
                    "failure_rate": info['failed'] / info['total'] if info['total'] > 0 else 0,
                    "total_samples": info['total'],
                    "failed_samples": info['failed']
                }
                for diff, info in difficulty_failure_rates.items()
            }
                }
        
        return {
            "results": results,
            "statistics": stats,
            "summary": {
                "total_evaluated": len(results),
                "successful_generations": successful_generations,
                "failed_generations": failed_generations,
                "generation_success_rate": successful_generations / total_generations if total_generations > 0 else 0,
                "evaluation_success_rate": stats["successful_evaluations"] / stats["total_samples"] if stats["total_samples"] > 0 else 0,
                "average_overall_score": stats["average_scores"].get("overall", 0) if "average_scores" in stats else 0
            }
        }
    
    def generate_difficulty_score_plot(self, results: List[Dict], save_path: str = None, show_plot: bool = True):
        """Generate difficulty-score curve plot"""
        if not results:
            logger.warning("No result data available, cannot generate curve plot")
            return None
        
        # æ”¶é›†æ•°æ®
        difficulty_scores = {}
        for result in results:
            evaluation = result.get('evaluation', {})
            if 'overall_score' in evaluation:
                difficulty = result.get('difficulty', 'unknown')
                score = evaluation['overall_score']
                
                if difficulty not in difficulty_scores:
                    difficulty_scores[difficulty] = []
                difficulty_scores[difficulty].append(score)
        
        if not difficulty_scores:
            logger.warning("No valid score data available, cannot generate curve plot")
            return None
        
        # è®¡ç®—æ¯ä¸ªéš¾åº¦çš„å¹³å‡åˆ†æ•°
        difficulties = []
        avg_scores = []
        score_counts = []
        
        for difficulty in sorted(difficulty_scores.keys(), key=lambda x: float(x) if x != 'unknown' else -999):
            scores = difficulty_scores[difficulty]
            if scores:
                difficulties.append(float(difficulty) if difficulty != 'unknown' else -1)
                avg_scores.append(sum(scores) / len(scores))
                score_counts.append(len(scores))
        
        if not difficulties:
            logger.warning("No valid difficulty data available, cannot generate curve plot")
            return None
        
        # åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(12, 8))
        
        # ä¸»æ›²çº¿å›¾
        plt.subplot(2, 1, 1)
        plt.plot(difficulties, avg_scores, 'bo-', linewidth=2, markersize=8, label='Average Score')
        plt.fill_between(difficulties, avg_scores, alpha=0.3)
        
        plt.xlabel('Problem Difficulty', fontsize=12)
        plt.ylabel('Average Score', fontsize=12)
        plt.title(f'{self.model_name} - Difficulty-Score Curve (Evaluated: {len(results)} samples)', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        # æ·»åŠ æ•°æ®ç‚¹æ ‡æ³¨
        for i, (diff, score, count) in enumerate(zip(difficulties, avg_scores, score_counts)):
            plt.annotate(f'n={count}', (diff, score), textcoords="offset points", 
                        xytext=(0,10), ha='center', fontsize=8)
        
        # æ ·æœ¬æ•°é‡æŸ±çŠ¶å›¾
        plt.subplot(2, 1, 2)
        plt.bar(difficulties, score_counts, alpha=0.7, color='skyblue', edgecolor='navy')
        plt.xlabel('Problem Difficulty', fontsize=12)
        plt.ylabel('Sample Count', fontsize=12)
        plt.title('Sample Distribution by Difficulty Level', fontsize=12)
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i, (diff, count) in enumerate(zip(difficulties, score_counts)):
            plt.text(diff, count + 0.1, str(count), ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        if save_path is None:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs("data/plots", exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_safe_name = self.model_name.replace('/', '_').replace('-', '_')
            save_path = f"data/plots/difficulty_score_plot_{model_safe_name}_{timestamp}.png"
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logger.info(f"ğŸ“ˆ æ›²çº¿å›¾å·²ä¿å­˜: {save_path}")
        
        # æ˜¾ç¤ºå›¾è¡¨
        if show_plot:
            plt.show()
        else:
            plt.close()  # ä¸æ˜¾ç¤ºæ—¶å…³é—­å›¾è¡¨ä»¥èŠ‚çœå†…å­˜
        
        return save_path
    

    
    def save_intermediate_results(self, results: List[Dict], count: int):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        # åˆ›å»ºæ¨¡å‹ä¸“ç”¨ç›®å½•
        model_safe_name = self.model_name.replace('/', '_').replace('-', '_')
        model_dir = f"data/intermediate/{model_safe_name}"
        os.makedirs(model_dir, exist_ok=True)
        
        # ä½¿ç”¨è¿è¡ŒIDåˆ›å»ºå­ç›®å½•
        run_dir = f"{model_dir}/{self.run_id}"
        os.makedirs(run_dir, exist_ok=True)
        
        filename = f"{run_dir}/intermediate_results_{count}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: {filename}")
    
    def save_final_results(self, final_results: Dict[str, Any]):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs("data/results", exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_safe_name = self.model_name.replace('/', '_').replace('-', '_')
        filename = f"data/results/final_evaluation_{model_safe_name}_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ æœ€ç»ˆç»“æœå·²ä¿å­˜: {filename}")
        
        # æ‰“å°æ‘˜è¦
        summary = final_results.get("summary", {})
        logger.info(f"\nğŸ“‹ è¯„ä¼°æ‘˜è¦:")
        logger.info(f"æ€»è¯„ä¼°æ ·æœ¬: {summary.get('total_evaluated', 0)}")
        logger.info(f"æˆåŠŸç‡: {summary.get('success_rate', 0):.2%}")
        logger.info(f"å¹³å‡æ€»åˆ†: {summary.get('average_overall_score', 0):.2f}")
        
        stats = final_results.get("statistics", {})
        if "average_scores" in stats:
            avg_scores = stats["average_scores"]
            logger.info(f"åˆ†æ•°èŒƒå›´: {avg_scores.get('min', 0):.2f} - {avg_scores.get('max', 0):.2f}")
            logger.info(f"æ ‡å‡†å·®: {avg_scores.get('std', 0):.2f}")

# é¢„å®šä¹‰çš„æ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    "deepseek_r1_1.5b": {
        "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "type": "1.5b",
        "max_new_tokens": 500,
        "description": "DeepSeek-R1 1.5B æ¨¡å‹"
    },
    "deepseek_r1_7b": {
        "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B", 
        "type": "7b_quantized",
        "max_new_tokens": 600,
        "description": "DeepSeek-R1 7B æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰"
    },
    "deepseek_r1_14b": {
        "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        "type": "14b_quantized",
        "max_new_tokens": 700,
        "description": "DeepSeek-R1 14B æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰"
    },
    "deepseek_r1_32b": {
        "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
        "type": "32b_quantized",
        "max_new_tokens": 800,
        "description": "DeepSeek-R1 32B æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰"
    },
    "deepseek_r1_70b": {
        "name": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
        "type": "70b_quantized",
        "max_new_tokens": 1000,
        "description": "DeepSeek-R1 70B æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰"
    }
}

def run_math_evaluation(model_key: str = "deepseek_r1_1.5b", max_samples: int = 200):
    """è¿è¡Œæ•°å­¦è¯„ä¼°æµç¨‹"""
    if model_key not in MODEL_CONFIGS:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_key}ã€‚æ”¯æŒçš„æ¨¡å‹: {list(MODEL_CONFIGS.keys())}")
    
    model_config = MODEL_CONFIGS[model_key]
    DATASET_PATH = "data/processed/deepmath_evaluation_dataset.csv"
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    
    logger.info("ğŸš€ å¼€å§‹æ•°å­¦è¯„ä¼°æµç¨‹")
    logger.info(f"æ¨¡å‹: {model_config['description']}")
    logger.info(f"æ¨¡å‹åç§°: {model_config['name']}")
    logger.info(f"æ•°æ®é›†: {DATASET_PATH}")
    logger.info(f"æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
    
    if not OPENAI_API_KEY:
        logger.warning("âš ï¸ æœªè®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼Œå°†è·³è¿‡OpenAIæ‰“åˆ†")
        logger.info("è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export OPENAI_API_KEY='your-api-key'")
    
    # åˆ›å»ºè¯„ä¼°æ¡†æ¶
    framework = MathEvaluationFramework(
        model_config=model_config,
        openai_api_key=OPENAI_API_KEY,
        max_samples=max_samples
    )
    
    # è¿è¡Œè¯„ä¼°
    try:
        results = framework.run_evaluation(DATASET_PATH)
        logger.info("ğŸ‰ æ•°å­¦è¯„ä¼°å®Œæˆï¼")
        return results
        
    except Exception as e:
        logger.error(f"âŒ æ•°å­¦è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    # é»˜è®¤è¿è¡Œ1.5Bæ¨¡å‹è¯„ä¼°
    run_math_evaluation("deepseek_r1_1.5b", max_samples=200) 