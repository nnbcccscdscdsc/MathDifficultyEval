#!/usr/bin/env python3
"""
æ¨¡å‹åŠ è½½æµ‹è¯•è„šæœ¬

ç”¨äºæµ‹è¯•ä¸åŒæ¨¡å‹çš„åŠ è½½å’ŒGPUé…ç½®æ˜¯å¦æ­£ç¡®
"""

import os
import argparse
import logging
from pathlib import Path
import torch
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from scripts.model_evaluation import ModelEvaluator
from scripts.utils import setup_logging

def test_model_loading(model_name: str, quantization: str = "4bit", num_gpus: int = None):
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹åŠ è½½: {model_name}")
    print("="*60)
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    logger = logging.getLogger(__name__)
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    else:
        print("âŒ æœªæ£€æµ‹åˆ°CUDA GPU")
        return False
    
    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = ModelEvaluator()
        
        # ç¡®å®šGPUæ•°é‡
        if num_gpus is None:
            model_gpu_config = {
                "mistral-community/Mistral-7B-v0.2": 1,
                "lmsys/longchat-7b-16k": 1,
                "Yukang/LongAlpaca-13B-16k": 2,
                "Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-hf": 4,
                "Yukang/LongAlpaca-70B-16k": 4
            }
            num_gpus = model_gpu_config.get(model_name, 1)
        
        print(f"ğŸ“Š é…ç½®ä¿¡æ¯:")
        print(f"   æ¨¡å‹: {model_name}")
        print(f"   é‡åŒ–: {quantization}")
        print(f"   GPUæ•°é‡: {num_gpus}")
        
        # æ£€æŸ¥GPUå†…å­˜æ˜¯å¦è¶³å¤Ÿ
        if num_gpus > gpu_count:
            print(f"âŒ è¯·æ±‚çš„GPUæ•°é‡({num_gpus})è¶…è¿‡å¯ç”¨æ•°é‡({gpu_count})")
            return False
        
        # åŠ è½½æ¨¡å‹
        print(f"\nğŸ”„ å¼€å§‹åŠ è½½æ¨¡å‹...")
        evaluator.load_model(model_name, quantization, num_gpus)
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        
        # æµ‹è¯•ç®€å•æ¨ç†
        print(f"\nğŸ§  æµ‹è¯•æ¨ç†...")
        test_prompt = "What is 2 + 2?"
        try:
            response = evaluator.generate_answer(test_prompt, "{problem}")
            print(f"âœ… æ¨ç†æµ‹è¯•æˆåŠŸ")
            print(f"   è¾“å…¥: {test_prompt}")
            print(f"   è¾“å‡º: {response[:100]}...")
        except Exception as e:
            print(f"âŒ æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
            return False
        
        # æ¸…ç†å†…å­˜
        del evaluator
        torch.cuda.empty_cache()
        
        print(f"\nğŸ‰ æ¨¡å‹ {model_name} æµ‹è¯•å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ¨¡å‹åŠ è½½æµ‹è¯•è„šæœ¬")
    parser.add_argument("--model", type=str, required=True,
                       choices=[
                           "mistral-community/Mistral-7B-v0.2",
                           "lmsys/longchat-7b-16k", 
                           "Yukang/LongAlpaca-13B-16k",
                           "Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-hf",
                           "Yukang/LongAlpaca-70B-16k"
                       ],
                       help="è¦æµ‹è¯•çš„æ¨¡å‹")
    parser.add_argument("--quantization", type=str, default="4bit",
                       choices=["none", "4bit", "8bit"],
                       help="é‡åŒ–æ–¹å¼")
    parser.add_argument("--num-gpus", type=int, default=None,
                       help="GPUæ•°é‡ï¼ˆé»˜è®¤æ ¹æ®æ¨¡å‹è‡ªåŠ¨è®¾ç½®ï¼‰")
    
    args = parser.parse_args()
    
    print("ğŸš€ æ¨¡å‹åŠ è½½æµ‹è¯•å·¥å…·")
    print("="*60)
    
    success = test_model_loading(
        model_name=args.model,
        quantization=args.quantization,
        num_gpus=args.num_gpus
    )
    
    if success:
        print(f"\nâœ… æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹ {args.model} å¯ä»¥æ­£å¸¸ä½¿ç”¨")
        return 0
    else:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥ï¼è¯·æ£€æŸ¥é…ç½®å’ŒGPUèµ„æº")
        return 1

if __name__ == "__main__":
    exit(main()) 