#!/usr/bin/env python3
"""
æ¨¡æ‹Ÿè¯„ä¼°è„šæœ¬ï¼šä½¿ç”¨æ¨¡æ‹Ÿçš„æ¨¡å‹è¾“å‡ºæ¥æµ‹è¯•OpenAIè¯„åˆ†åŠŸèƒ½
"""

import os
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
import time
from datetime import datetime
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from scripts.openai_scorer import OpenAIScorer
from scripts.results_analysis import ResultsAnalyzer
from scripts.utils import ConfigLoader, setup_logging

class MockModelEvaluator:
    """æ¨¡æ‹Ÿæ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.config = ConfigLoader.load_config(config_path)
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–OpenAIè¯„åˆ†å™¨
        self.openai_scorer = None
        try:
            self.openai_scorer = OpenAIScorer(config_path)
            self.logger.info("OpenAIè¯„åˆ†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"OpenAIè¯„åˆ†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def generate_mock_answers(self, problems: List[Dict[str, Any]], model_name: str) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„æ¨¡å‹ç­”æ¡ˆ"""
        self.logger.info(f"ä¸ºæ¨¡å‹ {model_name} ç”Ÿæˆæ¨¡æ‹Ÿç­”æ¡ˆ")
        
        # æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´ç­”æ¡ˆè´¨é‡
        model_quality = {
            'llama-7b': 0.7,
            'llama-13b': 0.8,
            'llama-70b': 0.9
        }
        
        quality = model_quality.get(model_name, 0.7)
        
        mock_answers = []
        
        for i, problem in enumerate(problems):
            problem_text = problem['problem']
            expected_answer = problem['solution']
            difficulty = problem['difficulty']
            
            # æ ¹æ®éš¾åº¦å’Œæ¨¡å‹è´¨é‡ç”Ÿæˆä¸åŒè´¨é‡çš„ç­”æ¡ˆ
            if '2 + 3' in problem_text:
                if quality > 0.8:
                    generated_answer = "The answer is 5. This is a simple addition problem."
                elif quality > 0.7:
                    generated_answer = "5"
                else:
                    generated_answer = "I think it might be 6, but I'm not sure."
            
            elif '2x + 5 = 13' in problem_text:
                if quality > 0.8:
                    generated_answer = "Let's solve this step by step:\n1) 2x + 5 = 13\n2) 2x = 13 - 5\n3) 2x = 8\n4) x = 8/2\n5) x = 4\n\nThe answer is x = 4."
                elif quality > 0.7:
                    generated_answer = "2x + 5 = 13\n2x = 8\nx = 4"
                else:
                    generated_answer = "I think x might be 4, but I'm not confident about the steps."
            
            elif 'circle with radius 5' in problem_text:
                if quality > 0.8:
                    generated_answer = "The area of a circle is A = Ï€rÂ².\nGiven radius r = 5:\nA = Ï€ Ã— 5Â² = Ï€ Ã— 25 = 25Ï€ â‰ˆ 78.54 square units."
                elif quality > 0.7:
                    generated_answer = "Area = Ï€rÂ² = Ï€ Ã— 5Â² = 25Ï€"
                else:
                    generated_answer = "I think it's something with Ï€ and 25, but I'm not sure of the exact formula."
            
            elif 'sin(30Â°)' in problem_text:
                if quality > 0.8:
                    generated_answer = "sin(30Â°) = 1/2 = 0.5\nThis is a standard trigonometric value."
                elif quality > 0.7:
                    generated_answer = "sin(30Â°) = 1/2"
                else:
                    generated_answer = "I think it's 0.5, but I'm not certain."
            
            else:
                # é€šç”¨ç­”æ¡ˆç”Ÿæˆ
                if quality > 0.8:
                    generated_answer = f"I would solve this by following the standard mathematical procedures. The answer should be {expected_answer}."
                elif quality > 0.7:
                    generated_answer = f"The answer is {expected_answer}."
                else:
                    generated_answer = f"I'm not entirely sure, but I think it might be related to {expected_answer}."
            
            # æ·»åŠ ä¸€äº›éšæœºæ€§
            import random
            if random.random() < (1 - quality):
                generated_answer += " However, I'm not completely confident about this answer."
            
            mock_answers.append({
                'id': problem.get('id', f'mock_{i}'),
                'problem': problem_text,
                'expected_answer': expected_answer,
                'generated_answer': generated_answer,
                'difficulty': difficulty,
                'model_name': model_name,
                'generation_time': random.uniform(1.0, 3.0)  # æ¨¡æ‹Ÿç”Ÿæˆæ—¶é—´
            })
        
        return mock_answers
    
    def evaluate_with_openai(self, mock_answers: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨OpenAIå¯¹æ¨¡æ‹Ÿç­”æ¡ˆè¿›è¡Œè¯„åˆ†"""
        self.logger.info("å¼€å§‹OpenAIè¯„åˆ†")
        
        results = []
        
        for i, answer in enumerate(mock_answers):
            self.logger.info(f"è¯„åˆ†è¿›åº¦: {i+1}/{len(mock_answers)}")
            
            try:
                # ä½¿ç”¨OpenAIè¯„åˆ†
                openai_result = self.openai_scorer.score_answer(
                    problem=answer['problem'],
                    reference_answer=answer['expected_answer'],
                    student_answer=answer['generated_answer']
                )
                
                # è®¡ç®—å…¶ä»–æŒ‡æ ‡
                from scripts.utils import calculate_metrics
                metrics = calculate_metrics(answer['generated_answer'], answer['expected_answer'])
                
                # åˆå¹¶ç»“æœ
                result = {
                    **answer,
                    'openai_score': openai_result['openai_score'],
                    'openai_score_text': openai_result.get('score_text', ''),
                    **metrics
                }
                
                results.append(result)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                time.sleep(1)
                
            except Exception as e:
                self.logger.error(f"è¯„åˆ†å¤±è´¥: {e}")
                # ä½¿ç”¨é»˜è®¤åˆ†æ•°
                result = {
                    **answer,
                    'openai_score': 50.0,
                    'openai_score_text': f"è¯„åˆ†å¤±è´¥: {str(e)}",
                    'accuracy': 0.5,
                    'exact_match': 0.0,
                    'rouge_score': 0.3,
                    'bleu_score': 0.3,
                    'step_accuracy': 0.5
                }
                results.append(result)
        
        return results
    
    def save_results(self, results: List[Dict[str, Any]], model_name: str) -> str:
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜CSV
        df = pd.DataFrame(results)
        csv_file = self.results_dir / f"{model_name}_mock_{timestamp}.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8')
        
        # ä¿å­˜JSON
        json_file = self.results_dir / f"{model_name}_mock_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜: {csv_file}")
        return str(csv_file)
    
    def run_mock_evaluation(self, model_name: str, dataset_name: str = "sample", max_samples: Optional[int] = None):
        """è¿è¡Œæ¨¡æ‹Ÿè¯„ä¼°"""
        self.logger.info(f"å¼€å§‹æ¨¡æ‹Ÿè¯„ä¼°: {model_name}")
        
        # åŠ è½½æ•°æ®é›†
        data_path = Path("data/processed") / f"{dataset_name}.csv"
        if not data_path.exists():
            self.logger.error(f"æ•°æ®é›†ä¸å­˜åœ¨: {data_path}")
            return None
        
        df = pd.read_csv(data_path)
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if max_samples and len(df) > max_samples:
            df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        problems = df.to_dict('records')
        
        # ç”Ÿæˆæ¨¡æ‹Ÿç­”æ¡ˆ
        mock_answers = self.generate_mock_answers(problems, model_name)
        
        # OpenAIè¯„åˆ†
        results = self.evaluate_with_openai(mock_answers)
        
        # ä¿å­˜ç»“æœ
        result_file = self.save_results(results, model_name)
        
        # æ‰“å°æ‘˜è¦
        avg_openai_score = sum(r['openai_score'] for r in results) / len(results)
        avg_generation_time = sum(r['generation_time'] for r in results) / len(results)
        
        print(f"\nğŸ“Š {model_name} æ¨¡æ‹Ÿè¯„ä¼°ç»“æœ:")
        print(f"  æ ·æœ¬æ•°: {len(results)}")
        print(f"  å¹³å‡OpenAIè¯„åˆ†: {avg_openai_score:.2f}")
        print(f"  å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_generation_time:.2f}ç§’")
        
        return results

def main():
    parser = argparse.ArgumentParser(description="æ¨¡æ‹Ÿæ¨¡å‹è¯„ä¼°è„šæœ¬")
    parser.add_argument("--model", type=str, default="llama-7b",
                       choices=["llama-7b", "llama-13b", "llama-70b"],
                       help="è¦è¯„ä¼°çš„æ¨¡å‹")
    parser.add_argument("--dataset", type=str, default="sample",
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--max-samples", type=int, default=10,
                       help="æœ€å¤§æ ·æœ¬æ•°é‡")
    parser.add_argument("--config", type=str, default="configs/config.yaml",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–è¯„ä¼°å™¨
        evaluator = MockModelEvaluator(args.config)
        
        # è¿è¡Œæ¨¡æ‹Ÿè¯„ä¼°
        results = evaluator.run_mock_evaluation(
            model_name=args.model,
            dataset_name=args.dataset,
            max_samples=args.max_samples
        )
        
        if results:
            print(f"\nâœ… æ¨¡æ‹Ÿè¯„ä¼°å®Œæˆï¼")
            print(f"ğŸ“ ç»“æœæ–‡ä»¶: results/{args.model}_mock_*.csv")
            print(f"ğŸ” å¯ä»¥è¿è¡Œç»“æœåˆ†æ: python scripts/results_analysis.py --results-file results/{args.model}_mock_*.csv --model-name {args.model}")
        
    except Exception as e:
        print(f"âŒ æ¨¡æ‹Ÿè¯„ä¼°å¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 