#!/usr/bin/env python3
"""
æ¨¡å‹è¯„ä¼°è„šæœ¬ï¼šè¯„ä¼°ä¸åŒå‚æ•°çš„Llamaæ¨¡å‹åœ¨æ•°å­¦é¢˜ä¸Šçš„è¡¨ç°
"""

import os
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    BitsAndBytesConfig,
    pipeline
)
import yaml
from tqdm import tqdm
import sys
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from scripts.utils import ConfigLoader, setup_logging, calculate_metrics
from scripts.openai_scorer import OpenAIScorer

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.config = ConfigLoader.load_config(config_path)
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # è®¾å¤‡é…ç½®
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        
        # åˆå§‹åŒ–OpenAIè¯„åˆ†å™¨
        self.openai_scorer = None
        if self.config.get('openai_scoring', {}).get('enabled', False):
            try:
                self.openai_scorer = OpenAIScorer(config_path)
                self.logger.info("OpenAIè¯„åˆ†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"OpenAIè¯„åˆ†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.openai_scorer = None
    
    def load_model(self, model_name: str, quantization: str = "4bit"):
        """åŠ è½½æ¨¡å‹"""
        self.logger.info(f"åŠ è½½æ¨¡å‹: {model_name}, é‡åŒ–: {quantization}")
        
        model_config = self.config['models'][model_name]
        quant_config = self.config['quantization'][quantization]
        
        # é…ç½®é‡åŒ–å‚æ•°
        quantization_config = None
        if quantization != "none":
            quantization_config = BitsAndBytesConfig(**quant_config)
        
        try:
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_config['model_name'],
                trust_remote_code=True
            )
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                model_config['model_name'],
                quantization_config=quantization_config,
                device_map="auto" if self.device == "cuda" else None,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                trust_remote_code=True
            )
            
            # åˆ›å»ºpipeline
            if quantization != "none":
                # ä½¿ç”¨é‡åŒ–æ—¶ï¼Œä¸æŒ‡å®šdeviceï¼Œè®©accelerateè‡ªåŠ¨å¤„ç†
                self.pipeline = pipeline(
                    "text-generation",
                    model=self.model,
                    tokenizer=self.tokenizer,
                    **{k: v for k, v in model_config.items() if k != 'model_name'}
                )
            else:
                # ä¸ä½¿ç”¨é‡åŒ–æ—¶ï¼Œå¯ä»¥æŒ‡å®šdevice
                self.pipeline = pipeline(
                    "text-generation",
                    model=self.model,
                    tokenizer=self.tokenizer,
                    device=self.device,
                    **{k: v for k, v in model_config.items() if k != 'model_name'}
                )
            
            self.logger.info(f"æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def load_dataset(self, dataset_name: str) -> pd.DataFrame:
        """åŠ è½½æ•°æ®é›†"""
        self.logger.info(f"åŠ è½½æ•°æ®é›†: {dataset_name}")
        
        data_path = Path("data/processed") / f"{dataset_name}.csv"
        if not data_path.exists():
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        
        df = pd.read_csv(data_path)
        self.logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(df)} ä¸ªæ ·æœ¬")
        return df
    
    def generate_answer(self, problem: str, prompt_template: str) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        try:
            # æ„å»ºæç¤º
            prompt = prompt_template.format(problem=problem)
            
            # ç”Ÿæˆå›ç­” - ä½¿ç”¨æ›´å®‰å…¨çš„å‚æ•°
            response = self.pipeline(
                prompt,
                max_new_tokens=128,  # è¿›ä¸€æ­¥å‡å°‘ç”Ÿæˆé•¿åº¦
                do_sample=True,
                temperature=0.8,     # æé«˜æ¸©åº¦ï¼Œé¿å…æ•°å€¼é—®é¢˜
                top_p=0.9,           # é™ä½top_p
                top_k=40,            # é™ä½top_k
                repetition_penalty=1.0,  # ç§»é™¤é‡å¤æƒ©ç½š
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                return_full_text=False  # åªè¿”å›æ–°ç”Ÿæˆçš„æ–‡æœ¬
            )
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            if isinstance(response, list) and len(response) > 0:
                generated_text = response[0].get('generated_text', '')
                if generated_text:
                    answer = generated_text.strip()
                else:
                    answer = "ç”Ÿæˆå¤±è´¥ï¼šæ— è¾“å‡º"
            else:
                answer = "ç”Ÿæˆå¤±è´¥ï¼šå“åº”æ ¼å¼é”™è¯¯"
            
            return answer
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def evaluate_dataset(self, dataset_name: str, max_samples: Optional[int] = None) -> Dict[str, Any]:
        """è¯„ä¼°æ•°æ®é›†"""
        self.logger.info(f"å¼€å§‹è¯„ä¼°æ•°æ®é›†: {dataset_name}")
        
        # åŠ è½½æ•°æ®é›†
        df = self.load_dataset(dataset_name)
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if max_samples and len(df) > max_samples:
            df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)
            self.logger.info(f"é™åˆ¶æ ·æœ¬æ•°é‡ä¸º: {max_samples}")
        
        # è·å–æ•°æ®é›†é…ç½®
        dataset_config = self.config['datasets'].get(dataset_name, {})
        prompt_template = dataset_config.get('prompt_template', "Problem: {problem}\n\nSolution:")
        
        results = []
        
        # æŒ‰éš¾åº¦åˆ†ç»„è¯„ä¼°
        for difficulty in ['elementary', 'middle', 'college']:
            difficulty_df = df[df['difficulty'] == difficulty]
            
            if len(difficulty_df) == 0:
                self.logger.warning(f"éš¾åº¦ç­‰çº§ {difficulty} æ²¡æœ‰æ•°æ®")
                continue
            
            self.logger.info(f"è¯„ä¼°éš¾åº¦ç­‰çº§: {difficulty}, æ ·æœ¬æ•°: {len(difficulty_df)}")
            
            difficulty_results = []
            
            for idx, row in tqdm(difficulty_df.iterrows(), total=len(difficulty_df), desc=f"è¯„ä¼° {difficulty}"):
                problem = row['problem']
                expected_answer = row.get('solution', '')
                
                # ç”Ÿæˆç­”æ¡ˆ
                start_time = time.time()
                generated_answer = self.generate_answer(problem, prompt_template)
                generation_time = time.time() - start_time
                
                # è®¡ç®—åŸºç¡€æŒ‡æ ‡
                metrics = calculate_metrics(generated_answer, expected_answer)
                
                # æ·»åŠ OpenAIè¯„åˆ†
                if self.openai_scorer:
                    try:
                        openai_result = self.openai_scorer.score_answer(
                            problem=problem,
                            reference_answer=expected_answer,
                            student_answer=generated_answer
                        )
                        metrics['openai_score'] = openai_result['openai_score']
                        metrics['openai_score_text'] = openai_result.get('score_text', '')
                    except Exception as e:
                        self.logger.warning(f"OpenAIè¯„åˆ†å¤±è´¥: {e}")
                        metrics['openai_score'] = 50.0
                        metrics['openai_score_text'] = f"è¯„åˆ†å¤±è´¥: {str(e)}"
                else:
                    metrics['openai_score'] = 50.0
                    metrics['openai_score_text'] = "OpenAIè¯„åˆ†æœªå¯ç”¨"
                
                result = {
                    'id': row.get('id', idx),
                    'problem': problem,
                    'expected_answer': expected_answer,
                    'generated_answer': generated_answer,
                    'difficulty': difficulty,
                    'generation_time': generation_time,
                    **metrics
                }
                
                difficulty_results.append(result)
            
            # è®¡ç®—è¯¥éš¾åº¦ç­‰çº§çš„å¹³å‡æŒ‡æ ‡
            if difficulty_results:
                avg_metrics = {}
                for key in ['accuracy', 'exact_match', 'rouge_score', 'bleu_score']:
                    if key in difficulty_results[0]:
                        values = [r[key] for r in difficulty_results if key in r]
                        avg_metrics[f'avg_{key}'] = sum(values) / len(values) if values else 0
                
                self.logger.info(f"éš¾åº¦ {difficulty} å¹³å‡æŒ‡æ ‡: {avg_metrics}")
                results.extend(difficulty_results)
        
        return results
    
    def save_results(self, results: List[Dict], model_name: str, dataset_name: str):
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = self.results_dir / f"{model_name}_{dataset_name}_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜CSVæ ¼å¼
        df_results = pd.DataFrame(results)
        csv_file = self.results_dir / f"{model_name}_{dataset_name}_{timestamp}.csv"
        df_results.to_csv(csv_file, index=False, encoding='utf-8')
        
        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        summary = self.generate_summary(results, model_name, dataset_name)
        summary_file = self.results_dir / f"{model_name}_{dataset_name}_{timestamp}_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {self.results_dir}")
        return summary
    
    def generate_summary(self, results: List[Dict], model_name: str, dataset_name: str) -> Dict[str, Any]:
        """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
        if not results:
            return {}
        
        df = pd.DataFrame(results)
        
        summary = {
            'model_name': model_name,
            'dataset_name': dataset_name,
            'total_samples': len(results),
            'evaluation_time': datetime.now().isoformat(),
            'overall_metrics': {},
            'difficulty_metrics': {}
        }
        
        # æ€»ä½“æŒ‡æ ‡
        for metric in ['accuracy', 'exact_match', 'rouge_score', 'bleu_score']:
            if metric in df.columns:
                summary['overall_metrics'][metric] = df[metric].mean()
        
        # æŒ‰éš¾åº¦åˆ†ç»„çš„æŒ‡æ ‡
        for difficulty in df['difficulty'].unique():
            difficulty_df = df[df['difficulty'] == difficulty]
            summary['difficulty_metrics'][difficulty] = {
                'sample_count': len(difficulty_df),
                'avg_generation_time': difficulty_df['generation_time'].mean()
            }
            
            for metric in ['accuracy', 'exact_match', 'rouge_score', 'bleu_score']:
                if metric in difficulty_df.columns:
                    summary['difficulty_metrics'][difficulty][f'avg_{metric}'] = difficulty_df[metric].mean()
        
        return summary

def main():
    parser = argparse.ArgumentParser(description="æ¨¡å‹è¯„ä¼°è„šæœ¬")
    parser.add_argument("--model", type=str, default="mistral-7b",
                       choices=["mistral-7b", "longalpaca-7b"],
                       help="è¦è¯„ä¼°çš„æ¨¡å‹ï¼ˆå¿«é€Ÿæµ‹è¯•ç‰ˆæœ¬ï¼‰")
    parser.add_argument("--quantization", type=str, default="4bit",
                       choices=["none", "4bit", "8bit"],
                       help="é‡åŒ–æ–¹å¼")
    parser.add_argument("--dataset", type=str, default="sample",
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--max-samples", type=int, default=None,
                       help="æœ€å¤§æ ·æœ¬æ•°é‡")
    parser.add_argument("--config", type=str, default="configs/config.yaml",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = ModelEvaluator(args.config)
    
    try:
        # åŠ è½½æ¨¡å‹
        evaluator.load_model(args.model, args.quantization)
        
        # è¯„ä¼°æ•°æ®é›†
        results = evaluator.evaluate_dataset(args.dataset, args.max_samples)
        
        # ä¿å­˜ç»“æœ
        summary = evaluator.save_results(results, args.model, args.dataset)
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
        print("="*60)
        print(f"æ¨¡å‹: {summary['model_name']}")
        print(f"æ•°æ®é›†: {summary['dataset_name']}")
        print(f"æ€»æ ·æœ¬æ•°: {summary['total_samples']}")
        
        print("\nğŸ“ˆ æ€»ä½“æŒ‡æ ‡:")
        for metric, value in summary['overall_metrics'].items():
            print(f"  {metric}: {value:.4f}")
        
        print("\nğŸ¯ å„éš¾åº¦ç­‰çº§æŒ‡æ ‡:")
        for difficulty, metrics in summary['difficulty_metrics'].items():
            print(f"\n  {difficulty.upper()}:")
            print(f"    æ ·æœ¬æ•°: {metrics['sample_count']}")
            print(f"    å¹³å‡ç”Ÿæˆæ—¶é—´: {metrics['avg_generation_time']:.2f}ç§’")
            for key, value in metrics.items():
                if key.startswith('avg_') and key != 'avg_generation_time':
                    print(f"    {key}: {value:.4f}")
        
        print("\nâœ… è¯„ä¼°å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 