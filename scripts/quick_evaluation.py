#!/usr/bin/env python3
"""
å¿«é€Ÿè¯„ä¼°è„šæœ¬ï¼šå•ä¸ªæ¨¡å‹çš„æ•°å­¦é¢˜å›ç­”å’ŒOpenAIæ‰“åˆ†

ä½¿ç”¨æ–¹æ³•ï¼š
python scripts/quick_evaluation.py --model mistral-community/Mistral-7B-v0.2 --max-samples 10
"""

import os
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
import time
from datetime import datetime
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from scripts.model_evaluation import ModelEvaluator
from scripts.utils import setup_logging

def quick_evaluation(model_name: str, dataset_name: str = "deepmath_evaluation_dataset", 
                    max_samples: int = 10, quantization: str = "4bit"):
    """å¿«é€Ÿè¯„ä¼°å•ä¸ªæ¨¡å‹"""
    print(f"\nğŸš€ å¼€å§‹å¿«é€Ÿè¯„ä¼°: {model_name}")
    print("="*60)
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    logger = logging.getLogger(__name__)
    
    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = ModelEvaluator()
        
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_name}")
        evaluator.load_model(model_name, quantization)
        
        # è¯„ä¼°æ•°æ®é›†
        print(f"ğŸ§® è¯„ä¼°æ•°æ®é›†: {dataset_name} (æ ·æœ¬æ•°: {max_samples})")
        results = evaluator.evaluate_dataset(dataset_name, max_samples)
        
        if not results:
            print("âŒ è¯„ä¼°ç»“æœä¸ºç©º")
            return None
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        summary = evaluator.save_results(results, model_name, dataset_name)
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(results)
        
        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼")
        print(f"ğŸ“Š è¯„ä¼°ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(results)}")
        
        # æ‰“å°OpenAIè¯„åˆ†ç»Ÿè®¡
        if 'openai_score' in df.columns:
            avg_openai_score = df['openai_score'].mean()
            min_openai_score = df['openai_score'].min()
            max_openai_score = df['openai_score'].max()
            print(f"   OpenAIè¯„åˆ†:")
            print(f"     å¹³å‡åˆ†: {avg_openai_score:.2f}")
            print(f"     æœ€ä½åˆ†: {min_openai_score:.2f}")
            print(f"     æœ€é«˜åˆ†: {max_openai_score:.2f}")
        
        # æ‰“å°å‡†ç¡®ç‡ç»Ÿè®¡
        if 'accuracy' in df.columns:
            avg_accuracy = df['accuracy'].mean()
            print(f"   å‡†ç¡®ç‡: {avg_accuracy:.4f}")
        
        # æ˜¾ç¤ºå‡ ä¸ªç¤ºä¾‹
        print(f"\nğŸ“ ç¤ºä¾‹ç»“æœ:")
        for i, result in enumerate(results[:3], 1):
            print(f"\n   ç¤ºä¾‹ {i}:")
            print(f"   é—®é¢˜: {result['problem'][:100]}...")
            print(f"   ç­”æ¡ˆ: {result['answer'][:100]}...")
            if 'openai_score' in result:
                print(f"   OpenAIè¯„åˆ†: {result['openai_score']:.2f}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = f"results/{model_name.replace('/', '_')}_{dataset_name}_{timestamp}.csv"
        df.to_csv(results_file, index=False)
        print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_file}")
        
        return {
            'model_name': model_name,
            'total_samples': len(results),
            'avg_openai_score': df['openai_score'].mean() if 'openai_score' in df.columns else 0,
            'avg_accuracy': df['accuracy'].mean() if 'accuracy' in df.columns else 0,
            'results_file': results_file
        }
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
        return None
    finally:
        # æ¸…ç†å†…å­˜
        try:
            del evaluator
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except:
            pass

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¿«é€Ÿè¯„ä¼°è„šæœ¬")
    parser.add_argument("--model", type=str, required=True,
                       choices=[
                           "mistral-community/Mistral-7B-v0.2",
                           "lmsys/longchat-7b-16k", 
                           "Yukang/LongAlpaca-13B-16k",
                           "Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-hf",
                           "Yukang/LongAlpaca-70B-16k"
                       ],
                       help="è¦è¯„ä¼°çš„æ¨¡å‹")
    parser.add_argument("--dataset", type=str, default="deepmath_evaluation_dataset",
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--max-samples", type=int, default=10,
                       help="æœ€å¤§æ ·æœ¬æ•°é‡")
    parser.add_argument("--quantization", type=str, default="4bit",
                       choices=["none", "4bit", "8bit"],
                       help="é‡åŒ–æ–¹å¼")
    
    args = parser.parse_args()
    
    print("ğŸ¯ å¿«é€Ÿè¯„ä¼°å·¥å…·")
    print("="*60)
    print(f"æ¨¡å‹: {args.model}")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"æ ·æœ¬æ•°: {args.max_samples}")
    print(f"é‡åŒ–: {args.quantization}")
    print("="*60)
    
    # æ‰§è¡Œè¯„ä¼°
    result = quick_evaluation(
        model_name=args.model,
        dataset_name=args.dataset,
        max_samples=args.max_samples,
        quantization=args.quantization
    )
    
    if result:
        print(f"\nğŸ‰ è¯„ä¼°æˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“Š æ¨¡å‹: {result['model_name']}")
        print(f"ğŸ“Š æ ·æœ¬æ•°: {result['total_samples']}")
        print(f"ğŸ“Š å¹³å‡OpenAIè¯„åˆ†: {result['avg_openai_score']:.2f}")
        print(f"ğŸ“Š å¹³å‡å‡†ç¡®ç‡: {result['avg_accuracy']:.4f}")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶: {result['results_file']}")
    else:
        print(f"\nâŒ è¯„ä¼°å¤±è´¥ï¼")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 