#!/usr/bin/env python3
"""
å•ä¸ªæ¨¡å‹æµ‹è¯•è„šæœ¬ï¼šæµ‹è¯•æŒ‡å®šçš„å•ä¸ªæ¨¡å‹
"""

import os
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
import time
from datetime import datetime
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from scripts.model_evaluation import ModelEvaluator
from scripts.results_analysis import ResultsAnalyzer
from scripts.utils import ConfigLoader, setup_logging

class SingleModelTest:
    """å•ä¸ªæ¨¡å‹æµ‹è¯•å™¨"""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.config = ConfigLoader.load_config(config_path)
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        setup_logging()
        self.logger = logging.getLogger(__name__)
    
    def test_single_model(self, model_name: str, max_samples: int = 5):
        """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
        print("="*60)
        print(f"ğŸš€ æµ‹è¯•æ¨¡å‹: {model_name}")
        print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {max_samples}")
        print("="*60)
        
        try:
            # åˆ›å»ºè¯„ä¼°å™¨
            evaluator = ModelEvaluator()
            
            # åŠ è½½æ¨¡å‹
            self.logger.info(f"åŠ è½½æ¨¡å‹: {model_name}")
            evaluator.load_model(model_name, "4bit")
            
            # è¯„ä¼°æ•°æ®é›†
            self.logger.info("å¼€å§‹è¯„ä¼°æ•°æ®é›†")
            results = evaluator.evaluate_dataset("sample", max_samples)
            
            # ä¿å­˜ç»“æœ
            summary = evaluator.save_results(results, model_name, "sample")
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(results)
            
            self.logger.info(f"æ¨¡å‹ {model_name} è¯„ä¼°å®Œæˆï¼Œå…± {len(results)} ä¸ªæ ·æœ¬")
            
            # æ‰“å°æ‘˜è¦
            if 'openai_score' in df.columns:
                avg_openai_score = df['openai_score'].mean()
                self.logger.info(f"å¹³å‡OpenAIè¯„åˆ†: {avg_openai_score:.2f}")
            
            # ç”Ÿæˆåˆ†æå›¾è¡¨
            self.generate_analysis(df, model_name)
            
            # æ¸…ç†å†…å­˜
            del evaluator
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            print("\n" + "="*60)
            print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
            print("="*60)
            print(f"ğŸ“ ç»“æœæ–‡ä»¶: results/")
            print(f"ğŸ“ˆ å›¾è¡¨æ–‡ä»¶: results/plots/")
            
            return True
            
        except Exception as e:
            self.logger.error(f"æµ‹è¯•æ¨¡å‹ {model_name} å¤±è´¥: {e}")
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            
            # æ¸…ç†GPUç¼“å­˜
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return False
    
    def generate_analysis(self, df: pd.DataFrame, model_name: str):
        """ç”Ÿæˆåˆ†æå›¾è¡¨"""
        self.logger.info("ç”Ÿæˆåˆ†æå›¾è¡¨")
        
        # åˆ›å»ºåˆ†æå™¨
        analyzer = ResultsAnalyzer()
        
        # ç”Ÿæˆéš¾åº¦åˆ†æ
        analyzer.analyze_accuracy_by_difficulty(df, model_name)
        
        # ç”Ÿæˆé”™è¯¯æ¨¡å¼åˆ†æ
        analyzer.analyze_error_patterns(df, model_name)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        self.generate_detailed_report(df, model_name)
    
    def generate_detailed_report(self, df: pd.DataFrame, model_name: str):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        report = f"""
# å•ä¸ªæ¨¡å‹æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è§ˆ
- æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- æµ‹è¯•æ¨¡å‹: {model_name}
- æ ·æœ¬æ•°é‡: {len(df)}

## æ€§èƒ½æŒ‡æ ‡
"""
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        metrics = {
            'accuracy': df['accuracy'].mean() if 'accuracy' in df.columns else 0,
            'exact_match': df['exact_match'].mean() if 'exact_match' in df.columns else 0,
            'rouge_score': df['rouge_score'].mean() if 'rouge_score' in df.columns else 0,
            'bleu_score': df['bleu_score'].mean() if 'bleu_score' in df.columns else 0,
            'openai_score': df['openai_score'].mean() if 'openai_score' in df.columns else 0,
            'generation_time': df['generation_time'].mean() if 'generation_time' in df.columns else 0
        }
        
        for metric, value in metrics.items():
            report += f"- {metric}: {value:.4f}\n"
        
        # æŒ‰éš¾åº¦åˆ†ç»„
        report += "\n## å„éš¾åº¦ç­‰çº§è¡¨ç°\n"
        for difficulty in ['elementary', 'middle', 'college']:
            difficulty_df = df[df['difficulty'] == difficulty]
            if len(difficulty_df) > 0:
                avg_score = difficulty_df['openai_score'].mean() if 'openai_score' in difficulty_df.columns else 0
                report += f"- {difficulty}: {avg_score:.2f}åˆ† ({len(difficulty_df)}ä¸ªæ ·æœ¬)\n"
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.results_dir / f"single_model_test_{model_name}_{timestamp}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.logger.info(f"è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return str(report_file)

def main():
    parser = argparse.ArgumentParser(description="å•ä¸ªæ¨¡å‹æµ‹è¯•è„šæœ¬")
    parser.add_argument("--model", type=str, required=True,
                       choices=["mistral-7b", "longalpaca-7b"],
                       help="è¦æµ‹è¯•çš„æ¨¡å‹")
    parser.add_argument("--max-samples", type=int, default=5,
                       help="æœ€å¤§æ ·æœ¬æ•°é‡")
    parser.add_argument("--config", type=str, default="configs/config.yaml",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–æµ‹è¯•å™¨
        tester = SingleModelTest(args.config)
        
        # è¿è¡Œæµ‹è¯•
        success = tester.test_single_model(args.model, args.max_samples)
        
        if success:
            print("âœ… æµ‹è¯•æˆåŠŸå®Œæˆï¼")
            return 0
        else:
            print("âŒ æµ‹è¯•å¤±è´¥ï¼")
            return 1
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    exit(main()) 