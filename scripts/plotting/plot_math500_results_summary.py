#!/usr/bin/env python3
"""
MATH-500å››ä¸ªæ¨¡å‹ç»“æœæ±‡æ€»å¯¹æ¯”å›¾
ç”Ÿæˆç±»ä¼¼è®ºæ–‡é£æ ¼çš„æ±‡æ€»å¯¹æ¯”å›¾è¡¨
"""

import os
import json
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
from typing import Dict, List, Any
import seaborn as sns

# è®¾ç½®ç»˜å›¾æ ·å¼
plt.style.use('default')
plt.rcParams['font.size'] = 12
plt.rcParams['axes.linewidth'] = 1.2
plt.rcParams['grid.linewidth'] = 0.8
plt.rcParams['grid.alpha'] = 0.3

class Math500SummaryPlotter:
    """MATH-500æ±‡æ€»ç»˜å›¾å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç»˜å›¾å™¨"""
        self.results_dir = Path("data/math500_results/deepseek-ai")
        self.plot_dir = Path("plot_data")
        self.plot_dir.mkdir(exist_ok=True)
        
        # æ¨¡å‹é…ç½® - ä½¿ç”¨æ›´ä¸“ä¸šçš„é¢œè‰²
        self.models = {
            "DeepSeek-R1-Distill-Qwen-1.5B": {
                "short_name": "1.5B",
                "color": "#FF6B6B",
                "marker": "o",
                "linewidth": 2
            },
            "DeepSeek-R1-Distill-Qwen-7B": {
                "short_name": "7B", 
                "color": "#4ECDC4",
                "marker": "s",
                "linewidth": 2
            },
            "DeepSeek-R1-Distill-Qwen-14B": {
                "short_name": "14B",
                "color": "#45B7D1", 
                "marker": "^",
                "linewidth": 2
            },
            "DeepSeek-R1-Distill-Qwen-32B": {
                "short_name": "32B",
                "color": "#96CEB4",
                "marker": "D",
                "linewidth": 2
            }
        }
    
    def load_model_results(self, model_name: str) -> List[Dict]:
        """åŠ è½½æŒ‡å®šæ¨¡å‹çš„ç»“æœ"""
        model_dir = self.results_dir / model_name
        
        if not model_dir.exists():
            print(f"âš ï¸ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
            return []
        
        # æŸ¥æ‰¾æœ€æ–°çš„ç»“æœç›®å½•
        run_dirs = list(model_dir.glob("*"))
        if not run_dirs:
            print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç»“æœç›®å½•: {model_dir}")
            return []
        
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
        latest_run_dir = max(run_dirs, key=lambda x: x.stat().st_ctime)
        results_file = latest_run_dir / "final_results.json"
        
        if not results_file.exists():
            print(f"âš ï¸ ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {results_file}")
            return []
        
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            print(f"âœ… åŠ è½½ {model_name} ç»“æœ: {len(results)} ä¸ªæ ·æœ¬")
            return results
        except Exception as e:
            print(f"âŒ åŠ è½½ç»“æœå¤±è´¥: {e}")
            return []
    
    def analyze_results_by_difficulty(self, results: List[Dict]) -> Dict[int, List[float]]:
        """æŒ‰éš¾åº¦ç­‰çº§åˆ†æç»“æœ"""
        difficulty_scores = {}
        
        for result in results:
            evaluation = result.get('evaluation', {})
            if isinstance(evaluation, dict) and 'overall_score' in evaluation:
                level = result.get('level', 0)
                score = evaluation['overall_score']
                
                if level not in difficulty_scores:
                    difficulty_scores[level] = []
                difficulty_scores[level].append(score)
        
        return difficulty_scores
    
    def create_summary_plot(self, all_difficulty_data: Dict[str, Dict[int, List[float]]]):
        """åˆ›å»ºæ±‡æ€»å¯¹æ¯”å›¾"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # å·¦ä¾§ï¼šæŒ‰éš¾åº¦ç­‰çº§çš„æ€§èƒ½å¯¹æ¯”ï¼ˆæŠ˜çº¿å›¾ï¼‰
        ax1.set_title('(a) Average DeepSeek R1 Series Model Performance Analysis by Difficulty', 
                     fontsize=14, fontweight='bold', pad=20)
        
        # æ”¶é›†æ‰€æœ‰éš¾åº¦ç­‰çº§
        all_levels = set()
        for model_data in all_difficulty_data.values():
            all_levels.update(model_data.keys())
        all_levels = sorted(all_levels)
        
        # ç»˜åˆ¶æ¯ä¸ªæ¨¡å‹çš„æ€§èƒ½æ›²çº¿
        for model_name, difficulty_data in all_difficulty_data.items():
            if not difficulty_data:
                continue
                
            config = self.models[model_name]
            levels = []
            avg_scores = []
            
            for level in all_levels:
                if level in difficulty_data:
                    scores = difficulty_data[level]
                    levels.append(level)
                    avg_scores.append(np.mean(scores))
            
            if levels:
                ax1.plot(levels, avg_scores, 
                        color=config['color'], 
                        marker=config['marker'],
                        linewidth=config['linewidth'],
                        markersize=8,
                        label=config['short_name'])
        
        ax1.set_xlabel('Problem Difficulty', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Average Score', fontsize=12, fontweight='bold')
        ax1.set_xlim(0, 6)  # MATH-500åªæœ‰1-5çº§ï¼Œä½†ç•™ä¸€äº›è¾¹è·
        ax1.set_ylim(0, 10)
        ax1.grid(True, alpha=0.3)
        ax1.legend(loc='best', frameon=True, fancybox=True, shadow=True)
        
        # æ·»åŠ å³°å€¼ç‚¹æ ‡æ³¨
        ax1.text(1, 9.5, 'Peak Point', fontsize=10, ha='center', va='bottom',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))
        ax1.text(5, 9.5, 'Peak Point', fontsize=10, ha='center', va='bottom',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))
        
        # å³ä¾§ï¼šæ ·æœ¬æ•°é‡åˆ†å¸ƒï¼ˆæŸ±çŠ¶å›¾ï¼‰
        ax2.set_title('(b) Sample Count Distribution by Difficulty', 
                     fontsize=14, fontweight='bold', pad=20)
        
        # ç»Ÿè®¡æ¯ä¸ªéš¾åº¦ç­‰çº§çš„æ ·æœ¬æ•°é‡
        level_counts = {}
        total_samples = 0
        
        for model_name, difficulty_data in all_difficulty_data.items():
            for level, scores in difficulty_data.items():
                if level not in level_counts:
                    level_counts[level] = 0
                level_counts[level] += len(scores)
                total_samples += len(scores)
        
        # ç»˜åˆ¶æŸ±çŠ¶å›¾
        levels = sorted(level_counts.keys())
        counts = [level_counts[level] for level in levels]
        
        bars = ax2.bar(levels, counts, color='skyblue', alpha=0.7, edgecolor='navy', linewidth=1)
        ax2.set_xlabel('Problem Difficulty', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Total Sample Count', fontsize=12, fontweight='bold')
        ax2.set_xlim(0, 6)
        ax2.grid(True, alpha=0.3, axis='y')
        
        # æ·»åŠ æ€»æ ·æœ¬æ•°æ ‡æ³¨
        ax2.text(0.5, 0.95, f'Total: {total_samples:,}', 
                transform=ax2.transAxes, fontsize=12, fontweight='bold',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgray', alpha=0.8))
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, count in zip(bars, counts):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + max(counts)*0.01,
                    f'{count}', ha='center', va='bottom', fontsize=10, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(self.plot_dir / 'math500_summary_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… æ±‡æ€»å¯¹æ¯”å›¾å·²ä¿å­˜: {self.plot_dir / 'math500_summary_comparison.png'}")
        
        return total_samples
    
    def create_performance_table(self, all_difficulty_data: Dict[str, Dict[int, List[float]]]):
        """åˆ›å»ºæ€§èƒ½å¯¹æ¯”è¡¨æ ¼"""
        # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„æ€»ä½“ç»Ÿè®¡
        model_stats = {}
        
        for model_name, difficulty_data in all_difficulty_data.items():
            if not difficulty_data:
                continue
                
            all_scores = []
            for scores in difficulty_data.values():
                all_scores.extend(scores)
            
            if all_scores:
                model_stats[model_name] = {
                    'short_name': self.models[model_name]['short_name'],
                    'total_samples': len(all_scores),
                    'mean_score': np.mean(all_scores),
                    'std_score': np.std(all_scores),
                    'min_score': np.min(all_scores),
                    'max_score': np.max(all_scores),
                    'median_score': np.median(all_scores)
                }
        
        # åˆ›å»ºè¡¨æ ¼
        fig, ax = plt.subplots(figsize=(12, 4))
        ax.axis('tight')
        ax.axis('off')
        
        # å‡†å¤‡è¡¨æ ¼æ•°æ®
        table_data = []
        headers = ['Model', 'Samples', 'Mean Score', 'Std Dev', 'Min Score', 'Max Score', 'Median Score']
        
        for model_name, stats in model_stats.items():
            table_data.append([
                stats['short_name'],
                stats['total_samples'],
                f"{stats['mean_score']:.2f}",
                f"{stats['std_score']:.2f}",
                f"{stats['min_score']:.2f}",
                f"{stats['max_score']:.2f}",
                f"{stats['median_score']:.2f}"
            ])
        
        # åˆ›å»ºè¡¨æ ¼
        table = ax.table(cellText=table_data, colLabels=headers, 
                        cellLoc='center', loc='center',
                        colWidths=[0.15, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12])
        
        # è®¾ç½®è¡¨æ ¼æ ·å¼
        table.auto_set_font_size(False)
        table.set_fontsize(11)
        table.scale(1.2, 1.5)
        
        # è®¾ç½®è¡¨å¤´æ ·å¼
        for i in range(len(headers)):
            table[(0, i)].set_facecolor('#4CAF50')
            table[(0, i)].set_text_props(weight='bold', color='white')
        
        # è®¾ç½®æ•°æ®è¡Œæ ·å¼
        for i in range(1, len(table_data) + 1):
            for j in range(len(headers)):
                if i % 2 == 0:
                    table[(i, j)].set_facecolor('#F5F5F5')
        
        ax.set_title('MATH-500 Model Performance Summary', fontsize=16, fontweight='bold', pad=20)
        
        plt.tight_layout()
        plt.savefig(self.plot_dir / 'math500_performance_table.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… æ€§èƒ½å¯¹æ¯”è¡¨æ ¼å·²ä¿å­˜: {self.plot_dir / 'math500_performance_table.png'}")
    
    def run_summary_analysis(self):
        """è¿è¡Œæ±‡æ€»åˆ†æ"""
        print("ğŸš€ å¼€å§‹MATH-500å››ä¸ªæ¨¡å‹æ±‡æ€»åˆ†æ...")
        
        all_difficulty_data = {}
        
        # åŠ è½½æ‰€æœ‰æ¨¡å‹çš„ç»“æœ
        for model_name in self.models.keys():
            print(f"\nğŸ“Š åˆ†æ {model_name}...")
            results = self.load_model_results(model_name)
            difficulty_data = self.analyze_results_by_difficulty(results)
            all_difficulty_data[model_name] = difficulty_data
            
            if difficulty_data:
                total_samples = sum(len(scores) for scores in difficulty_data.values())
                print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
        
        # ç”Ÿæˆæ±‡æ€»å¯¹æ¯”å›¾
        print("\nğŸ“ˆ ç”Ÿæˆæ±‡æ€»å¯¹æ¯”å›¾...")
        total_samples = self.create_summary_plot(all_difficulty_data)
        
        # ç”Ÿæˆæ€§èƒ½å¯¹æ¯”è¡¨æ ¼
        print("\nğŸ“‹ ç”Ÿæˆæ€§èƒ½å¯¹æ¯”è¡¨æ ¼...")
        self.create_performance_table(all_difficulty_data)
        
        print(f"\nâœ… MATH-500å››ä¸ªæ¨¡å‹æ±‡æ€»åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰å›¾è¡¨ä¿å­˜åœ¨: {self.plot_dir}")
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_samples}")

def main():
    """ä¸»å‡½æ•°"""
    plotter = Math500SummaryPlotter()
    plotter.run_summary_analysis()

if __name__ == "__main__":
    main() 