#!/usr/bin/env python3
"""
MATH-500å››ä¸ªæ¨¡å‹ç»“æœå¯¹æ¯”ç»˜å›¾è„šæœ¬
ç»˜åˆ¶ä¸åŒæ¨¡å‹åœ¨MATH-500æ•°æ®é›†ä¸Šçš„æ€§èƒ½å¯¹æ¯”å›¾
"""

import os
import json
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import glob
from typing import Dict, List, Any
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®ç»˜å›¾æ ·å¼
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class Math500ResultsPlotter:
    """MATH-500ç»“æœç»˜å›¾å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç»˜å›¾å™¨"""
        self.results_dir = Path("data/math500_results/deepseek-ai")
        self.plot_dir = Path("plot_data")
        self.plot_dir.mkdir(exist_ok=True)
        
        # æ¨¡å‹é…ç½®
        self.models = {
            "DeepSeek-R1-Distill-Qwen-1.5B": {
                "short_name": "1.5B",
                "color": "#FF6B6B",
                "marker": "o"
            },
            "DeepSeek-R1-Distill-Qwen-7B": {
                "short_name": "7B", 
                "color": "#4ECDC4",
                "marker": "s"
            },
            "DeepSeek-R1-Distill-Qwen-14B": {
                "short_name": "14B",
                "color": "#45B7D1", 
                "marker": "^"
            },
            "DeepSeek-R1-Distill-Qwen-32B": {
                "short_name": "32B",
                "color": "#96CEB4",
                "marker": "D"
            }
        }
    
    def load_model_results(self, model_name: str) -> List[Dict]:
        """åŠ è½½æŒ‡å®šæ¨¡å‹çš„ç»“æœ"""
        model_dir = self.results_dir / model_name
        
        if not model_dir.exists():
            print(f"âš ï¸ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
            return []
        
        # æŸ¥æ‰¾æœ€æ–°çš„ç»“æœç›®å½•
        run_dirs = list(model_dir.glob("*"))
        if not run_dirs:
            print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç»“æœç›®å½•: {model_dir}")
            return []
        
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
        latest_run_dir = max(run_dirs, key=lambda x: x.stat().st_ctime)
        results_file = latest_run_dir / "final_results.json"
        
        if not results_file.exists():
            print(f"âš ï¸ ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {results_file}")
            return []
        
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            print(f"âœ… åŠ è½½ {model_name} ç»“æœ: {len(results)} ä¸ªæ ·æœ¬")
            return results
        except Exception as e:
            print(f"âŒ åŠ è½½ç»“æœå¤±è´¥: {e}")
            return []
    
    def analyze_results(self, results: List[Dict]) -> Dict[str, Any]:
        """åˆ†æç»“æœæ•°æ®"""
        if not results:
            return {}
        
        # æå–è¯„ä¼°åˆ†æ•°
        scores = []
        subjects = []
        levels = []
        
        for result in results:
            evaluation = result.get('evaluation', {})
            if isinstance(evaluation, dict) and 'overall_score' in evaluation:
                scores.append(evaluation['overall_score'])
                subjects.append(result.get('subject', 'Unknown'))
                levels.append(result.get('level', 0))
        
        if not scores:
            return {}
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        analysis = {
            'total_samples': len(results),
            'valid_samples': len(scores),
            'mean_score': np.mean(scores),
            'std_score': np.std(scores),
            'min_score': np.min(scores),
            'max_score': np.max(scores),
            'median_score': np.median(scores),
            'scores': scores,
            'subjects': subjects,
            'levels': levels
        }
        
        # æŒ‰ä¸»é¢˜ç»Ÿè®¡
        subject_stats = {}
        for subject, score in zip(subjects, scores):
            if subject not in subject_stats:
                subject_stats[subject] = []
            subject_stats[subject].append(score)
        
        analysis['subject_stats'] = {
            subject: {
                'count': len(scores),
                'mean': np.mean(scores),
                'std': np.std(scores)
            }
            for subject, scores in subject_stats.items()
        }
        
        # æŒ‰éš¾åº¦ç­‰çº§ç»Ÿè®¡
        level_stats = {}
        for level, score in zip(levels, scores):
            if level not in level_stats:
                level_stats[level] = []
            level_stats[level].append(score)
        
        analysis['level_stats'] = {
            level: {
                'count': len(scores),
                'mean': np.mean(scores),
                'std': np.std(scores)
            }
            for level, scores in level_stats.items()
        }
        
        return analysis
    
    def plot_overall_comparison(self, all_analyses: Dict[str, Dict]):
        """ç»˜åˆ¶æ•´ä½“å¯¹æ¯”å›¾"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('MATH-500å››ä¸ªæ¨¡å‹æ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        # 1. å¹³å‡åˆ†æ•°å¯¹æ¯”
        model_names = []
        mean_scores = []
        std_scores = []
        
        for model_name, analysis in all_analyses.items():
            if analysis:
                short_name = self.models[model_name]['short_name']
                model_names.append(short_name)
                mean_scores.append(analysis['mean_score'])
                std_scores.append(analysis['std_score'])
        
        x = np.arange(len(model_names))
        bars = ax1.bar(x, mean_scores, yerr=std_scores, capsize=5, 
                       color=[self.models[model]['color'] for model in all_analyses.keys() if all_analyses[model]])
        ax1.set_title('å¹³å‡åˆ†æ•°å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax1.set_xlabel('æ¨¡å‹å¤§å°')
        ax1.set_ylabel('å¹³å‡åˆ†æ•°')
        ax1.set_xticks(x)
        ax1.set_xticklabels(model_names)
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars, mean_scores):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{score:.2f}', ha='center', va='bottom', fontweight='bold')
        
        # 2. åˆ†æ•°åˆ†å¸ƒç®±çº¿å›¾
        score_data = []
        labels = []
        colors = []
        
        for model_name, analysis in all_analyses.items():
            if analysis and analysis['scores']:
                score_data.append(analysis['scores'])
                labels.append(self.models[model_name]['short_name'])
                colors.append(self.models[model_name]['color'])
        
        bp = ax2.boxplot(score_data, labels=labels, patch_artist=True)
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        ax2.set_title('åˆ†æ•°åˆ†å¸ƒå¯¹æ¯”', fontsize=14, fontweight='bold')
        ax2.set_xlabel('æ¨¡å‹å¤§å°')
        ax2.set_ylabel('åˆ†æ•°')
        ax2.grid(True, alpha=0.3)
        
        # 3. æŒ‰éš¾åº¦ç­‰çº§çš„åˆ†æ•°å¯¹æ¯”
        level_data = {}
        for model_name, analysis in all_analyses.items():
            if analysis and analysis['level_stats']:
                short_name = self.models[model_name]['short_name']
                for level, stats in analysis['level_stats'].items():
                    if level not in level_data:
                        level_data[level] = {}
                    level_data[level][short_name] = stats['mean']
        
        if level_data:
            levels = sorted(level_data.keys())
            x = np.arange(len(levels))
            width = 0.2
            
            for i, (model_name, analysis) in enumerate(all_analyses.items()):
                if analysis:
                    short_name = self.models[model_name]['short_name']
                    scores = [level_data[level].get(short_name, 0) for level in levels]
                    ax3.bar(x + i*width, scores, width, 
                           label=short_name, color=self.models[model_name]['color'], alpha=0.8)
            
            ax3.set_title('æŒ‰éš¾åº¦ç­‰çº§çš„åˆ†æ•°å¯¹æ¯”', fontsize=14, fontweight='bold')
            ax3.set_xlabel('éš¾åº¦ç­‰çº§')
            ax3.set_ylabel('å¹³å‡åˆ†æ•°')
            ax3.set_xticks(x + width * 1.5)
            ax3.set_xticklabels(levels)
            ax3.legend()
            ax3.grid(True, alpha=0.3)
        
        # 4. æŒ‰ä¸»é¢˜çš„åˆ†æ•°å¯¹æ¯”
        subject_data = {}
        for model_name, analysis in all_analyses.items():
            if analysis and analysis['subject_stats']:
                short_name = self.models[model_name]['short_name']
                for subject, stats in analysis['subject_stats'].items():
                    if subject not in subject_data:
                        subject_data[subject] = {}
                    subject_data[subject][short_name] = stats['mean']
        
        if subject_data:
            subjects = list(subject_data.keys())
            x = np.arange(len(subjects))
            width = 0.2
            
            for i, (model_name, analysis) in enumerate(all_analyses.items()):
                if analysis:
                    short_name = self.models[model_name]['short_name']
                    scores = [subject_data[subject].get(short_name, 0) for subject in subjects]
                    ax4.bar(x + i*width, scores, width, 
                           label=short_name, color=self.models[model_name]['color'], alpha=0.8)
            
            ax4.set_title('æŒ‰ä¸»é¢˜çš„åˆ†æ•°å¯¹æ¯”', fontsize=14, fontweight='bold')
            ax4.set_xlabel('ä¸»é¢˜')
            ax4.set_ylabel('å¹³å‡åˆ†æ•°')
            ax4.set_xticks(x + width * 1.5)
            ax4.set_xticklabels(subjects, rotation=45, ha='right')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.plot_dir / 'math500_overall_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… æ•´ä½“å¯¹æ¯”å›¾å·²ä¿å­˜: {self.plot_dir / 'math500_overall_comparison.png'}")
    
    def plot_individual_model_analysis(self, model_name: str, analysis: Dict):
        """ç»˜åˆ¶å•ä¸ªæ¨¡å‹çš„è¯¦ç»†åˆ†æ"""
        if not analysis:
            return
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'{model_name} è¯¦ç»†åˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾
        scores = analysis['scores']
        ax1.hist(scores, bins=20, alpha=0.7, color=self.models[model_name]['color'])
        ax1.axvline(analysis['mean_score'], color='red', linestyle='--', 
                    label=f'å¹³å‡å€¼: {analysis["mean_score"]:.2f}')
        ax1.set_title('åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾', fontsize=14, fontweight='bold')
        ax1.set_xlabel('åˆ†æ•°')
        ax1.set_ylabel('é¢‘æ¬¡')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. æŒ‰éš¾åº¦ç­‰çº§çš„åˆ†æ•°
        level_stats = analysis['level_stats']
        if level_stats:
            levels = sorted(level_stats.keys())
            level_scores = [level_stats[level]['mean'] for level in levels]
            level_counts = [level_stats[level]['count'] for level in levels]
            
            bars = ax2.bar(levels, level_scores, color=self.models[model_name]['color'], alpha=0.7)
            ax2.set_title('æŒ‰éš¾åº¦ç­‰çº§çš„åˆ†æ•°', fontsize=14, fontweight='bold')
            ax2.set_xlabel('éš¾åº¦ç­‰çº§')
            ax2.set_ylabel('å¹³å‡åˆ†æ•°')
            ax2.grid(True, alpha=0.3)
            
            # æ·»åŠ æ ·æœ¬æ•°é‡æ ‡ç­¾
            for bar, count in zip(bars, level_counts):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                        f'n={count}', ha='center', va='bottom', fontsize=10)
        
        # 3. æŒ‰ä¸»é¢˜çš„åˆ†æ•°
        subject_stats = analysis['subject_stats']
        if subject_stats:
            subjects = list(subject_stats.keys())
            subject_scores = [subject_stats[subject]['mean'] for subject in subjects]
            subject_counts = [subject_stats[subject]['count'] for subject in subjects]
            
            bars = ax3.bar(subjects, subject_scores, color=self.models[model_name]['color'], alpha=0.7)
            ax3.set_title('æŒ‰ä¸»é¢˜çš„åˆ†æ•°', fontsize=14, fontweight='bold')
            ax3.set_xlabel('ä¸»é¢˜')
            ax3.set_ylabel('å¹³å‡åˆ†æ•°')
            ax3.tick_params(axis='x', rotation=45)
            ax3.grid(True, alpha=0.3)
            
            # æ·»åŠ æ ·æœ¬æ•°é‡æ ‡ç­¾
            for bar, count in zip(bars, subject_counts):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                        f'n={count}', ha='center', va='bottom', fontsize=10)
        
        # 4. ç»Ÿè®¡æ‘˜è¦
        ax4.axis('off')
        summary_text = f"""
ç»Ÿè®¡æ‘˜è¦:
æ€»æ ·æœ¬æ•°: {analysis['total_samples']}
æœ‰æ•ˆæ ·æœ¬æ•°: {analysis['valid_samples']}
å¹³å‡åˆ†æ•°: {analysis['mean_score']:.2f} Â± {analysis['std_score']:.2f}
æœ€é«˜åˆ†æ•°: {analysis['max_score']:.2f}
æœ€ä½åˆ†æ•°: {analysis['min_score']:.2f}
ä¸­ä½æ•°: {analysis['median_score']:.2f}
        """
        ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes, 
                fontsize=12, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        model_safe_name = model_name.replace('/', '_').replace('-', '_')
        plt.savefig(self.plot_dir / f'{model_safe_name}_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… {model_name} è¯¦ç»†åˆ†æå›¾å·²ä¿å­˜: {self.plot_dir / f'{model_safe_name}_analysis.png'}")
    
    def generate_summary_report(self, all_analyses: Dict[str, Dict]):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        report_file = self.plot_dir / 'math500_summary_report.txt'
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("MATH-500å››ä¸ªæ¨¡å‹æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            # æ€»ä½“å¯¹æ¯”
            f.write("1. æ€»ä½“æ€§èƒ½å¯¹æ¯”\n")
            f.write("-" * 30 + "\n")
            
            for model_name, analysis in all_analyses.items():
                if analysis:
                    short_name = self.models[model_name]['short_name']
                    f.write(f"{short_name}æ¨¡å‹:\n")
                    f.write(f"  æ€»æ ·æœ¬æ•°: {analysis['total_samples']}\n")
                    f.write(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {analysis['valid_samples']}\n")
                    f.write(f"  å¹³å‡åˆ†æ•°: {analysis['mean_score']:.2f} Â± {analysis['std_score']:.2f}\n")
                    f.write(f"  åˆ†æ•°èŒƒå›´: {analysis['min_score']:.2f} - {analysis['max_score']:.2f}\n")
                    f.write(f"  ä¸­ä½æ•°: {analysis['median_score']:.2f}\n\n")
            
            # æŒ‰éš¾åº¦ç­‰çº§å¯¹æ¯”
            f.write("2. æŒ‰éš¾åº¦ç­‰çº§çš„æ€§èƒ½å¯¹æ¯”\n")
            f.write("-" * 30 + "\n")
            
            level_data = {}
            for model_name, analysis in all_analyses.items():
                if analysis and analysis['level_stats']:
                    short_name = self.models[model_name]['short_name']
                    for level, stats in analysis['level_stats'].items():
                        if level not in level_data:
                            level_data[level] = {}
                        level_data[level][short_name] = stats['mean']
            
            for level in sorted(level_data.keys()):
                f.write(f"éš¾åº¦ç­‰çº§ {level}:\n")
                for model_name, analysis in all_analyses.items():
                    if analysis:
                        short_name = self.models[model_name]['short_name']
                        score = level_data[level].get(short_name, 0)
                        f.write(f"  {short_name}: {score:.2f}\n")
                f.write("\n")
            
            # æŒ‰ä¸»é¢˜å¯¹æ¯”
            f.write("3. æŒ‰ä¸»é¢˜çš„æ€§èƒ½å¯¹æ¯”\n")
            f.write("-" * 30 + "\n")
            
            subject_data = {}
            for model_name, analysis in all_analyses.items():
                if analysis and analysis['subject_stats']:
                    short_name = self.models[model_name]['short_name']
                    for subject, stats in analysis['subject_stats'].items():
                        if subject not in subject_data:
                            subject_data[subject] = {}
                        subject_data[subject][short_name] = stats['mean']
            
            for subject in sorted(subject_data.keys()):
                f.write(f"{subject}:\n")
                for model_name, analysis in all_analyses.items():
                    if analysis:
                        short_name = self.models[model_name]['short_name']
                        score = subject_data[subject].get(short_name, 0)
                        f.write(f"  {short_name}: {score:.2f}\n")
                f.write("\n")
        
        print(f"âœ… æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ å¼€å§‹MATH-500å››ä¸ªæ¨¡å‹ç»“æœåˆ†æ...")
        
        all_analyses = {}
        
        # åŠ è½½æ‰€æœ‰æ¨¡å‹çš„ç»“æœ
        for model_name in self.models.keys():
            print(f"\nğŸ“Š åˆ†æ {model_name}...")
            results = self.load_model_results(model_name)
            analysis = self.analyze_results(results)
            all_analyses[model_name] = analysis
            
            if analysis:
                print(f"  å¹³å‡åˆ†æ•°: {analysis['mean_score']:.2f} Â± {analysis['std_score']:.2f}")
                print(f"  æ ·æœ¬æ•°é‡: {analysis['valid_samples']}/{analysis['total_samples']}")
        
        # ç”Ÿæˆå›¾è¡¨
        print("\nğŸ“ˆ ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
        self.plot_overall_comparison(all_analyses)
        
        # ç”Ÿæˆå•ä¸ªæ¨¡å‹è¯¦ç»†åˆ†æ
        print("\nğŸ“Š ç”Ÿæˆå•ä¸ªæ¨¡å‹è¯¦ç»†åˆ†æ...")
        for model_name, analysis in all_analyses.items():
            if analysis:
                self.plot_individual_model_analysis(model_name, analysis)
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        print("\nğŸ“‹ ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
        self.generate_summary_report(all_analyses)
        
        print("\nâœ… MATH-500å››ä¸ªæ¨¡å‹ç»“æœåˆ†æå®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰å›¾è¡¨å’ŒæŠ¥å‘Šä¿å­˜åœ¨: {self.plot_dir}")

def main():
    """ä¸»å‡½æ•°"""
    plotter = Math500ResultsPlotter()
    plotter.run_analysis()

if __name__ == "__main__":
    main() 