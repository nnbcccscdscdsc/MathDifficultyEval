#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MATH-500æ•°æ®é›†ä¼˜åŒ–ç»˜å›¾è„šæœ¬
ç‰¹ç‚¹ï¼š
1. å»æ‰32Bæ¨¡å‹ï¼Œåªæ˜¾ç¤º1.5Bã€7Bã€14Bä¸‰ä¸ªæ¨¡å‹
2. Yè½´8-10åˆ†æ®µåˆ»åº¦ç»†åŒ–ï¼Œä¾¿äºè§‚å¯Ÿç»†å¾®å·®å¼‚
3. ä¿æŒä¸åŸå§‹æ ·å¼ä¸€è‡´çš„ä¸¤é¢æ¿å¸ƒå±€
"""

import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Optional
import logging

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®ç»˜å›¾æ ·å¼
sns.set_style("whitegrid")
plt.style.use('seaborn-v0_8')

class Math500OptimizedPlotter:
    """MATH-500æ•°æ®é›†ä¼˜åŒ–ç»˜å›¾å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç»˜å›¾å™¨"""
        self.results_dir = Path("data/math500_results")
        self.plot_dir = Path("plot_data")
        self.plot_dir.mkdir(exist_ok=True)
        
        # æ·»åŠ 32Bå’Œ70Bæ¨¡å‹ï¼Œç°åœ¨åŒ…å«5ä¸ªæ¨¡å‹
        self.models = {
            "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B": {
                "short_name": "1.5B",
                "color": "#FF6B6B",
                "marker": "o",
                "linewidth": 2,
                "markersize": 8
            },
            "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": {
                "short_name": "7B", 
                "color": "#4ECDC4",
                "marker": "s",
                "linewidth": 2,
                "markersize": 8
            },
            "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B": {
                "short_name": "14B",
                "color": "#45B7D1", 
                "marker": "^",
                "linewidth": 2,
                "markersize": 8
            },
            "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B": {
                "short_name": "32B",
                "color": "#96CEB4",
                "marker": "D",
                "linewidth": 2,
                "markersize": 8
            },
            "deepseek-ai/DeepSeek-R1-Distill-Llama-70B": {
                "short_name": "70B",
                "color": "#FFA07A",
                "marker": "p",
                "linewidth": 2,
                "markersize": 8
            }
        }
        
        self.logger = logging.getLogger(__name__)
    
    def load_model_results(self, model_name: str) -> List[Dict]:
        """åŠ è½½æ¨¡å‹ç»“æœ"""
        # ä¿®å¤è·¯å¾„ï¼šMATH-500ç»“æœåœ¨deepseek-aiå­ç›®å½•ä¸‹
        model_dir = self.results_dir / "deepseek-ai" / model_name.split('/')[-1]
        
        if not model_dir.exists():
            self.logger.warning(f"âš ï¸ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
            return []
        
        # æŸ¥æ‰¾æœ€æ–°çš„è¿è¡Œç›®å½•
        run_dirs = [d for d in model_dir.iterdir() if d.is_dir() and d.name.startswith('math500_')]
        
        if not run_dirs:
            self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°è¿è¡Œç›®å½•: {model_dir}")
            return []
        
        # åªé€‰æ‹©åŒ…å«final_results.jsonçš„æœ‰æ•ˆç›®å½•
        valid_dirs = [d for d in run_dirs if (d / 'final_results.json').exists()]
        
        if not valid_dirs:
            self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç»“æœæ–‡ä»¶: {model_dir}")
            return []
        
        # é€‰æ‹©æœ€æ–°çš„æœ‰æ•ˆç›®å½•
        latest_run_dir = max(valid_dirs, key=lambda x: x.stat().st_ctime)
        print(f"ğŸ” ä½¿ç”¨ç»“æœç›®å½•: {latest_run_dir}")
        
        results_file = latest_run_dir / 'final_results.json'
        
        if not results_file.exists():
            self.logger.warning(f"âš ï¸ ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {results_file}")
            return []
        
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            print(f"âœ… åŠ è½½ {model_name} ç»“æœ: {len(results)} ä¸ªæ ·æœ¬")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def analyze_results_by_difficulty(self, results: List[Dict]) -> Dict[int, List[float]]:
        """æŒ‰éš¾åº¦ç­‰çº§åˆ†æç»“æœ"""
        difficulty_scores = {}
        
        for result in results:
            # MATH-500ä½¿ç”¨levelå­—æ®µï¼Œä¸æ˜¯difficulty
            if 'level' in result and 'evaluation' in result and 'overall_score' in result['evaluation']:
                level = result['level']
                score = result['evaluation']['overall_score']
                
                if level not in difficulty_scores:
                    difficulty_scores[level] = []
                difficulty_scores[level].append(score)
        
        return difficulty_scores
    
    def create_optimized_plot(self, all_difficulty_data: Dict[str, Dict[int, List[float]]]):
        """åˆ›å»ºä¼˜åŒ–åçš„æ±‡æ€»å¯¹æ¯”å›¾"""
        # åˆ›å»ºä¼˜åŒ–åçš„å¯¹æ¯”å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # å·¦ä¾§ï¼šæ¨ªè½´ä¸ºæ¨¡å‹å‚æ•°ï¼Œçºµè½´ä¸ºæ‰“åˆ†æƒ…å†µï¼Œäº”æ¡çº¿åˆ†åˆ«ä»£è¡¨ä¸åŒéš¾åº¦ç­‰çº§
        ax1.set_title('(a) Model Performance by Difficulty Level', 
                     fontsize=14, fontweight='bold', pad=20)
        
        # æ”¶é›†æ‰€æœ‰éš¾åº¦ç­‰çº§
        all_levels = set()
        for model_name, difficulty_data in all_difficulty_data.items():
            if difficulty_data:
                all_levels.update(difficulty_data.keys())
        all_levels = sorted(all_levels)
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹åç§°ï¼ˆçŸ­åç§°ï¼‰
        model_names = []
        for model_name in self.models.keys():
            if model_name in all_difficulty_data:
                config = self.models[model_name]
                model_names.append(config['short_name'])
        
        # ä¸ºæ¯ä¸ªéš¾åº¦ç­‰çº§ç»˜åˆ¶ä¸€æ¡çº¿
        level_colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFA07A']
        level_markers = ['o', 's', '^', 'D', 'p']
        
        for i, level in enumerate(all_levels):
            level_scores = []
            
            # æ”¶é›†è¯¥éš¾åº¦ç­‰çº§åœ¨æ‰€æœ‰æ¨¡å‹ä¸­çš„å¹³å‡åˆ†æ•°
            for model_name in self.models.keys():
                if model_name in all_difficulty_data and level in all_difficulty_data[model_name]:
                    scores = all_difficulty_data[model_name][level]
                    level_scores.append(np.mean(scores))
                else:
                    level_scores.append(np.nan)  # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œç”¨NaN
            
            # ç»˜åˆ¶è¯¥éš¾åº¦ç­‰çº§çš„çº¿
            ax1.plot(model_names, level_scores, 
                    color=level_colors[i % len(level_colors)], 
                    marker=level_markers[i % len(level_markers)],
                    linewidth=2,
                    markersize=8,
                    label=f'Level {level}')
        
        ax1.set_xlabel('Model Parameters', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Average Score', fontsize=12, fontweight='bold')
        ax1.set_ylim(7.5, 10.0)  # ä¸å³å›¾ä¿æŒä¸€è‡´çš„Yè½´èŒƒå›´
        
        # è®¾ç½®Yè½´åˆ»åº¦ï¼ˆ7.5-10.0åˆ†æ®µç»†åŒ–ï¼‰
        ax1.set_yticks([7.5, 8.0, 8.5, 9.0, 9.5, 10.0])
        ax1.set_yticklabels(['7.5', '8.0', '8.5', '9.0', '9.5', '10.0'])
        
        ax1.grid(True, alpha=0.3)
        ax1.legend(loc='best', frameon=True, fancybox=True, shadow=True)
        
        # å³ä¾§ï¼šå¹³å‡æ¨¡å‹æ€§èƒ½åˆ†æï¼ˆæŒ‰éš¾åº¦ï¼‰
        ax2.set_title('(b) Average DeepSeek R1 Series Model Performance Analysis by Difficulty', 
                     fontsize=14, fontweight='bold', pad=20)
        
        # ç»˜åˆ¶æ¯ä¸ªæ¨¡å‹çš„æ€§èƒ½æ›²çº¿
        for model_name, difficulty_data in all_difficulty_data.items():
            if difficulty_data:
                config = self.models[model_name]
                levels = []
                avg_scores = []
                
                for level in all_levels:
                    if level in difficulty_data:
                        scores = difficulty_data[level]
                        levels.append(level)
                        avg_scores.append(np.mean(scores))
                    else:
                        levels.append(level) # Keep levels aligned
                        avg_scores.append(np.nan) # Use NaN for missing data
                
                # Filter out NaNs for plotting
                plot_levels = [l for l, s in zip(levels, avg_scores) if not np.isnan(s)]
                plot_scores = [s for s in avg_scores if not np.isnan(s)]

                if plot_levels:
                    ax2.plot(plot_levels, plot_scores, 
                            color=config['color'], 
                            marker=config['marker'],
                            linewidth=config['linewidth'],
                            markersize=8,
                            label=config['short_name'])
        
        ax2.set_xlabel('Problem Difficulty', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Average Score', fontsize=12, fontweight='bold')
        ax2.set_xlim(0.5, 5.5)  # è°ƒæ•´Xè½´èŒƒå›´ï¼Œå‡å°‘ä¸¤ä¾§ç©ºç™½
        
        # ä¿®æ”¹Yè½´åˆ»åº¦ï¼š7.5-10.0åˆ†æ®µç»†åŒ–
        ax2.set_ylim(7.5, 10.0)  # è®¾ç½®Yè½´èŒƒå›´ä¸º7.5-10.0
        ax2.set_yticks([7.5, 8.0, 8.5, 9.0, 9.5, 10.0])  # æ¯0.5ä¸€ä¸ªåˆ»åº¦
        ax2.set_yticklabels(['7.5', '8.0', '8.5', '9.0', '9.5', '10.0'])
        
        # ä¼˜åŒ–æ¨ªè½´æ ‡ç­¾ä½ç½®å’Œæ˜¾ç¤º
        ax2.set_xticks([1, 2, 3, 4, 5])
        ax2.set_xticklabels(['Level 1', 'Level 2', 'Level 3', 'Level 4', 'Level 5'], rotation=0)
        
        ax2.grid(True, alpha=0.3)
        ax2.legend(loc='best', frameon=True, fancybox=True, shadow=True)
        
        # è®¡ç®—æ€»æ ·æœ¬æ•°ï¼ˆç”¨äºè¿”å›å€¼å’ŒæŠ¥å‘Šï¼‰
        total_samples = 0
        first_model_data = next(iter(all_difficulty_data.values()), {})
        for level, scores in first_model_data.items():
            total_samples += len(scores)
        
        plt.tight_layout()
        plt.savefig(self.plot_dir / 'math500_optimized_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… ä¼˜åŒ–å¯¹æ¯”å›¾å·²ä¿å­˜: {self.plot_dir / 'math500_optimized_comparison.png'}")
        
        return total_samples
    
    def generate_summary_report(self, all_difficulty_data: Dict[str, Dict[int, List[float]]], total_samples: int):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        report_lines = []
        report_lines.append("=" * 60)
        report_lines.append("MATH-500æ•°æ®é›†ä¼˜åŒ–åˆ†ææŠ¥å‘Š")
        report_lines.append("=" * 60)
        report_lines.append(f"åˆ†ææ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"æ€»æ ·æœ¬æ•°: {total_samples:,}")
        report_lines.append(f"æ¨¡å‹æ•°é‡: {len(self.models)}")
        report_lines.append("")
        
        # æ¨¡å‹æ€§èƒ½æ±‡æ€»
        report_lines.append("ğŸ“Š æ¨¡å‹æ€§èƒ½æ±‡æ€»:")
        report_lines.append("-" * 40)
        
        for model_name, difficulty_data in all_difficulty_data.items():
            if not difficulty_data:
                continue
                
            config = self.models[model_name]
            all_scores = []
            
            for level, scores in difficulty_data.items():
                all_scores.extend(scores)
            
            if all_scores:
                mean_score = np.mean(all_scores)
                std_score = np.std(all_scores)
                total_count = len(all_scores)
                
                report_lines.append(f"{config['short_name']}:")
                report_lines.append(f"  å¹³å‡åˆ†æ•°: {mean_score:.2f} Â± {std_score:.2f}")
                report_lines.append(f"  æ ·æœ¬æ•°é‡: {total_count:,}")
                report_lines.append("")
        
        # æŒ‰éš¾åº¦ç­‰çº§çš„æ€§èƒ½å¯¹æ¯”
        report_lines.append("ğŸ“ˆ æŒ‰éš¾åº¦ç­‰çº§çš„æ€§èƒ½å¯¹æ¯”:")
        report_lines.append("-" * 40)
        
        all_levels = set()
        for model_data in all_difficulty_data.values():
            all_levels.update(model_data.keys())
        all_levels = sorted(all_levels)
        
        # è¡¨å¤´
        header = "éš¾åº¦ç­‰çº§"
        for model_name in self.models.keys():
            if model_name in all_difficulty_data:
                config = self.models[model_name]
                header += f"\t{config['short_name']}"
        report_lines.append(header)
        report_lines.append("-" * 40)
        
        # æ•°æ®è¡Œ
        for level in all_levels:
            row = f"Level {level}"
            for model_name in self.models.keys():
                if model_name in all_difficulty_data and level in all_difficulty_data[model_name]:
                    scores = all_difficulty_data[model_name][level]
                    mean_score = np.mean(scores)
                    row += f"\t{mean_score:.2f}"
                else:
                    row += "\t-"
            report_lines.append(row)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.plot_dir / 'math500_optimized_summary_report.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"âœ… æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def run_optimized_analysis(self):
        """è¿è¡Œä¼˜åŒ–åˆ†æ"""
        print("ğŸš€ å¼€å§‹MATH-500æ•°æ®é›†ä¼˜åŒ–åˆ†æ...")
        print(f"ğŸ“Š åˆ†ææ¨¡å‹: {', '.join([config['short_name'] for config in self.models.values()])}")
        print()
        
        # åŠ è½½æ‰€æœ‰æ¨¡å‹ç»“æœ
        all_difficulty_data = {}
        
        for model_name in self.models.keys():
            print(f"ğŸ“Š åˆ†æ {model_name}...")
            results = self.load_model_results(model_name)
            
            if results:
                difficulty_data = self.analyze_results_by_difficulty(results)
                all_difficulty_data[model_name] = difficulty_data
                
                # è®¡ç®—æ€»ä½“ç»Ÿè®¡
                all_scores = []
                for scores in difficulty_data.values():
                    all_scores.extend(scores)
                
                if all_scores:
                    mean_score = np.mean(all_scores)
                    std_score = np.std(all_scores)
                    total_count = len(all_scores)
                    print(f"  å¹³å‡åˆ†æ•°: {mean_score:.2f} Â± {std_score:.2f}")
                    print(f"  æ ·æœ¬æ•°é‡: {total_count:,}")
                    print()
        
        if not all_difficulty_data:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ç»“æœæ•°æ®")
            return
        
        # ç”Ÿæˆä¼˜åŒ–å›¾è¡¨
        print("ğŸ“ˆ ç”Ÿæˆä¼˜åŒ–å¯¹æ¯”å›¾è¡¨...")
        total_samples = self.create_optimized_plot(all_difficulty_data)
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        print("ğŸ“‹ ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
        self.generate_summary_report(all_difficulty_data, total_samples)
        
        print("âœ… MATH-500æ•°æ®é›†ä¼˜åŒ–åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰å›¾è¡¨å’ŒæŠ¥å‘Šä¿å­˜åœ¨: {self.plot_dir}")

def main():
    """ä¸»å‡½æ•°"""
    plotter = Math500OptimizedPlotter()
    plotter.run_optimized_analysis()

if __name__ == "__main__":
    main() 