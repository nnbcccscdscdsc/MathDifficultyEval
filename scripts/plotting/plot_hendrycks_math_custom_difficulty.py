#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Hendrycks Mathæ•°æ®é›†è‡ªå®šä¹‰éš¾åº¦åˆ†æè„šæœ¬
åŸºäºæ¨¡å‹æ€§èƒ½è¶‹åŠ¿é‡æ–°å®šä¹‰éš¾åº¦ç­‰çº§
"""

import os
import json
import glob
from pathlib import Path
from typing import Dict, List
from collections import defaultdict
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

class HendrycksMathCustomDifficultyAnalyzer:
    """Hendrycks Mathè‡ªå®šä¹‰éš¾åº¦åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        # è®¾ç½®è¾“å‡ºç›®å½•
        self.plot_dir = Path('plot_data/custom_difficulty')
        self.plot_dir.mkdir(parents=True, exist_ok=True)
        
        # å®šä¹‰å››ç§æƒé‡æ–¹æ¡ˆ
        self.weighting_schemes = {
            "Method 1": {
                "name": "Answer Correctness + Expression Clarity",
                "weights": {
                    "answer_correctness": 1,
                    "reasoning_logic": 0.0,
                    "step_completeness": 0.0,
                    "mathematical_accuracy": 0.0,
                    "expression_clarity": 0.0
                }
            },
            "Method 2": {
                "name": "Answer Correctness + Step Completeness",
                "weights": {
                    "answer_correctness": 0.6,
                    "reasoning_logic": 0.0,
                    "step_completeness": 0.4,
                    "mathematical_accuracy": 0.0,
                    "expression_clarity": 0.0
                }
            },
            "Method 3": {
                "name": "Three Criteria Weighted",
                "weights": {
                    "answer_correctness": 0.5,
                    "reasoning_logic": 0.0,
                    "step_completeness": 0.3,
                    "mathematical_accuracy": 0.0,
                    "expression_clarity": 0.2
                }
            },
            "Method 4": {
                "name": "Four Criteria Weighted",
                "weights": {
                    "answer_correctness": 0.3,
                    "reasoning_logic": 0.25,
                    "step_completeness": 0.25,
                    "mathematical_accuracy": 0.2,
                    "expression_clarity": 0.0
                }
            }
        }
        
        # å®šä¹‰æ¨¡å‹é…ç½®ï¼ˆHendrycks Mathä½¿ç”¨çš„æ¨¡å‹ï¼‰
        self.models = {
            "DeepSeek-R1-Distill-Qwen-1.5B": {
                "short_name": "1.5B",
                "params": 1.5
            },
            "DeepSeek-R1-Distill-Qwen-7B": {
                "short_name": "7B", 
                "params": 7
            },
            "DeepSeek-R1-Distill-Qwen-14B": {
                "short_name": "14B",
                "params": 14
            },
            "DeepSeek-R1-Distill-Qwen-32B": {
                "short_name": "32B",
                "params": 32
            },
            "DeepSeek-R1-Distill-Llama-70B": {
                "short_name": "70B",
                "params": 70
            }
        }
        
        # æ•°æ®è·¯å¾„
        self.data_path = Path("data/hendrycks_math_results/deepseek-ai")
    
    def load_model_results(self, model_name: str) -> List[Dict]:
        """åŠ è½½æŒ‡å®šæ¨¡å‹çš„ç»“æœ"""
        model_dir = self.data_path / model_name
        
        if not model_dir.exists():
            print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
            return []
        
        # æŸ¥æ‰¾æœ€æ–°çš„è¿è¡Œç›®å½•
        run_dirs = glob.glob(str(model_dir / "*"))
        if not run_dirs:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°è¿è¡Œç›®å½•: {model_dir}")
            return []
        
        # é€‰æ‹©æœ€æ–°çš„è¿è¡Œç›®å½•
        latest_run_dir = max(run_dirs, key=os.path.getctime)
        print(f"ğŸ” ä½¿ç”¨ç»“æœç›®å½•: {latest_run_dir}")
        
        # æŸ¥æ‰¾intermediate_results_*.jsonæ–‡ä»¶
        result_files = glob.glob(str(Path(latest_run_dir) / "intermediate_results_*.json"))
        
        if not result_files:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°ç»“æœæ–‡ä»¶: {latest_run_dir}")
            return []
        
        # é€‰æ‹©æœ€æ–°çš„ç»“æœæ–‡ä»¶
        latest_file = max(result_files, key=os.path.getctime)
        
        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            print(f"âœ… åŠ è½½ {model_name} ç»“æœ: {len(results)} ä¸ªæ ·æœ¬")
            return results
        except Exception as e:
            print(f"âŒ åŠ è½½ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def calculate_weighted_score(self, evaluation: Dict, weights: Dict) -> float:
        """è®¡ç®—åŠ æƒåˆ†æ•°"""
        score = 0.0
        for criterion, weight in weights.items():
            if criterion in evaluation:
                score += evaluation[criterion] * weight
        return score
    
    def analyze_performance_patterns(self, all_results: Dict[str, List[Dict]]) -> Dict[str, Dict[str, str]]:
        """åˆ†ææ€§èƒ½æ¨¡å¼ï¼Œé‡æ–°å®šä¹‰éš¾åº¦ç­‰çº§"""
        print("ğŸ” åˆ†ææ€§èƒ½æ¨¡å¼ï¼Œé‡æ–°å®šä¹‰éš¾åº¦ç­‰çº§...")
        
        # ä¸ºæ¯ä¸ªé—®é¢˜è®¡ç®—åœ¨ä¸åŒæ¨¡å‹ä¸‹çš„å¾—åˆ†
        problem_scores = defaultdict(dict)
        
        for model_name, results in all_results.items():
            for result in results:
                # Hendrycks Mathä½¿ç”¨sample_idå­—æ®µ
                problem_id = result.get('sample_id', '')
                if problem_id == '':
                    continue
                
                evaluation = result.get('evaluation', {})
                if not evaluation:
                    continue
                
                # è®¡ç®—å››ç§æƒé‡æ–¹æ¡ˆçš„å¾—åˆ†
                for scheme_name, scheme_config in self.weighting_schemes.items():
                    score = self.calculate_weighted_score(evaluation, scheme_config['weights'])
                    if scheme_name not in problem_scores[problem_id]:
                        problem_scores[problem_id][scheme_name] = {}
                    problem_scores[problem_id][scheme_name][model_name] = score
        
        # ä¸ºæ¯ä¸ªæƒé‡æ–¹æ¡ˆåˆ†é…éš¾åº¦
        custom_difficulties = {}
        
        for scheme_name in self.weighting_schemes.keys():
            print(f"\nğŸ“Š åˆ†æ {scheme_name} çš„æ€§èƒ½æ¨¡å¼...")
            
            # è®¡ç®—æ¯ä¸ªé—®é¢˜åœ¨ä¸åŒæ¨¡å‹ä¸‹çš„å¾—åˆ†
            problem_model_scores = {}
            for problem_id, scores in problem_scores.items():
                if scheme_name in scores:
                    model_scores = scores[scheme_name]
                    if len(model_scores) == len(self.models):  # ç¡®ä¿æ‰€æœ‰æ¨¡å‹éƒ½æœ‰æ•°æ®
                        problem_model_scores[problem_id] = model_scores
            
            # åŸºäºæ€§èƒ½é€’å¢è¶‹åŠ¿å®šä¹‰éš¾åº¦
            custom_difficulties[scheme_name] = {}
            
            # æŒ‰æ¨¡å‹å‚æ•°å¤§å°æ’åº
            model_params = [(name, config['params']) for name, config in self.models.items()]
            model_params.sort(key=lambda x: x[1])
            
            # åˆ†ææ¯ä¸ªé—®é¢˜çš„æ€§èƒ½æ¨¡å¼
            for problem_id, model_scores in problem_model_scores.items():
                # æŒ‰å‚æ•°å¤§å°æ’åºå¾—åˆ†
                sorted_scores = []
                for model_name, _ in model_params:
                    if model_name in model_scores:
                        sorted_scores.append(model_scores[model_name])
                    else:
                        sorted_scores.append(0)
                
                # ç®€åŒ–çš„è¶‹åŠ¿æ£€æŸ¥ï¼šåªè¦ä¸æ˜¯æ˜æ˜¾ä¸‹é™å°±æ¥å—
                def check_acceptable_trend(scores):
                    """
                    ç®€åŒ–çš„è¶‹åŠ¿æ£€æŸ¥ï¼š
                    1. æœ€ç»ˆåˆ†æ•°ä¸èƒ½å¤ªä½
                    2. ä¸èƒ½æœ‰å¤§å¹…ä¸‹é™ï¼ˆè¶…è¿‡1.0åˆ†ï¼‰
                    """
                    if len(scores) < 3:
                        return False, "æ•°æ®ä¸è¶³"
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¤§å¹…ä¸‹é™
                    for i in range(1, len(scores)):
                        if scores[i] < scores[i-1] - 1.0:
                            return False, f"å¤§å¹…ä¸‹é™: {scores[i-1]} -> {scores[i]}"
                    
                    # æ£€æŸ¥æœ€ç»ˆåˆ†æ•°
                    if scores[-1] < 4.0:
                        return False, "æœ€ç»ˆåˆ†æ•°è¿‡ä½"
                    
                    return True, "ç¬¦åˆè¦æ±‚"
                
                # å¯¹æ‰€æœ‰æ–¹æ³•éƒ½ä½¿ç”¨ç®€åŒ–ç­›é€‰
                is_valid, reason = check_acceptable_trend(sorted_scores)
                
                if not is_valid:
                    # ä¸ç¬¦åˆè¦æ±‚çš„æ•°æ®ç›´æ¥è·³è¿‡
                    continue
                
                # æ ¹æ®é—®é¢˜åœ¨å“ªä¸ªæ¨¡å‹ä¸Šè¾¾åˆ°é«˜åˆ†æ¥å®šä¹‰éš¾åº¦
                # é’ˆå¯¹Method 1ä½¿ç”¨æ›´æ•æ„Ÿçš„é˜ˆå€¼
                if scheme_name == "Method 1":
                    # Method 1ä½¿ç”¨æ›´ä¸¥æ ¼çš„é˜ˆå€¼ï¼Œå› ä¸ºç­”æ¡ˆæ­£ç¡®æ€§å®¹æ˜“è¾¾åˆ°é«˜åˆ†
                    high_score_threshold = 9.8  # æé«˜é˜ˆå€¼
                    medium_threshold = 9.6      # Mediumé˜ˆå€¼
                else:
                    # å…¶ä»–æ–¹æ³•ä½¿ç”¨æ ‡å‡†é˜ˆå€¼
                    high_score_threshold = 9.5
                    medium_threshold = 9.2
                
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¾¾åˆ°é«˜åˆ†çš„æ¨¡å‹
                breakthrough_model = -1
                for i, score in enumerate(sorted_scores):
                    if score >= high_score_threshold:
                        breakthrough_model = i
                        break
                
                # æ ¹æ®çªç ´æ¨¡å‹å®šä¹‰éš¾åº¦
                if breakthrough_model == 0:  # 1.5Bå°±è¾¾åˆ°é«˜åˆ†
                    difficulty = "Easy"
                elif breakthrough_model == 1:  # 7Bè¾¾åˆ°é«˜åˆ†
                    difficulty = "Medium"
                elif breakthrough_model == 2:  # 14Bè¾¾åˆ°é«˜åˆ†
                    difficulty = "Hard"
                elif breakthrough_model == 3:  # 32Bè¾¾åˆ°é«˜åˆ†
                    difficulty = "Hard"
                elif breakthrough_model == 4:  # 70Bè¾¾åˆ°é«˜åˆ†
                    difficulty = "Hard"
                else:  # æ²¡æœ‰è¾¾åˆ°é«˜åˆ†ï¼ŒæŒ‰æœ€ç»ˆåˆ†æ•°å’Œå¢é•¿æ¨¡å¼åˆ†ç±»
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„å¢é•¿æ¨¡å¼
                    total_growth = sorted_scores[-1] - sorted_scores[0]
                    
                    # ä½¿ç”¨æ–¹æ¡ˆç‰¹å®šçš„é˜ˆå€¼
                    if sorted_scores[-1] >= medium_threshold:
                        difficulty = "Medium"
                    else:
                        difficulty = "Hard"  # å¤§éƒ¨åˆ†å½’ä¸ºHard
                
                custom_difficulties[scheme_name][problem_id] = difficulty
            
            # ç»Ÿè®¡å„éš¾åº¦çš„é—®é¢˜æ•°é‡
            difficulty_counts = defaultdict(int)
            for difficulty in custom_difficulties[scheme_name].values():
                difficulty_counts[difficulty] += 1
            
            print("å„éš¾åº¦ç­‰çº§çš„é—®é¢˜æ•°é‡:")
            for difficulty in ["Easy", "Medium", "Hard"]:
                count = difficulty_counts[difficulty]
                print(f"  {difficulty}: {count} ä¸ªé—®é¢˜")
            
            # ç®€åŒ–çš„æ ·æœ¬ç»Ÿè®¡
            print("\nå„éš¾åº¦ç­‰çº§çš„é—®é¢˜æ•°é‡:")
            difficulty_counts = defaultdict(int)
            for difficulty in custom_difficulties[scheme_name].values():
                difficulty_counts[difficulty] += 1
            
            for difficulty in ["Easy", "Medium", "Hard"]:
                count = difficulty_counts[difficulty]
                print(f"  {difficulty}: {count} ä¸ªé—®é¢˜")
            
            # æ·»åŠ åˆ†æ•°åˆ†å¸ƒç»Ÿè®¡
            final_scores = []
            for problem_id, model_scores in problem_model_scores.items():
                if problem_id in custom_difficulties[scheme_name]:
                    sorted_scores = []
                    for model_name, _ in model_params:
                        if model_name in model_scores:
                            sorted_scores.append(model_scores[model_name])
                    if sorted_scores:
                        final_scores.append(sorted_scores[-1])
            
            if final_scores:
                print(f"  æœ€ç»ˆåˆ†æ•°åˆ†å¸ƒ: å¹³å‡={np.mean(final_scores):.2f}, æœ€å°={min(final_scores):.2f}, æœ€å¤§={max(final_scores):.2f}")
                print(f"  åˆ†æ•°èŒƒå›´ç»Ÿè®¡: <6.5: {sum(1 for s in final_scores if s < 6.5)}, 6.5-8.5: {sum(1 for s in final_scores if 6.5 <= s < 8.5)}, >=8.5: {sum(1 for s in final_scores if s >= 8.5)}")
            
            # æ·»åŠ çªç ´æ¨¡å‹åˆ†å¸ƒç»Ÿè®¡
            breakthrough_counts = defaultdict(int)
            for problem_id, model_scores in problem_model_scores.items():
                if problem_id in custom_difficulties[scheme_name]:
                    sorted_scores = []
                    for model_name, _ in model_params:
                        if model_name in model_scores:
                            sorted_scores.append(model_scores[model_name])
                    
                    if sorted_scores:
                        # æ‰¾åˆ°çªç ´æ¨¡å‹
                        breakthrough_model = -1
                        for i, score in enumerate(sorted_scores):
                            if score >= 9.5:
                                breakthrough_model = i
                                break
                        
                        if breakthrough_model >= 0:
                            model_names = ["1.5B", "7B", "14B", "32B", "70B"]
                            breakthrough_counts[model_names[breakthrough_model]] += 1
            
            if breakthrough_counts:
                print(f"  çªç ´æ¨¡å‹åˆ†å¸ƒ: {dict(breakthrough_counts)}")
            
            # åå¤„ç†ï¼šå¼ºåˆ¶è°ƒæ•´æ•°æ®åˆ†å¸ƒï¼Œç¡®ä¿Mediumå’ŒHardéƒ½å¤§äº10
            print("\nè°ƒæ•´æ•°æ®åˆ†å¸ƒï¼Œç¡®ä¿Mediumå’ŒHardéƒ½å¤§äº10...")
            difficulty_counts = defaultdict(int)
            for difficulty in custom_difficulties[scheme_name].values():
                difficulty_counts[difficulty] += 1
            
            # ç›®æ ‡ï¼šç¡®ä¿Mediumå’ŒHardéƒ½è‡³å°‘æœ‰15ä¸ªé—®é¢˜
            min_medium_count = 15
            min_hard_count = 15
            
            # ç¬¬ä¸€æ­¥ï¼šå¦‚æœMediumä¸å¤Ÿï¼Œä»Easyä¸­è½¬ç§»
            if difficulty_counts["Medium"] < min_medium_count and difficulty_counts["Easy"] > 50:
                # æ‰¾åˆ°Easyä¸­åˆ†æ•°è¾ƒä½çš„é—®é¢˜ï¼Œè½¬ç§»åˆ°Medium
                easy_problems = []
                for problem_id, difficulty in custom_difficulties[scheme_name].items():
                    if difficulty == "Easy":
                        model_scores = problem_model_scores[problem_id]
                        sorted_scores = []
                        for model_name, _ in model_params:
                            if model_name in model_scores:
                                sorted_scores.append(model_scores[model_name])
                        if sorted_scores:
                            easy_problems.append((problem_id, sorted_scores[-1]))  # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
                
                # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åºï¼Œé€‰æ‹©åˆ†æ•°è¾ƒä½çš„è½¬ç§»åˆ°Medium
                easy_problems.sort(key=lambda x: x[1])
                
                # è½¬ç§»æ•°é‡ï¼šç¡®ä¿Mediumè‡³å°‘æœ‰15ä¸ª
                transfer_count = min(
                    min_medium_count - difficulty_counts["Medium"],
                    difficulty_counts["Easy"] - 400  # ä¿ç•™è‡³å°‘400ä¸ªEasy
                )
                
                for i in range(transfer_count):
                    if i < len(easy_problems):
                        problem_id = easy_problems[i][0]
                        custom_difficulties[scheme_name][problem_id] = "Medium"
                        print(f"    å°†é—®é¢˜ {problem_id} ä»Easyè½¬ç§»åˆ°Medium")
            
            # é‡æ–°ç»Ÿè®¡Mediumæ•°é‡
            difficulty_counts = defaultdict(int)
            for difficulty in custom_difficulties[scheme_name].values():
                difficulty_counts[difficulty] += 1
            
            # ç¬¬äºŒæ­¥ï¼šå¦‚æœHardä¸å¤Ÿï¼Œä»Mediumä¸­è½¬ç§»
            if difficulty_counts["Hard"] < min_hard_count and difficulty_counts["Medium"] > min_medium_count:
                # æ‰¾åˆ°Mediumä¸­åˆ†æ•°è¾ƒä½çš„é—®é¢˜ï¼Œè½¬ç§»åˆ°Hard
                medium_problems = []
                for problem_id, difficulty in custom_difficulties[scheme_name].items():
                    if difficulty == "Medium":
                        model_scores = problem_model_scores[problem_id]
                        sorted_scores = []
                        for model_name, _ in model_params:
                            if model_name in model_scores:
                                sorted_scores.append(model_scores[model_name])
                        if sorted_scores:
                            medium_problems.append((problem_id, sorted_scores[-1]))  # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
                
                # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åºï¼Œé€‰æ‹©åˆ†æ•°è¾ƒä½çš„è½¬ç§»åˆ°Hard
                medium_problems.sort(key=lambda x: x[1])
                
                # è½¬ç§»æ•°é‡ï¼šç¡®ä¿Hardè¾¾åˆ°15ä¸ªï¼ŒåŒæ—¶ä¿ç•™Mediumè‡³å°‘15ä¸ª
                transfer_count = min(
                    min_hard_count - difficulty_counts["Hard"],
                    difficulty_counts["Medium"] - min_medium_count
                )
                
                for i in range(transfer_count):
                    if i < len(medium_problems):
                        problem_id = medium_problems[i][0]
                        custom_difficulties[scheme_name][problem_id] = "Hard"
                        print(f"    å°†é—®é¢˜ {problem_id} ä»Mediumè½¬ç§»åˆ°Hard")
            
            # ç¬¬ä¸‰æ­¥ï¼šå¦‚æœMediumè¿˜æ˜¯ä¸å¤Ÿï¼Œå†æ¬¡ä»Easyä¸­è½¬ç§»
            difficulty_counts = defaultdict(int)
            for difficulty in custom_difficulties[scheme_name].values():
                difficulty_counts[difficulty] += 1
            
            if difficulty_counts["Medium"] < min_medium_count and difficulty_counts["Easy"] > 400:
                # æ‰¾åˆ°Easyä¸­åˆ†æ•°è¾ƒä½çš„é—®é¢˜ï¼Œè½¬ç§»åˆ°Medium
                easy_problems = []
                for problem_id, difficulty in custom_difficulties[scheme_name].items():
                    if difficulty == "Easy":
                        model_scores = problem_model_scores[problem_id]
                        sorted_scores = []
                        for model_name, _ in model_params:
                            if model_name in model_scores:
                                sorted_scores.append(model_scores[model_name])
                        if sorted_scores:
                            easy_problems.append((problem_id, sorted_scores[-1]))  # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
                
                # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åºï¼Œé€‰æ‹©åˆ†æ•°è¾ƒä½çš„è½¬ç§»åˆ°Medium
                easy_problems.sort(key=lambda x: x[1])
                
                # è½¬ç§»æ•°é‡ï¼šç¡®ä¿Mediumè‡³å°‘æœ‰15ä¸ª
                transfer_count = min(
                    min_medium_count - difficulty_counts["Medium"],
                    difficulty_counts["Easy"] - 380  # ä¿ç•™è‡³å°‘380ä¸ªEasy
                )
                
                for i in range(transfer_count):
                    if i < len(easy_problems):
                        problem_id = easy_problems[i][0]
                        custom_difficulties[scheme_name][problem_id] = "Medium"
                        print(f"    å°†é—®é¢˜ {problem_id} ä»Easyè½¬ç§»åˆ°Medium")
            
            # ç¬¬å››æ­¥ï¼šç¡®ä¿æ‰€æœ‰æ–¹æ³•éƒ½æœ‰Hardé—®é¢˜ï¼Œä½†é¿å…è¿‡åº¦è½¬ç§»
            difficulty_counts = defaultdict(int)
            for difficulty in custom_difficulties[scheme_name].values():
                difficulty_counts[difficulty] += 1
            
            # å¦‚æœHardé—®é¢˜å¤ªå°‘ï¼ˆå°‘äº5ä¸ªï¼‰ï¼Œä»Mediumä¸­è½¬ç§»ä¸€äº›ï¼Œä½†é™åˆ¶è½¬ç§»æ•°é‡
            if difficulty_counts["Hard"] < 5 and difficulty_counts["Medium"] > 15:
                # æ‰¾åˆ°Mediumä¸­åˆ†æ•°è¾ƒä½çš„é—®é¢˜ï¼Œè½¬ç§»åˆ°Hard
                medium_problems = []
                for problem_id, difficulty in custom_difficulties[scheme_name].items():
                    if difficulty == "Medium":
                        model_scores = problem_model_scores[problem_id]
                        sorted_scores = []
                        for model_name, _ in model_params:
                            if model_name in model_scores:
                                sorted_scores.append(model_scores[model_name])
                        if sorted_scores:
                            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„ä¸‹é™è¶‹åŠ¿ï¼Œé¿å…è½¬ç§»æœ‰ä¸¥é‡ä¸‹é™çš„é—®é¢˜
                            if len(sorted_scores) >= 3:
                                # è®¡ç®—æœ€åå‡ ä¸ªæ¨¡å‹çš„å¹³å‡åˆ†æ•°ï¼Œé¿å…è½¬ç§»åˆ†æ•°è¿‡ä½çš„é—®é¢˜
                                recent_avg = np.mean(sorted_scores[-2:])  # æœ€åä¸¤ä¸ªæ¨¡å‹çš„å¹³å‡åˆ†
                                if recent_avg >= 8.5:  # åªè½¬ç§»æœ€ç»ˆåˆ†æ•°ä¸å¤ªä½çš„é—®é¢˜
                                    medium_problems.append((problem_id, recent_avg))
                
                # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åºï¼Œé€‰æ‹©åˆ†æ•°è¾ƒä½çš„è½¬ç§»åˆ°Hard
                medium_problems.sort(key=lambda x: x[1])
                
                # é™åˆ¶è½¬ç§»æ•°é‡ï¼Œé¿å…è¿‡åº¦è½¬ç§»
                transfer_count = min(
                    5 - difficulty_counts["Hard"],
                    difficulty_counts["Medium"] - 15,  # ä¿ç•™æ›´å¤šMedium
                    3  # æœ€å¤šåªè½¬ç§»3ä¸ªé—®é¢˜
                )
                
                for i in range(transfer_count):
                    if i < len(medium_problems):
                        problem_id = medium_problems[i][0]
                        custom_difficulties[scheme_name][problem_id] = "Hard"
                        print(f"    å°†é—®é¢˜ {problem_id} ä»Mediumè½¬ç§»åˆ°Hard")
            
            # å¦‚æœMediumä¸å¤Ÿï¼Œä»Easyä¸­è¡¥å……ï¼Œä½†ä¹Ÿè¦è°¨æ…
            difficulty_counts = defaultdict(int)
            for difficulty in custom_difficulties[scheme_name].values():
                difficulty_counts[difficulty] += 1
            
            if difficulty_counts["Medium"] < 10 and difficulty_counts["Easy"] > 380:
                # æ‰¾åˆ°Easyä¸­åˆ†æ•°è¾ƒä½çš„é—®é¢˜ï¼Œè½¬ç§»åˆ°Medium
                easy_problems = []
                for problem_id, difficulty in custom_difficulties[scheme_name].items():
                    if difficulty == "Easy":
                        model_scores = problem_model_scores[problem_id]
                        sorted_scores = []
                        for model_name, _ in model_params:
                            if model_name in model_scores:
                                sorted_scores.append(model_scores[model_name])
                        if sorted_scores:
                            # åªè½¬ç§»æœ€ç»ˆåˆ†æ•°ä¸å¤ªä½çš„é—®é¢˜
                            if sorted_scores[-1] >= 9.0:
                                easy_problems.append((problem_id, sorted_scores[-1]))
                
                # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åºï¼Œé€‰æ‹©åˆ†æ•°è¾ƒä½çš„è½¬ç§»åˆ°Medium
                easy_problems.sort(key=lambda x: x[1])
                
                # é™åˆ¶è½¬ç§»æ•°é‡
                transfer_count = min(
                    10 - difficulty_counts["Medium"],
                    difficulty_counts["Easy"] - 370,  # ä¿ç•™è‡³å°‘370ä¸ªEasy
                    5  # æœ€å¤šåªè½¬ç§»5ä¸ªé—®é¢˜
                )
                
                for i in range(transfer_count):
                    if i < len(easy_problems):
                        problem_id = easy_problems[i][0]
                        custom_difficulties[scheme_name][problem_id] = "Medium"
                        print(f"    å°†é—®é¢˜ {problem_id} ä»Easyè½¬ç§»åˆ°Medium")
            
            # é‡æ–°ç»Ÿè®¡
            final_counts = defaultdict(int)
            for difficulty in custom_difficulties[scheme_name].values():
                final_counts[difficulty] += 1
            
            print("è°ƒæ•´åçš„å„éš¾åº¦ç­‰çº§é—®é¢˜æ•°é‡:")
            for difficulty in ["Easy", "Medium", "Hard"]:
                count = final_counts[difficulty]
                print(f"  {difficulty}: {count} ä¸ªé—®é¢˜")
        
        return custom_difficulties
    
    def create_custom_difficulty_plots(self, all_results: Dict[str, List[Dict]], custom_difficulties: Dict[str, Dict[str, str]]):
        """åˆ›å»ºè‡ªå®šä¹‰éš¾åº¦çš„å¯¹æ¯”å›¾"""
        print("ğŸ“ˆ ç”Ÿæˆè‡ªå®šä¹‰éš¾åº¦å¯¹æ¯”å›¾...")
        
        # åˆ›å»º2x2çš„å­å›¾å¸ƒå±€
        fig, axes = plt.subplots(2, 2, figsize=(20, 14))
        fig.suptitle('Hendrycks Math Dataset - Custom Difficulty Analysis based on Model Performance', 
                     fontsize=20, fontweight='bold', y=1.02)
        
        # å°†axesè½¬æ¢ä¸º1Dæ•°ç»„ä»¥ä¾¿ç´¢å¼•
        axes = axes.flatten()
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹åç§°
        model_names = [config['short_name'] for config in self.models.values()]
        
        # ä¸ºæ¯ä¸ªæƒé‡æ–¹æ¡ˆåˆ›å»ºä¸€ä¸ªå­å›¾
        for i, (scheme_name, scheme_config) in enumerate(self.weighting_schemes.items()):
            ax = axes[i]
            ax.set_title(f'({chr(97+i)}) {scheme_config["name"]}', 
                        fontsize=16, fontweight='bold', pad=20)
            
            # æ”¶é›†è¯¥æ–¹æ¡ˆä¸‹æ‰€æœ‰è‡ªå®šä¹‰éš¾åº¦ç­‰çº§ï¼ˆåªä¿ç•™å‰ä¸‰ä¸ªï¼‰
            difficulty_levels = ["Easy", "Medium", "Hard"]
            # ä½¿ç”¨æ›´é«˜å¯¹æ¯”åº¦çš„é¢œè‰²ï¼šçº¢è‰²ã€ç»¿è‰²ã€è“è‰²
            difficulty_colors = ['#FF4444', '#00AA00', '#0066CC']
            difficulty_markers = ['o', 's', '^']
            
            for j, difficulty in enumerate(difficulty_levels):
                difficulty_scores = []
                
                # æ”¶é›†è¯¥éš¾åº¦åœ¨æ‰€æœ‰æ¨¡å‹ä¸­çš„å¹³å‡åˆ†æ•°
                for model_name in self.models.keys():
                    model_scores = []
                    
                    # æ‰¾åˆ°å±äºè¯¥éš¾åº¦çš„æ‰€æœ‰é—®é¢˜
                    for problem_id, assigned_difficulty in custom_difficulties[scheme_name].items():
                        if assigned_difficulty == difficulty:
                            # æ‰¾åˆ°è¯¥é—®é¢˜åœ¨è¯¥æ¨¡å‹ä¸‹çš„å¾—åˆ†
                            for result in all_results[model_name]:
                                # Hendrycks Mathä½¿ç”¨sample_idå­—æ®µ
                                result_id = result.get('sample_id', '')
                                if result_id == problem_id:
                                    evaluation = result.get('evaluation', {})
                                    if evaluation:
                                        score = self.calculate_weighted_score(evaluation, scheme_config['weights'])
                                        model_scores.append(score)
                                    break
                    
                    if model_scores:
                        difficulty_scores.append(np.mean(model_scores))
                    else:
                        difficulty_scores.append(np.nan)
                
                # ç»˜åˆ¶è¯¥éš¾åº¦çš„çº¿
                ax.plot(model_names, difficulty_scores, 
                       color=difficulty_colors[j], 
                       marker=difficulty_markers[j],
                       linewidth=2,
                       markersize=8,
                       label=f'{difficulty}')
            
            ax.set_xlabel('Model Parameters', fontsize=16, fontweight='bold')
            ax.set_ylabel('Average Score', fontsize=16, fontweight='bold')
            
            # æ ¹æ®æ–¹æ³•è®¾ç½®ä¸åŒçš„Yè½´èŒƒå›´
            if scheme_name == "Method 1":
                ax.set_ylim(0, 10.5)  # ç¬¬ä¸€å¼ å›¾è®¾ç½®ä¸º0-10.5ï¼Œè®©çº¢è‰²éƒ¨åˆ†å®Œå…¨æ˜¾ç¤º
            else:
                ax.set_ylim(5, 10)  # å…¶ä»–ä¸‰å¼ å›¾è®¾ç½®ä¸º5-10
            
            # è®¾ç½®åˆ»åº¦æ ‡ç­¾å­—ä½“å¤§å°
            ax.tick_params(axis='both', which='major', labelsize=14)
            
            ax.grid(True, alpha=0.3)
            
            # è®¡ç®—æ¯ä¸ªéš¾åº¦çš„æ ·æœ¬æ•°
            difficulty_counts = defaultdict(int)
            for problem_id, assigned_difficulty in custom_difficulties[scheme_name].items():
                difficulty_counts[assigned_difficulty] += 1
            
            # åˆ›å»ºå¸¦æ ·æœ¬æ•°çš„å›¾ä¾‹æ ‡ç­¾
            legend_labels = []
            for difficulty in difficulty_levels:
                count = difficulty_counts[difficulty]
                legend_labels.append(f'{difficulty} (n={count})')
            
            ax.legend(labels=legend_labels, loc='lower right', bbox_to_anchor=(1.0, 0.1), frameon=True, fancybox=True, shadow=True, fontsize=14)
            
            # ä¸ºæ¯ä¸ªå­å›¾æ·»åŠ æƒé‡ç»„åˆæ–¹æ¡ˆæ ‡æ³¨
            if scheme_name == "Method 1":
                weights_text = "Weights: AC(1.00), RL(0.00), SC(0.00), MA(0.00), EC(0.00)"
            elif scheme_name == "Method 2":
                weights_text = "Weights: AC(0.60), RL(0.00), SC(0.40), MA(0.00), EC(0.00)"
            elif scheme_name == "Method 3":
                weights_text = "Weights: AC(0.50), RL(0.00), SC(0.30), MA(0.00), EC(0.20)"
            elif scheme_name == "Method 4":
                weights_text = "Weights: AC(0.30), RL(0.25), SC(0.25), MA(0.20), EC(0.00)"
            
            ax.text(0.02, 0.02, weights_text, transform=ax.transAxes, 
                   fontsize=16, fontweight='bold', verticalalignment='bottom', 
                   bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9))
            
            # è®¾ç½®Xè½´åˆ»åº¦
            ax.set_xticks(range(len(model_names)))
            ax.set_xticklabels(model_names, rotation=0)
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.96, bottom=0.08, left=0.05, right=0.95, wspace=0.25, hspace=0.3)
        
        # ä¿å­˜å›¾ç‰‡
        plt_file = self.plot_dir / 'Hendrycks_Math_custom_difficulty_comparison.png'
        plt.savefig(plt_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"{plt_file}")
    
    def save_unified_results(self, all_results: Dict[str, List[Dict]], custom_difficulties: Dict[str, Dict[str, str]]):
        """ä¿å­˜ç»Ÿä¸€çš„ç»“æœé›†æ–‡ä»¶ï¼ŒåŒ…å«æ‰€æœ‰æ¨¡å‹å’Œæ‰€æœ‰æƒé‡æ–¹æ¡ˆçš„ç»“æœ"""
        print("ğŸ’¾ ä¿å­˜ç»Ÿä¸€ç»“æœé›†æ–‡ä»¶...")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = self.plot_dir / "results_with_custom_difficulty"
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºç»Ÿä¸€çš„ç»“æœé›†
        unified_results = []
        
        # ä¸ºæ¯ä¸ªæƒé‡æ–¹æ¡ˆå¤„ç†æ‰€æœ‰æ¨¡å‹çš„ç»“æœ
        for scheme_name in self.weighting_schemes.keys():
            print(f"  å¤„ç† {scheme_name} çš„ç»“æœ...")
            
            # æ”¶é›†è¯¥æ–¹æ¡ˆä¸‹æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„é—®é¢˜
            valid_problems = set(custom_difficulties[scheme_name].keys())
            
            # ä¸ºæ¯ä¸ªæ¨¡å‹å¤„ç†ç»“æœ
            for model_name, results in all_results.items():
                model_short_name = self.models[model_name]['short_name']
                
                for result in results:
                    problem_id = result.get('sample_id', '')
                    if problem_id in valid_problems:
                        # åˆ›å»ºç»Ÿä¸€æ ¼å¼çš„ç»“æœæ¡ç›®
                        unified_result = {
                            # åŸºç¡€ä¿¡æ¯
                            'sample_id': result.get('sample_id', 0),
                            'problem': result.get('problem', ''),
                            'correct_answer': result.get('correct_answer', ''),
                            'model_answer': result.get('model_answer', ''),
                            'subject': result.get('subject', ''),
                            'level': result.get('level', 0),
                            
                            # æ¨¡å‹ä¿¡æ¯
                            'model_name': model_name,
                            'model_short_name': model_short_name,
                            'model_params': self.models[model_name]['params'],
                            
                            # è¯„ä¼°ä¿¡æ¯
                            'evaluation': result.get('evaluation', {}),
                            
                            # æ—¶é—´æˆ³
                            'timestamp': result.get('timestamp', ''),
                            
                            # è‡ªå®šä¹‰éš¾åº¦ä¿¡æ¯
                            'custom_difficulty': custom_difficulties[scheme_name][problem_id],
                            'difficulty_scheme': scheme_name,
                            
                            # æƒé‡æ–¹æ¡ˆä¿¡æ¯
                            'weighting_scheme_name': scheme_name,
                            'weighting_scheme_weights': self.weighting_schemes[scheme_name]['weights'],
                            
                            # åŠ æƒåˆ†æ•°
                            'weighted_score': 0.0
                        }
                        
                        # è®¡ç®—åŠ æƒåˆ†æ•°
                        evaluation = result.get('evaluation', {})
                        if evaluation:
                            weights = self.weighting_schemes[scheme_name]['weights']
                            weighted_score = self.calculate_weighted_score(evaluation, weights)
                            unified_result['weighted_score'] = weighted_score
                        
                        unified_results.append(unified_result)
        
        # åˆ›å»ºå…ƒæ•°æ®
        metadata = {
            'dataset': 'Hendrycks Math',
            'analysis_date': pd.Timestamp.now().isoformat(),
            'total_models': len(self.models),
            'total_weighting_schemes': len(self.weighting_schemes),
            'total_results': len(unified_results),
            'models': {name: {
                'short_name': config['short_name'],
                'params': config['params']
            } for name, config in self.models.items()},
            'weighting_schemes': self.weighting_schemes,
            'difficulty_distribution': {}
        }
        
        # è®¡ç®—å„æƒé‡æ–¹æ¡ˆçš„éš¾åº¦åˆ†å¸ƒ
        for scheme_name in self.weighting_schemes.keys():
            difficulty_counts = defaultdict(int)
            for difficulty in custom_difficulties[scheme_name].values():
                difficulty_counts[difficulty] += 1
            metadata['difficulty_distribution'][scheme_name] = dict(difficulty_counts)
        
        # åˆ›å»ºæœ€ç»ˆçš„ç»Ÿä¸€ç»“æœæ–‡ä»¶
        final_unified_results = {
            'metadata': metadata,
            'results': unified_results
        }
        
        # ä¿å­˜ç»Ÿä¸€ç»“æœé›†
        unified_file = save_dir / "HendrycksMath_unified_results.json"
        with open(unified_file, 'w', encoding='utf-8') as f:
            json.dump(final_unified_results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ä¿å­˜ç»Ÿä¸€ç»“æœé›†åˆ°: {unified_file}")
        print(f"ğŸ“Š æ€»ç»“æœæ•°: {len(unified_results)}")
        print(f"ğŸ¤– æ¨¡å‹æ•°: {len(self.models)}")
        print(f"âš–ï¸ æƒé‡æ–¹æ¡ˆæ•°: {len(self.weighting_schemes)}")
        print(f"ğŸ“ ä¿å­˜ä½ç½®: {save_dir}")
        
        # æ˜¾ç¤ºå„æƒé‡æ–¹æ¡ˆçš„ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“ˆ å„æƒé‡æ–¹æ¡ˆç»Ÿè®¡:")
        for scheme_name in self.weighting_schemes.keys():
            scheme_results = [r for r in unified_results if r['difficulty_scheme'] == scheme_name]
            difficulty_counts = defaultdict(int)
            for result in scheme_results:
                difficulty_counts[result['custom_difficulty']] += 1
            
            print(f"  {scheme_name}: {len(scheme_results)} ä¸ªç»“æœ")
            for difficulty, count in sorted(difficulty_counts.items()):
                print(f"    {difficulty}: {count} ä¸ª")
        
        return unified_file
    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ å¼€å§‹Hendrycks Mathè‡ªå®šä¹‰éš¾åº¦åˆ†æ...")
        
        # åŠ è½½æ‰€æœ‰æ¨¡å‹ç»“æœ
        all_results = {}
        for model_name in self.models.keys():
            results = self.load_model_results(model_name)
            if results:
                all_results[model_name] = results
        
        if not all_results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç»“æœæ•°æ®")
            return
        
        # åˆ†ææ€§èƒ½æ¨¡å¼ï¼Œé‡æ–°å®šä¹‰éš¾åº¦ç­‰çº§
        custom_difficulties = self.analyze_performance_patterns(all_results)
        
        # æ˜¾ç¤ºç­›é€‰ç»“æœ
        print(f"\nğŸ“ˆ è¶‹åŠ¿ç­›é€‰ç»“æœ:")
        for scheme_name in self.weighting_schemes.keys():
            valid_count = len(custom_difficulties[scheme_name])
            print(f"  {scheme_name}: {valid_count} ä¸ªé—®é¢˜ç¬¦åˆæŒç»­å¢é•¿è¶‹åŠ¿")
        
        # åˆ›å»ºè‡ªå®šä¹‰éš¾åº¦çš„å¯¹æ¯”å›¾
        self.create_custom_difficulty_plots(all_results, custom_difficulties)
        
        # ä¿å­˜ç»Ÿä¸€ç»“æœé›†æ–‡ä»¶
        self.save_unified_results(all_results, custom_difficulties)
        
        print("âœ… Hendrycks Mathè‡ªå®šä¹‰éš¾åº¦åˆ†æå®Œæˆï¼")

def main():
    """ä¸»å‡½æ•°"""
    analyzer = HendrycksMathCustomDifficultyAnalyzer()
    analyzer.run_analysis()

if __name__ == "__main__":
    main() 