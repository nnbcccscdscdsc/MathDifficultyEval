#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€æ•°æ®é›†æƒé‡æ–¹æ¡ˆå¯¹æ¯”ç»˜å›¾è„šæœ¬
ä¸ºä¸‰ä¸ªæ•°æ®é›†ï¼ˆDeepMath-103Kã€Hendrycks Mathã€MATH-500ï¼‰åˆ†åˆ«ç”Ÿæˆä¸‰å¼ å­å›¾
æ¯å¼ å­å›¾ä½¿ç”¨ä¸åŒçš„æƒé‡è®¡ç®—æ–¹å¼ï¼Œä¿æŒä¸ä¹‹å‰å›¾ç‰‡ç±»ä¼¼çš„æ ·å¼
"""

import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Tuple
import glob

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®ç»˜å›¾æ ·å¼
sns.set_style("whitegrid")
plt.style.use('seaborn-v0_8')

class UnifiedWeightedPlotter:
    """ç»Ÿä¸€æ•°æ®é›†æƒé‡æ–¹æ¡ˆå¯¹æ¯”ç»˜å›¾å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç»˜å›¾å™¨"""
        self.plot_dir = Path("plot_data") / "weighted_comparisons"
        self.plot_dir.mkdir(parents=True, exist_ok=True)
        
        # å®šä¹‰ä¸‰ä¸ªæ•°æ®é›†
        self.datasets = {
            "DeepMath-103K": {
                "path": "data/DeepMath-103K_result",
                "title": "DeepMath-103K Dataset"
            },
            "Hendrycks Math": {
                "path": "data/hendrycks_math_results/deepseek-ai", 
                "title": "Hendrycks Math Dataset"
            },
            "MATH-500": {
                "path": "data/math500_results/deepseek-ai",
                "title": "MATH-500 Dataset"
            }
        }
        
        # å®šä¹‰ä¸‰ç§æƒé‡æ–¹æ¡ˆ
        self.weighting_schemes = {
            "Method 1": {
                "name": "Answer Correctness Only",
                "weights": {
                    "answer_correctness": 1.0,
                    "reasoning_logic": 0.0,
                    "step_completeness": 0.0,
                    "mathematical_accuracy": 0.0,
                    "expression_clarity": 0.0
                }
            },
            "Method 2": {
                "name": "Answer + Reasoning + Steps",
                "weights": {
                    "answer_correctness": 0.4,
                    "reasoning_logic": 0.3,
                    "step_completeness": 0.3,
                    "mathematical_accuracy": 0.0,
                    "expression_clarity": 0.0
                }
            },
            "Method 3": {
                "name": "Four Criteria Weighted",
                "weights": {
                    "answer_correctness": 0.3,
                    "reasoning_logic": 0.25,
                    "step_completeness": 0.25,
                    "mathematical_accuracy": 0.2,
                    "expression_clarity": 0.0
                }
            }
        }
        
        # æ¨¡å‹é…ç½®
        self.models = {
            "DeepSeek-R1-Distill-Qwen-1.5B": {
                "short_name": "1.5B",
                "color": "#FF6B6B",
                "marker": "o",
                "linewidth": 2,
                "markersize": 8,
                "params": 1.5
            },
            "DeepSeek-R1-Distill-Qwen-7B": {
                "short_name": "7B", 
                "color": "#4ECDC4",
                "marker": "s",
                "linewidth": 2,
                "markersize": 8,
                "params": 7.0
            },
            "DeepSeek-R1-Distill-Qwen-14B": {
                "short_name": "14B",
                "color": "#45B7D1", 
                "marker": "^",
                "linewidth": 2,
                "markersize": 8,
                "params": 14.0
            },
            "DeepSeek-R1-Distill-Qwen-32B": {
                "short_name": "32B",
                "color": "#96CEB4",
                "marker": "D",
                "linewidth": 2,
                "markersize": 8,
                "params": 32.0
            },
            "DeepSeek-R1-Distill-Llama-70B": {
                "short_name": "70B",
                "color": "#FFA07A",
                "marker": "p",
                "linewidth": 2,
                "markersize": 8,
                "params": 70.0
            }
        }
    
    def load_model_results(self, dataset_path: str, model_name: str) -> List[Dict]:
        """åŠ è½½æŒ‡å®šæ•°æ®é›†å’Œæ¨¡å‹çš„ç»“æœ"""
        # å¤„ç†DeepMath-103Kçš„ç‰¹æ®Šè·¯å¾„æ ¼å¼
        if "DeepMath-103K" in dataset_path:
            # DeepMath-103Kä½¿ç”¨ä¸åŒçš„æ¨¡å‹åç§°æ ¼å¼
            model_name_mapping = {
                "DeepSeek-R1-Distill-Qwen-1.5B": "deepseek_ai_DeepSeek_R1_Distill_Qwen_1.5B",
                "DeepSeek-R1-Distill-Qwen-7B": "deepseek_ai_DeepSeek_R1_Distill_Qwen_7B", 
                "DeepSeek-R1-Distill-Qwen-14B": "deepseek_ai_DeepSeek_R1_Distill_Qwen_14B",
                "DeepSeek-R1-Distill-Qwen-32B": "deepseek_ai_DeepSeek_R1_Distill_Qwen_32B",
                "DeepSeek-R1-Distill-Llama-70B": "deepseek_ai_DeepSeek_R1_Distill_Llama_70B"
            }
            actual_model_name = model_name_mapping.get(model_name, model_name)
        else:
            actual_model_name = model_name
        
        model_dir = Path(dataset_path) / actual_model_name
        
        if not model_dir.exists():
            print(f"âš ï¸ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
            return []
        
        # å¤„ç†DeepMath-103Kçš„ç‰¹æ®Šæ ¼å¼
        if "DeepMath-103K" in dataset_path:
            # DeepMath-103Kç›´æ¥åœ¨æ¨¡å‹ç›®å½•ä¸‹æœ‰intermediate_results_*.jsonæ–‡ä»¶
            result_files = list(model_dir.glob("intermediate_results_*.json"))
            
            if not result_files:
                print(f"âš ï¸ æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶: {model_dir}")
                return []
            
            # é€‰æ‹©æœ€æ–°çš„ç»“æœæ–‡ä»¶
            latest_file = max(result_files, key=lambda x: x.stat().st_ctime)
            print(f"ğŸ” ä½¿ç”¨ç»“æœæ–‡ä»¶: {latest_file}")
            
            results_file = latest_file
        else:
            # å…¶ä»–æ•°æ®é›†ä½¿ç”¨è¿è¡Œç›®å½•æ ¼å¼
            run_dirs = [d for d in model_dir.iterdir() if d.is_dir()]
            
            if not run_dirs:
                print(f"âš ï¸ æœªæ‰¾åˆ°è¿è¡Œç›®å½•: {model_dir}")
                return []
            
            # åªé€‰æ‹©åŒ…å«final_results.jsonçš„æœ‰æ•ˆç›®å½•
            valid_dirs = [d for d in run_dirs if (d / 'final_results.json').exists()]
            
            if not valid_dirs:
                print(f"âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç»“æœæ–‡ä»¶: {model_dir}")
                return []
            
            # é€‰æ‹©æœ€æ–°çš„æœ‰æ•ˆç›®å½•
            latest_run_dir = max(valid_dirs, key=lambda x: x.stat().st_ctime)
            print(f"ğŸ” ä½¿ç”¨ç»“æœç›®å½•: {latest_run_dir}")
            
            results_file = latest_run_dir / 'final_results.json'
        
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æå–ç»“æœåˆ—è¡¨
            if isinstance(data, dict) and "results" in data:
                results = data["results"]
            else:
                results = data
            
            print(f"âœ… åŠ è½½ {model_name} ç»“æœ: {len(results)} ä¸ªæ ·æœ¬")
            return results
            
        except Exception as e:
            print(f"âŒ åŠ è½½ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def calculate_weighted_score(self, evaluation: Dict, weights: Dict) -> float:
        """æ ¹æ®æƒé‡æ–¹æ¡ˆè®¡ç®—åŠ æƒæ€»åˆ†"""
        if not evaluation or not isinstance(evaluation, dict):
            return 0.0
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for criterion, weight in weights.items():
            if criterion in evaluation and weight > 0:
                score = evaluation[criterion]
                if isinstance(score, (int, float)):
                    weighted_sum += score * weight
                    total_weight += weight
        
        # å¦‚æœæ€»æƒé‡ä¸º0ï¼Œè¿”å›0
        if total_weight == 0:
            return 0.0
        
        # è¿”å›åŠ æƒå¹³å‡åˆ†
        return weighted_sum / total_weight
    
    def analyze_model_performance(self, results: List[Dict]) -> Dict:
        """åˆ†ææ¨¡å‹æ€§èƒ½"""
        if not results:
            return {}
        
        # æŒ‰éš¾åº¦åˆ†ç»„
        difficulty_scores = {}
        
        for result in results:
            # è·å–éš¾åº¦ï¼ˆæ”¯æŒdifficultyå’Œlevelå­—æ®µï¼‰
            difficulty = result.get('difficulty', result.get('level', 0))
            
            # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„éš¾åº¦ï¼ˆå¦‚"Level 1"ï¼‰
            if isinstance(difficulty, str):
                if difficulty.startswith('Level '):
                    try:
                        difficulty = int(difficulty.split(' ')[1])
                    except (ValueError, IndexError):
                        continue
                else:
                    try:
                        difficulty = float(difficulty)
                    except ValueError:
                        continue
            
            if difficulty == 0:
                continue
            
            # è·å–è¯„ä¼°ç»“æœ
            evaluation = result.get('evaluation', {})
            if not evaluation:
                continue
            
            # è®¡ç®—ä¸‰ç§æƒé‡æ–¹æ¡ˆçš„æ€»åˆ†
            scores = {}
            for scheme_name, scheme_config in self.weighting_schemes.items():
                weighted_score = self.calculate_weighted_score(evaluation, scheme_config['weights'])
                scores[scheme_name] = weighted_score
            
            # æ·»åŠ åˆ°éš¾åº¦åˆ†ç»„
            if difficulty not in difficulty_scores:
                difficulty_scores[difficulty] = {
                    "Method 1": [],
                    "Method 2": [],
                    "Method 3": []
                }
            
            for scheme_name, score in scores.items():
                difficulty_scores[difficulty][scheme_name].append(score)
        
        # è®¡ç®—æ¯ä¸ªéš¾åº¦çš„å¹³å‡åˆ†
        performance = {}
        for difficulty, scores_dict in difficulty_scores.items():
            performance[difficulty] = {}
            for scheme_name, scores_list in scores_dict.items():
                if scores_list:
                    performance[difficulty][scheme_name] = {
                        'mean': np.mean(scores_list),
                        'std': np.std(scores_list),
                        'count': len(scores_list)
                    }
        
        return performance
    
    def create_dataset_comparison_plots(self, dataset_name: str, all_performances: Dict[str, Dict]):
        """ä¸ºæŒ‡å®šæ•°æ®é›†åˆ›å»ºä¸‰ç§æƒé‡æ–¹æ¡ˆçš„å¯¹æ¯”å›¾"""
        dataset_config = self.datasets[dataset_name]
        
        # åˆ›å»º1x3çš„å­å›¾å¸ƒå±€
        fig, axes = plt.subplots(1, 3, figsize=(20, 7))
        fig.suptitle(f'{dataset_config["title"]} - Model Performance Comparison under Different Weighting Schemes', 
                     fontsize=18, fontweight='bold', y=0.92)
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹åç§°ï¼ˆåŒ…æ‹¬æ²¡æœ‰æ•°æ®çš„æ¨¡å‹ï¼‰
        model_names = []
        for model_name in self.models.keys():
            config = self.models[model_name]
            model_names.append(config['short_name'])
        
        # ä¸ºæ¯ä¸ªæƒé‡æ–¹æ¡ˆåˆ›å»ºä¸€ä¸ªå­å›¾
        for i, (scheme_name, scheme_config) in enumerate(self.weighting_schemes.items()):
            ax = axes[i]
            ax.set_title(f'({chr(97+i)}) {scheme_config["name"]}', 
                        fontsize=16, fontweight='bold', pad=30)
            
            # æ”¶é›†è¯¥æ–¹æ¡ˆä¸‹æ‰€æœ‰æ¨¡å‹åœ¨æ¯ä¸ªéš¾åº¦çš„å¹³å‡åˆ†
            all_difficulties = set()
            for model_name, performance in all_performances.items():
                if performance:
                    all_difficulties.update(performance.keys())
            all_difficulties = sorted(all_difficulties)
            
            # ä¸ºæ¯ä¸ªéš¾åº¦ç»˜åˆ¶ä¸€æ¡çº¿
            difficulty_colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFA07A', '#DDA0DD']
            difficulty_markers = ['o', 's', '^', 'D', 'p', '*']
            
            for j, difficulty in enumerate(all_difficulties):
                difficulty_scores = []
                
                # æ”¶é›†è¯¥éš¾åº¦åœ¨æ‰€æœ‰æ¨¡å‹ä¸­çš„å¹³å‡åˆ†æ•°
                for model_name in self.models.keys():
                    if model_name in all_performances and difficulty in all_performances[model_name]:
                        if scheme_name in all_performances[model_name][difficulty]:
                            scores = all_performances[model_name][difficulty][scheme_name]
                            difficulty_scores.append(scores['mean'])
                        else:
                            difficulty_scores.append(np.nan)
                    else:
                        difficulty_scores.append(np.nan)
                
                # ç»˜åˆ¶è¯¥éš¾åº¦çš„çº¿
                ax.plot(model_names, difficulty_scores, 
                       color=difficulty_colors[j % len(difficulty_colors)], 
                       marker=difficulty_markers[j % len(difficulty_markers)],
                       linewidth=2,
                       markersize=8,
                       label=f'Level {difficulty}')
            
            ax.set_xlabel('Model Parameters', fontsize=14, fontweight='bold')
            ax.set_ylabel('Average Score', fontsize=14, fontweight='bold')
            ax.set_ylim(0, 10)  # è®¾ç½®Yè½´èŒƒå›´ä¸º0-10
            ax.grid(True, alpha=0.3)
            ax.legend(loc='lower left', frameon=True, fancybox=True, shadow=True, fontsize=12)
            
            # è®¾ç½®Xè½´åˆ»åº¦
            ax.set_xticks(range(len(model_names)))
            ax.set_xticklabels(model_names, rotation=0)
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.88, bottom=0.12, left=0.05, right=0.95, wspace=0.25)
        
        # ä¿å­˜å›¾ç‰‡
        safe_dataset_name = dataset_name.replace('-', '_').replace(' ', '_')
        plt_file = self.plot_dir / f'{safe_dataset_name}_weighted_comparison.png'
        plt.savefig(plt_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"{plt_file}")  # åªæ‰“å°å›¾ç‰‡è·¯å¾„
    
    def generate_dataset_report(self, dataset_name: str, all_performances: Dict[str, Dict]):
        """ç”Ÿæˆæ•°æ®é›†æ±‡æ€»æŠ¥å‘Š"""
        dataset_config = self.datasets[dataset_name]
        safe_dataset_name = dataset_name.replace('-', '_').replace(' ', '_')
        
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append(f"{dataset_config['title']} - Performance Analysis Report under Different Weighting Schemes")
        report_lines.append("=" * 80)
        report_lines.append(f"Analysis Time: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        
        # æƒé‡æ–¹æ¡ˆè¯´æ˜
        report_lines.append("ğŸ“Š Weighting Schemes Description:")
        report_lines.append("-" * 40)
        for scheme_name, scheme_config in self.weighting_schemes.items():
            report_lines.append(f"{scheme_name}: {scheme_config['name']}")
            for criterion, weight in scheme_config['weights'].items():
                if weight > 0:
                    report_lines.append(f"  - {criterion}: {weight*100:.0f}%")
            report_lines.append("")
        
        # æ¨¡å‹æ€§èƒ½æ±‡æ€»
        report_lines.append("ğŸ“ˆ Model Performance Summary:")
        report_lines.append("-" * 40)
        
        for model_name, performance in all_performances.items():
            if not performance:
                continue
                
            config = self.models[model_name]
            report_lines.append(f"\n{config['short_name']} Model ({config['params']}B parameters):")
            
            # è®¡ç®—æ¯ä¸ªæƒé‡æ–¹æ¡ˆçš„å¹³å‡åˆ†
            for scheme_name in self.weighting_schemes.keys():
                all_scores = []
                for difficulty, diff_perf in performance.items():
                    if scheme_name in diff_perf:
                        all_scores.append(diff_perf[scheme_name]['mean'])
                
                if all_scores:
                    mean_score = np.mean(all_scores)
                    std_score = np.std(all_scores)
                    report_lines.append(f"  {scheme_name}: {mean_score:.2f} Â± {std_score:.2f}")
        
        # æŒ‰éš¾åº¦ç­‰çº§çš„æ€§èƒ½å¯¹æ¯”
        report_lines.append("\nğŸ“Š Performance Comparison by Difficulty Level:")
        report_lines.append("-" * 40)
        
        # æ”¶é›†æ‰€æœ‰éš¾åº¦
        all_difficulties = set()
        for performance in all_performances.values():
            if performance:
                all_difficulties.update(performance.keys())
        all_difficulties = sorted(all_difficulties)
        
        for difficulty in all_difficulties:
            report_lines.append(f"\nLevel {difficulty}:")
            for scheme_name in self.weighting_schemes.keys():
                report_lines.append(f"  {scheme_name}:")
                for model_name, performance in all_performances.items():
                    if performance and difficulty in performance and scheme_name in performance[difficulty]:
                        config = self.models[model_name]
                        scores = performance[difficulty][scheme_name]
                        report_lines.append(f"    {config['short_name']}: {scores['mean']:.2f} Â± {scores['std']:.2f} (n={scores['count']})")
        
        # ä¸ä¿å­˜æŠ¥å‘Šæ–‡ä»¶ï¼Œåªæ‰“å°å›¾ç‰‡è·¯å¾„
        pass
    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        for dataset_name, dataset_config in self.datasets.items():
            # æ£€æŸ¥æ•°æ®é›†è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not Path(dataset_config['path']).exists():
                continue
            
            # åŠ è½½æ‰€æœ‰æ¨¡å‹ç»“æœ
            all_performances = {}
            
            for model_name in self.models.keys():
                results = self.load_model_results(dataset_config['path'], model_name)
                
                if results:
                    performance = self.analyze_model_performance(results)
                    all_performances[model_name] = performance
            
            if not all_performances:
                continue
            
            # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
            self.create_dataset_comparison_plots(dataset_name, all_performances)

def main():
    """ä¸»å‡½æ•°"""
    plotter = UnifiedWeightedPlotter()
    plotter.run_analysis()

if __name__ == "__main__":
    main() 