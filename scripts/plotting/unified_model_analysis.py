#!/usr/bin/env python3
"""
ç»Ÿä¸€æ¨¡å‹æ€§èƒ½åˆ†æè„šæœ¬
æ•´åˆäº†æ¨¡å‹æ€§èƒ½æ‹ç‚¹åˆ†æå’Œä¸“ä¸šå¯è§†åŒ–åŠŸèƒ½
æ”¯æŒå¤šæ¨¡å‹å¯¹æ¯”ã€æ‹ç‚¹åˆ†æã€ä¸“ä¸šå›¾è¡¨ç”Ÿæˆ
"""

import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import glob
from typing import Dict, List, Tuple
import warnings
from pathlib import Path
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸“ä¸šæ˜¾ç¤ºæ ·å¼
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid", {'grid.linestyle': '--', 'grid.alpha': 0.3})
sns.set_palette("husl")

class UnifiedModelAnalyzer:
    def __init__(self, data_dir: str = "data"):
        self.data_dir = data_dir
        self.results = []
        self.df = None
        self.model_results = {}
        
    def load_all_data(self):
        """åŠ è½½plot_dataç›®å½•ä¸‹çš„æ•°æ®"""
        print("ğŸ“ åŠ è½½plot_dataç›®å½•ä¸‹çš„æ•°æ®...")
        
        # åŠ è½½plot_dataç›®å½•ä¸‹çš„æ•°æ®
        plot_data_dir = os.path.join(self.data_dir, "plot_data")
        if os.path.exists(plot_data_dir):
            # é€’å½’æœç´¢æ‰€æœ‰å­ç›®å½•ä¸­çš„ä¸­é—´ç»“æœæ–‡ä»¶
            pattern = os.path.join(plot_data_dir, "**", "intermediate_results_*.json")
            files = glob.glob(pattern, recursive=True)
            
            for file_path in files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # ä»æ–‡ä»¶è·¯å¾„æå–æ¨¡å‹ä¿¡æ¯å’Œè¿è¡ŒID
                    model_name = self._extract_model_name(file_path)
                    run_id = self._extract_run_id(file_path)
                    
                    # ä¸ºæ¯ä¸ªç»“æœæ·»åŠ æ¨¡å‹ä¿¡æ¯å’Œè¿è¡ŒID
                    for result in data:
                        result['model'] = model_name
                        result['run_id'] = run_id
                    
                    self.results.extend(data)
                    print(f"âœ… åŠ è½½plot_data: {file_path} ({len(data)} ä¸ªæ ·æœ¬, è¿è¡ŒID: {run_id})")
                    
                except Exception as e:
                    print(f"âŒ åŠ è½½ {file_path} å¤±è´¥: {e}")
        else:
            print(f"âŒ plot_dataç›®å½•ä¸å­˜åœ¨: {plot_data_dir}")
            return False
        
        # è½¬æ¢ä¸ºDataFrame
        if self.results:
            self.df = pd.DataFrame(self.results)
            
            # è¿‡æ»¤æ‰Unknownæ¨¡å‹çš„æ•°æ®
            original_count = len(self.df)
            self.df = self.df[self.df['model'] != 'Unknown']
            filtered_count = len(self.df)
            
            if original_count != filtered_count:
                print(f"âš ï¸  è¿‡æ»¤æ‰ {original_count - filtered_count} ä¸ªUnknownæ¨¡å‹çš„æ ·æœ¬")
            
            # åªä¿ç•™1.5Bã€7Bã€14Bã€32Bæ¨¡å‹çš„æ•°æ®
            target_models = ['1.5B', '7B', '14B', '32B']
            self.df = self.df[self.df['model'].isin(target_models)]
            final_count = len(self.df)
            
            print(f"ğŸ“Š æ€»å…±åŠ è½½ {final_count} ä¸ªæœ‰æ•ˆæ ·æœ¬ (åŒ…å«1.5Bã€7Bã€14Bã€32Bæ¨¡å‹)")
            return True
        else:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ•°æ®")
            return False
    
    def _extract_model_name(self, file_path: str) -> str:
        """ä»æ–‡ä»¶è·¯å¾„æå–æ¨¡å‹åç§°"""
        # ä»è·¯å¾„ä¸­æå–æ¨¡å‹ç›®å½•å
        path_parts = file_path.split(os.sep)
        
        # æŸ¥æ‰¾plot_dataç›®å½•åçš„æ¨¡å‹ç›®å½•å
        for i, part in enumerate(path_parts):
            if part == "plot_data" and i + 1 < len(path_parts):
                model_dir = path_parts[i + 1]
                # ä»ç›®å½•åæå–æ¨¡å‹å¤§å°
                if "1.5B" in model_dir:
                    return "1.5B"
                elif "7B" in model_dir:
                    return "7B"
                elif "14B" in model_dir:
                    return "14B"
                elif "32B" in model_dir:
                    return "32B"
                elif "70B" in model_dir:
                    return "70B"
                else:
                    # æ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                    print(f"âš ï¸  æ— æ³•è¯†åˆ«çš„æ¨¡å‹ç›®å½•: {model_dir}")
                    return "Unknown"
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»æ–‡ä»¶åæå–
        filename = os.path.basename(file_path)
        if "1.5B" in filename:
            return "1.5B"
        elif "7B" in filename:
            return "7B"
        elif "14B" in filename:
            return "14B"
        elif "32B" in filename:
            return "32B"
        elif "70B" in filename:
            return "70B"
        else:
            # æ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
            print(f"âš ï¸  æ— æ³•è¯†åˆ«çš„æ–‡ä»¶å: {filename}")
            return "Unknown"
    
    def _extract_run_id(self, file_path: str) -> str:
        """ä»æ–‡ä»¶è·¯å¾„æå–è¿è¡ŒID"""
        path_parts = file_path.split(os.sep)
        
        # æŸ¥æ‰¾æ¨¡å‹ç›®å½•åçš„è¿è¡ŒIDç›®å½•
        for i, part in enumerate(path_parts):
            if part == "intermediate" and i + 2 < len(path_parts):
                # è¿è¡ŒIDæ˜¯æ¨¡å‹ç›®å½•åçš„ç¬¬ä¸€ä¸ªå­ç›®å½•
                run_id = path_parts[i + 2]
                return run_id
        
        return "unknown_run"
    
    def get_model_params(self, model_name: str) -> float:
        """è·å–æ¨¡å‹å‚æ•°é‡"""
        param_mapping = {
            "1.5B": 1.5,
            "7B": 7.0,
            "14B": 14.0,
            "32B": 32.0,
            "70B": 70.0
        }
        return param_mapping.get(model_name, 0)
    
    def analyze_performance(self):
        """åˆ†ææ¨¡å‹æ€§èƒ½"""
        if self.df is None or self.df.empty:
            print("âŒ æ²¡æœ‰æ•°æ®å¯åˆ†æ")
            return None
            
        print("\nğŸ“ˆ åˆ†ææ¨¡å‹æ€§èƒ½...")
        
        # æ·»åŠ ç”ŸæˆçŠ¶æ€ä¿¡æ¯
        self.df['generation_status'] = self.df.get('generation_status', 'success')
        
        # ç»Ÿè®¡ç”Ÿæˆå¤±è´¥ç‡
        total_samples = len(self.df)
        failed_samples = len(self.df[self.df['generation_status'] == 'failed'])
        success_samples = len(self.df[self.df['generation_status'] == 'success'])
        
        print(f"ğŸ“Š ç”Ÿæˆç»Ÿè®¡:")
        print(f"  - æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"  - æˆåŠŸç”Ÿæˆ: {success_samples} ({success_samples/total_samples*100:.1f}%)")
        print(f"  - ç”Ÿæˆå¤±è´¥: {failed_samples} ({failed_samples/total_samples*100:.1f}%)")
        
        # æå–evaluationä¸­çš„overall_score
        self.df['overall_score'] = self.df['evaluation'].apply(
            lambda x: x.get('overall_score', 0) if isinstance(x, dict) else 0
        )
        
        # åªè€ƒè™‘æˆåŠŸç”Ÿæˆçš„æ ·æœ¬è¿›è¡Œæ€§èƒ½åˆ†æ
        successful_df = self.df[self.df['generation_status'] == 'success'].copy()
        
        if successful_df.empty:
            print("âŒ æ²¡æœ‰æˆåŠŸç”Ÿæˆçš„æ ·æœ¬å¯åˆ†æ")
            return None
        
        # æŒ‰éš¾åº¦å’Œæ¨¡å‹åˆ†ç»„è®¡ç®—å¹³å‡å¾—åˆ†
        performance = successful_df.groupby(['difficulty', 'model']).agg({
            'overall_score': ['mean', 'std', 'count']
        }).round(3)
        
        # é‡å‘½ååˆ—
        performance.columns = ['score_mean', 'score_std', 'sample_count']
        performance = performance.reset_index()
        
        # æ·»åŠ æ¨¡å‹å‚æ•°é‡
        performance['params'] = performance['model'].apply(self.get_model_params)
        
        # æ·»åŠ å¹³å‡æ¨ç†æ—¶é—´ï¼ˆå¦‚æœæ²¡æœ‰inference_timeå­—æ®µï¼Œè®¾ä¸º1ï¼‰
        if 'inference_time' in successful_df.columns:
            avg_times = successful_df.groupby(['difficulty', 'model'])['inference_time'].mean()
            performance['avg_time'] = performance.set_index(['difficulty', 'model']).index.map(
                lambda x: avg_times.get(x, 1)
            )
        else:
            performance['avg_time'] = 1  # é»˜è®¤å€¼
        
        # è®¡ç®—æ•ˆç‡ (å¾—åˆ†/æ—¶é—´)
        performance['efficiency'] = performance['score_mean'] / performance['avg_time']
        
        # æ ‡å‡†åŒ–å¾—åˆ†å’Œæ•ˆç‡
        for difficulty in performance['difficulty'].unique():
            diff_mask = performance['difficulty'] == difficulty
            if diff_mask.sum() > 1:  # è‡³å°‘æœ‰ä¸¤ä¸ªæ¨¡å‹æ‰èƒ½æ ‡å‡†åŒ–
                # æ ‡å‡†åŒ–å¾—åˆ†
                score_min = performance.loc[diff_mask, 'score_mean'].min()
                score_max = performance.loc[diff_mask, 'score_mean'].max()
                if score_max != score_min:
                    performance.loc[diff_mask, 'score_normalized'] = (
                        performance.loc[diff_mask, 'score_mean'] - score_min
                    ) / (score_max - score_min)
                else:
                    performance.loc[diff_mask, 'score_normalized'] = 0.5
                
                # æ ‡å‡†åŒ–æ•ˆç‡
                eff_min = performance.loc[diff_mask, 'efficiency'].min()
                eff_max = performance.loc[diff_mask, 'efficiency'].max()
                if eff_max != eff_min:
                    performance.loc[diff_mask, 'efficiency_normalized'] = (
                        performance.loc[diff_mask, 'efficiency'] - eff_min
                    ) / (eff_max - eff_min)
                else:
                    performance.loc[diff_mask, 'efficiency_normalized'] = 0.5
            else:
                performance.loc[diff_mask, 'score_normalized'] = 0.5
                performance.loc[diff_mask, 'efficiency_normalized'] = 0.5
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        performance['composite_score'] = (
            performance['score_normalized'] * 0.7 + 
            performance['efficiency_normalized'] * 0.3
        )
        
        print(f"âœ… æ€§èƒ½åˆ†æå®Œæˆï¼Œå…±åˆ†æäº† {len(successful_df)} ä¸ªæˆåŠŸç”Ÿæˆçš„æ ·æœ¬")
        
        return performance
    
    def find_optimal_thresholds(self, performance_df):
        """æ‰¾åˆ°æœ€ä¼˜æ¨¡å‹é€‰æ‹©çš„æ‹ç‚¹"""
        print("\nğŸ¯ å¯»æ‰¾æœ€ä¼˜æ¨¡å‹é€‰æ‹©æ‹ç‚¹...")
        
        # æŒ‰éš¾åº¦æ’åº
        performance_df = performance_df.sort_values('difficulty')
        
        # è®¡ç®—æ¯ä¸ªéš¾åº¦ä¸‹å“ªä¸ªæ¨¡å‹è¡¨ç°æœ€å¥½
        best_models = []
        for difficulty in performance_df['difficulty'].unique():
            diff_data = performance_df[performance_df['difficulty'] == difficulty]
            best_model = diff_data.loc[diff_data['composite_score'].idxmax()]
            best_models.append(best_model)
            
            print(f"éš¾åº¦ {difficulty}: æœ€ä¼˜æ¨¡å‹ {best_model['model']} (ç»¼åˆå¾—åˆ†: {best_model['composite_score']:.3f})")
        
        return best_models
    
    def generate_comprehensive_plots(self, performance_df):
        """ç”Ÿæˆç®€æ´çš„æ‹ç‚¹åˆ†æå›¾è¡¨"""
        if performance_df is None:
            return
            
        print("\nğŸ“Š ç”Ÿæˆç®€æ´çš„æ‹ç‚¹åˆ†æå›¾è¡¨...")
        
        # åˆ›å»ºç®€æ´çš„1x2å¸ƒå±€
        fig = plt.figure(figsize=(20, 10))
        fig.suptitle('DeepSeek-R1 Series Model Performance Analysis (1.5B, 7B, 14B, 32B)', 
                     fontsize=22, fontweight='bold', y=0.92)
        
        # å®šä¹‰ç®€æ´çš„é¢œè‰²æ–¹æ¡ˆ
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA500']  # 1.5B, 7B, 14B, 32B
        
        # 1. å¹³å‡å¾—åˆ†å¯¹æ¯”å›¾ (Left) - æ˜¾ç¤ºæ‹ç‚¹
        ax1 = plt.subplot(1, 2, 1)
        ax1.set_title('(a) Average Score by Difficulty Level', fontsize=18, fontweight='bold', pad=30)
        
        pivot_scores = performance_df.pivot(index='difficulty', columns='model', values='score_mean')
        pivot_scores.plot(kind='line', marker='o', ax=ax1, linewidth=3, markersize=10)
        ax1.set_xlabel('Problem Difficulty', fontsize=14)
        ax1.set_ylabel('Average Score', fontsize=14)
        # å°†å›¾ä¾‹ç§»åˆ°å·¦ä¸Šè§’ï¼Œé¿å…é®æŒ¡æ›²çº¿
        ax1.legend(title='Model', loc='upper left', fontsize=12, bbox_to_anchor=(0.02, 0.98))
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0, 10)
        
        # æ ‡è®°æ‹ç‚¹
        for model in pivot_scores.columns:
            scores = pivot_scores[model].dropna()
            if len(scores) > 2:
                # æ‰¾åˆ°å¾—åˆ†å˜åŒ–æœ€å¤§çš„ç‚¹ï¼ˆæ‹ç‚¹ï¼‰
                diff_scores = scores.diff().abs()
                inflection_point = diff_scores.idxmax()
                if not pd.isna(inflection_point):
                    ax1.annotate(f'Peak Point', 
                                xy=(inflection_point, scores[inflection_point]), 
                                xytext=(20, 20), 
                                textcoords='offset points',
                                bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.8),
                                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.3', lw=2),
                                fontsize=12, fontweight='bold')
        
        # 2. éš¾åº¦æ ·æœ¬æ•°æ®æŸ±å½¢å›¾ (Right) - ä¿®å¤æ ·æœ¬ç»Ÿè®¡
        ax2 = plt.subplot(1, 2, 2)
        ax2.set_title('(b) Sample Count Distribution by Difficulty', fontsize=18, fontweight='bold', pad=30)
        
        # é‡æ–°è®¡ç®—æ¯ä¸ªéš¾åº¦çš„å®é™…æ ·æœ¬æ•°é‡ï¼ˆä»åŸå§‹æ•°æ®è®¡ç®—ï¼‰
        if self.df is not None and not self.df.empty:
            # ä»åŸå§‹æ•°æ®è®¡ç®—æ¯ä¸ªéš¾åº¦çš„æ ·æœ¬æ•°é‡
            actual_sample_counts = self.df.groupby('difficulty').size().reset_index(name='count')
            sample_counts = actual_sample_counts
        else:
            # å¦‚æœåŸå§‹æ•°æ®ä¸å¯ç”¨ï¼Œä½¿ç”¨æ€§èƒ½æ•°æ®ä¸­çš„æ ·æœ¬æ•°
            sample_counts = performance_df.groupby('difficulty')['sample_count'].sum().reset_index()
        
        # ä½¿ç”¨ç®€æ´çš„è“è‰²æ¸å˜
        base_color = '#4A90E2'  # ç®€æ´çš„è“è‰²
        colors_simple = [base_color] * len(sample_counts)
        
        # ç»˜åˆ¶æŸ±å½¢å›¾
        bars = ax2.bar(sample_counts['difficulty'], sample_counts['count'] if 'count' in sample_counts.columns else sample_counts['sample_count'], 
                      color=colors_simple, alpha=0.7, edgecolor='#2E5BBA', linewidth=1)
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, bar in enumerate(bars):
            height = bar.get_height()
            # æ˜¾ç¤ºæ‰€æœ‰æŸ±å­çš„æ•°å€¼
            ax2.text(bar.get_x() + bar.get_width()/2., height + max(sample_counts['count'] if 'count' in sample_counts.columns else sample_counts['sample_count']) * 0.01,
                    f'{int(height)}', ha='center', va='bottom', fontsize=10, 
                    color='#2E5BBA', fontweight='bold')
        
        ax2.set_xlabel('Problem Difficulty', fontsize=14)
        ax2.set_ylabel('Total Sample Count', fontsize=14)
        ax2.grid(True, alpha=0.2, axis='y')
        
        # æ·»åŠ ç®€æ´çš„ç»Ÿè®¡ä¿¡æ¯
        total_samples = (sample_counts['count'] if 'count' in sample_counts.columns else sample_counts['sample_count']).sum()
        ax2.text(0.02, 0.98, f'Total: {total_samples:,}', 
                transform=ax2.transAxes, fontsize=12, verticalalignment='top',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='#F0F8FF', alpha=0.9, edgecolor='#4A90E2'))
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.85, bottom=0.15, left=0.05, right=0.95, wspace=0.3)
        
        # ä¿å­˜å›¾ç‰‡
        os.makedirs("data/plots", exist_ok=True)
        plt.savefig('data/plots/focused_model_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… ç®€æ´æ‹ç‚¹åˆ†æå›¾å·²ä¿å­˜åˆ°: data/plots/focused_model_analysis.png")
        
        # è¿”å›æ€§èƒ½æ•°æ®ç”¨äºè¿›ä¸€æ­¥åˆ†æ
        return performance_df
    
    def generate_recommendations(self, performance_df, composite_scores):
        """ç”Ÿæˆæ¨¡å‹é€‰æ‹©å»ºè®®"""
        if performance_df is None:
            return
            
        print("\n" + "="*60)
        print("ğŸ¯ æ¨¡å‹é€‰æ‹©å»ºè®®")
        print("="*60)
        
        # æŒ‰éš¾åº¦åˆ†æ
        for difficulty in sorted(performance_df['difficulty'].unique()):
            print(f"\nğŸ“Š éš¾åº¦ {difficulty} é¢˜ç›®:")
            
            # è·å–è¯¥éš¾åº¦ä¸‹æ‰€æœ‰æ¨¡å‹çš„æ•°æ®
            diff_data = performance_df[performance_df['difficulty'] == difficulty]
            
            # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
            diff_data = diff_data.sort_values('composite_score', ascending=False)
            
            for i, (_, row) in enumerate(diff_data.iterrows()):
                if i == 0:
                    print(f"  ğŸ¥‡ æ¨è: {row['model']} (ç»¼åˆå¾—åˆ†: {row['composite_score']:.3f})")
                    print(f"      - å¹³å‡å¾—åˆ†: {row['score_mean']:.3f}")
                    print(f"      - å¹³å‡æ—¶é—´: {row['avg_time']:.3f}s")
                    print(f"      - æ•ˆç‡: {row['efficiency']:.3f}")
                elif i == 1:
                    print(f"  ğŸ¥ˆ å¤‡é€‰: {row['model']} (ç»¼åˆå¾—åˆ†: {row['composite_score']:.3f})")
                else:
                    print(f"  ğŸ¥‰ å¤‡é€‰: {row['model']} (ç»¼åˆå¾—åˆ†: {row['composite_score']:.3f})")
        
        print("\n" + "="*60)
        print("ğŸ’¡ å»ºè®®è¯´æ˜:")
        print("- ç»¼åˆå¾—åˆ† = æ ‡å‡†åŒ–å¾—åˆ† Ã— 0.7 + æ ‡å‡†åŒ–æ•ˆç‡ Ã— 0.3")
        print("- æ•ˆç‡ = å¾—åˆ† / æ¨ç†æ—¶é—´")
        print("- æ¨èæ¨¡å‹åœ¨å‡†ç¡®ç‡å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡")
        print("="*60)
    
    def debug_data_structure(self):
        """è°ƒè¯•æ•°æ®ç»“æ„"""
        print("\nğŸ” æ•°æ®ç»“æ„è°ƒè¯•ä¿¡æ¯:")
        print("="*50)
        
        if self.df is not None and not self.df.empty:
            print(f"æ€»æ ·æœ¬æ•°: {len(self.df)}")
            print(f"æ¨¡å‹æ•°é‡: {self.df['model'].nunique()}")
            print(f"éš¾åº¦èŒƒå›´: {self.df['difficulty'].min()} - {self.df['difficulty'].max()}")
            print(f"æˆåŠŸç”Ÿæˆç‡: {(self.df['generation_status'] == 'success').mean()*100:.1f}%")
            
            # æ£€æŸ¥å¾—åˆ†åˆ†å¸ƒ
            successful_df = self.df[self.df['generation_status'] == 'success']
            if not successful_df.empty:
                scores = successful_df['evaluation'].apply(
                    lambda x: x.get('overall_score', 0) if isinstance(x, dict) else 0
                )
                print(f"å¾—åˆ†èŒƒå›´: {scores.min():.2f} - {scores.max():.2f}")
                print(f"å¹³å‡å¾—åˆ†: {scores.mean():.2f}")
        
        if self.model_results:
            print(f"æœ€ç»ˆç»“æœæ–‡ä»¶æ•°: {len(self.model_results)}")
            for model, data in self.model_results.items():
                sample_count = len(data.get('results', []))
                print(f"  - {model}: {sample_count} ä¸ªæ ·æœ¬")

def main():
    """ä¸»å‡½æ•°"""
    analyzer = UnifiedModelAnalyzer()
    
    # 1. åŠ è½½æ‰€æœ‰æ•°æ®
    if not analyzer.load_all_data():
        return
    
    # 2. è°ƒè¯•æ•°æ®ç»“æ„
    analyzer.debug_data_structure()
    
    # 3. åˆ†ææ€§èƒ½
    performance_df = analyzer.analyze_performance()
    
    if performance_df is not None:
        # 4. å¯»æ‰¾æœ€ä¼˜æ‹ç‚¹
        best_models = analyzer.find_optimal_thresholds(performance_df)
        
        # 5. ç”Ÿæˆç»¼åˆå›¾è¡¨
        composite_scores = analyzer.generate_comprehensive_plots(performance_df)
        
        # 6. ç”Ÿæˆå»ºè®®
        analyzer.generate_recommendations(performance_df, composite_scores)
        
        print("\nâœ… ç»Ÿä¸€åˆ†æå®Œæˆï¼")
        print("ğŸ“Š ç”Ÿæˆçš„å›¾ç‰‡:")
        print("- data/plots/focused_model_analysis.png")
    else:
        print("âŒ æ— æ³•è¿›è¡Œæ€§èƒ½åˆ†æ")

if __name__ == "__main__":
    main() 