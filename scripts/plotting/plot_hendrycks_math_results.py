#!/usr/bin/env python3
"""
Hendrycks Mathå››ä¸ªæ¨¡å‹ç»“æœå¯¹æ¯”ç»˜å›¾è„šæœ¬
ç»˜åˆ¶ä¸åŒæ¨¡å‹åœ¨Hendrycks Mathæ•°æ®é›†ä¸Šçš„æ€§èƒ½å¯¹æ¯”å›¾
"""

import os
import json
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import glob
from typing import Dict, List, Any
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®ç»˜å›¾æ ·å¼
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class HendrycksMathResultsPlotter:
    """Hendrycks Mathç»“æœç»˜å›¾å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç»˜å›¾å™¨"""
        self.results_dir = Path("data/hendrycks_math_results/deepseek-ai")
        self.plot_dir = Path("plot_data")
        self.plot_dir.mkdir(exist_ok=True)
        
        # æ¨¡å‹é…ç½®
        self.models = {
            "DeepSeek-R1-Distill-Qwen-1.5B": {
                "short_name": "1.5B",
                "color": "#FF6B6B",
                "marker": "o",
                "linewidth": 2,
                "markersize": 8
            },
            "DeepSeek-R1-Distill-Qwen-7B": {
                "short_name": "7B", 
                "color": "#4ECDC4",
                "marker": "s",
                "linewidth": 2,
                "markersize": 8
            },
            "DeepSeek-R1-Distill-Qwen-14B": {
                "short_name": "14B",
                "color": "#45B7D1", 
                "marker": "^",
                "linewidth": 2,
                "markersize": 8
            },
            "DeepSeek-R1-Distill-Qwen-32B": {
                "short_name": "32B",
                "color": "#96CEB4",
                "marker": "D",
                "linewidth": 2,
                "markersize": 8
            },
            "DeepSeek-R1-Distill-Llama-70B": {
                "short_name": "70B",
                "color": "#FFA07A",
                "marker": "p",
                "linewidth": 2,
                "markersize": 8
            }
        }
    
    def load_model_results(self, model_name: str) -> List[Dict]:
        """åŠ è½½æŒ‡å®šæ¨¡å‹çš„ç»“æœ"""
        model_dir = self.results_dir / model_name
        
        if not model_dir.exists():
            print(f"âš ï¸ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
            return []
        
        # æŸ¥æ‰¾æ‰€æœ‰çš„ç»“æœç›®å½•
        run_dirs = list(model_dir.glob("*"))
        if not run_dirs:
            print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç»“æœç›®å½•: {model_dir}")
            return []
        
        # è¿‡æ»¤å‡ºåŒ…å«final_results.jsonçš„ç›®å½•
        valid_dirs = []
        for run_dir in run_dirs:
            if run_dir.is_dir() and (run_dir / "final_results.json").exists():
                valid_dirs.append(run_dir)
        
        if not valid_dirs:
            print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°åŒ…å«final_results.jsonçš„æœ‰æ•ˆç›®å½•: {model_dir}")
            return []
        
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
        latest_run_dir = max(valid_dirs, key=lambda x: x.stat().st_ctime)
        results_file = latest_run_dir / "final_results.json"
        
        print(f"ğŸ” ä½¿ç”¨ç»“æœç›®å½•: {latest_run_dir}")
        
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                results_data = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ—§çš„MATH-500æ ¼å¼ï¼ˆç›´æ¥æ˜¯åˆ—è¡¨ï¼‰è¿˜æ˜¯æ–°çš„åŒ…å«"results"é”®çš„æ ¼å¼
            if isinstance(results_data, dict) and "results" in results_data:
                results = results_data["results"]
            else:
                results = results_data # Assume it's the direct list of results
                
            print(f"âœ… åŠ è½½ {model_name} ç»“æœ: {len(results)} ä¸ªæ ·æœ¬")
            return results
        except Exception as e:
            print(f"âŒ åŠ è½½ç»“æœå¤±è´¥: {e}")
            return []
    
    def analyze_results(self, results: List[Dict]) -> Dict[str, Any]:
        """åˆ†æç»“æœæ•°æ®"""
        if not results:
            return {}
        
        # æå–è¯„ä¼°åˆ†æ•°
        scores = []
        levels = []
        types = []
        subsets = []
        
        for result in results:
            evaluation = result.get('evaluation', {})
            if isinstance(evaluation, dict) and 'overall_score' in evaluation:
                scores.append(evaluation['overall_score'])
                
                # å¤„ç†levelå­—æ®µï¼šå°†"Level 1"è½¬æ¢ä¸º1
                level_str = result.get('level', 'Unknown')
                if isinstance(level_str, str) and level_str.startswith('Level '):
                    try:
                        level_num = int(level_str.split(' ')[1])
                        levels.append(level_num)
                    except (ValueError, IndexError):
                        levels.append('Unknown')
                else:
                    levels.append(level_str)
                
                types.append(result.get('type', 'Unknown'))
                subsets.append(result.get('subset', 'Unknown'))
        
        if not scores:
            return {}
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        analysis = {
            'total_samples': len(results),
            'valid_samples': len(scores),
            'mean_score': np.mean(scores),
            'std_score': np.std(scores),
            'min_score': np.min(scores),
            'max_score': np.max(scores),
            'median_score': np.median(scores),
            'scores': scores,
            'levels': levels,
            'types': types,
            'subsets': subsets
        }
        
        # æŒ‰éš¾åº¦ç­‰çº§ç»Ÿè®¡
        level_stats = {}
        for level, score in zip(levels, scores):
            if level not in level_stats:
                level_stats[level] = []
            level_stats[level].append(score)
        
        analysis['level_stats'] = {
            level: {
                'count': len(scores),
                'mean': np.mean(scores),
                'std': np.std(scores)
            }
            for level, scores in level_stats.items()
        }
        
        # æŒ‰é—®é¢˜ç±»å‹ç»Ÿè®¡
        type_stats = {}
        for problem_type, score in zip(types, scores):
            if problem_type not in type_stats:
                type_stats[problem_type] = []
            type_stats[problem_type].append(score)
        
        analysis['type_stats'] = {
            problem_type: {
                'count': len(scores),
                'mean': np.mean(scores),
                'std': np.std(scores)
            }
            for problem_type, scores in type_stats.items()
        }
        
        # æŒ‰å­é›†ç»Ÿè®¡
        subset_stats = {}
        for subset, score in zip(subsets, scores):
            if subset not in subset_stats:
                subset_stats[subset] = []
            subset_stats[subset].append(score)
        
        analysis['subset_stats'] = {
            subset: {
                'count': len(scores),
                'mean': np.mean(scores),
                'std': np.std(scores)
            }
            for subset, scores in subset_stats.items()
        }
        
        return analysis
    
    def plot_overall_comparison(self, all_analyses: Dict[str, Dict]):
        """ç»˜åˆ¶æ•´ä½“å¯¹æ¯”å›¾ - é‡‡ç”¨ä¸MATH-500å®Œå…¨ä¸€è‡´çš„æ ·å¼"""
        # åˆ›å»ºå¯¹æ¯”å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # å·¦ä¾§ï¼šæ¨ªè½´ä¸ºæ¨¡å‹å‚æ•°ï¼Œçºµè½´ä¸ºæ‰“åˆ†æƒ…å†µï¼Œäº”æ¡çº¿åˆ†åˆ«ä»£è¡¨ä¸åŒéš¾åº¦ç­‰çº§
        ax1.set_title('(a) Model Performance by Difficulty Level', 
                     fontsize=14, fontweight='bold', pad=20)
        
        # æ”¶é›†æ‰€æœ‰éš¾åº¦ç­‰çº§
        all_levels = set()
        for model_name, analysis in all_analyses.items():
            if analysis and analysis['level_stats']:
                all_levels.update(analysis['level_stats'].keys())
        all_levels = sorted(all_levels)
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹åç§°ï¼ˆçŸ­åç§°ï¼‰
        model_names = []
        for model_name in self.models.keys():
            if model_name in all_analyses:
                config = self.models[model_name]
                model_names.append(config['short_name'])
        
        # ä¸ºæ¯ä¸ªéš¾åº¦ç­‰çº§ç»˜åˆ¶ä¸€æ¡çº¿
        level_colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFA07A']
        level_markers = ['o', 's', '^', 'D', 'p']
        
        for i, level in enumerate(all_levels):
            level_scores = []
            
            # æ”¶é›†è¯¥éš¾åº¦ç­‰çº§åœ¨æ‰€æœ‰æ¨¡å‹ä¸­çš„å¹³å‡åˆ†æ•°
            for model_name in self.models.keys():
                if model_name in all_analyses and level in all_analyses[model_name]['level_stats']:
                    scores = all_analyses[model_name]['level_stats'][level]
                    level_scores.append(scores['mean'])
                else:
                    level_scores.append(np.nan)  # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œç”¨NaN
            
            # ç»˜åˆ¶è¯¥éš¾åº¦ç­‰çº§çš„çº¿
            ax1.plot(model_names, level_scores, 
                    color=level_colors[i % len(level_colors)], 
                    marker=level_markers[i % len(level_markers)],
                    linewidth=2,
                    markersize=8,
                    label=f'Level {level}')
        
        ax1.set_xlabel('Model Parameters', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Average Score', fontsize=12, fontweight='bold')
        ax1.set_ylim(9.0, 10.0)  # ä¸å³å›¾ä¿æŒä¸€è‡´çš„Yè½´èŒƒå›´
        
        # è®¾ç½®Yè½´åˆ»åº¦ï¼ˆ9.0-10.0åˆ†æ®µç»†åŒ–ï¼‰
        ax1.set_yticks([9.0, 9.2, 9.4, 9.6, 9.8, 10.0])
        ax1.set_yticklabels(['9.0', '9.2', '9.4', '9.6', '9.8', '10.0'])
        
        ax1.grid(True, alpha=0.3)
        ax1.legend(loc='lower left', frameon=True, fancybox=True, shadow=True)  # å°†å›¾ä¾‹ç§»åˆ°å·¦ä¸‹è§’
        
        # å³ä¾§ï¼šå¹³å‡æ¨¡å‹æ€§èƒ½åˆ†æï¼ˆæŒ‰éš¾åº¦ï¼‰
        ax2.set_title('(b) Average DeepSeek R1 Series Model Performance Analysis by Difficulty', 
                     fontsize=14, fontweight='bold', pad=20)
        
        # ç»˜åˆ¶æ¯ä¸ªæ¨¡å‹çš„æ€§èƒ½æ›²çº¿
        for model_name, analysis in all_analyses.items():
            if analysis and analysis['level_stats']:
                config = self.models[model_name]
                levels = []
                avg_scores = []
                
                for level in all_levels:
                    if level in analysis['level_stats']:
                        levels.append(level)
                        avg_scores.append(analysis['level_stats'][level]['mean'])
                    else:
                        levels.append(level) # Keep levels aligned
                        avg_scores.append(np.nan) # Use NaN for missing data
                
                # Filter out NaNs for plotting
                plot_levels = [l for l, s in zip(levels, avg_scores) if not np.isnan(s)]
                plot_scores = [s for s in avg_scores if not np.isnan(s)]

                if plot_levels:
                    ax2.plot(plot_levels, plot_scores, 
                            color=config['color'], 
                            marker=config['marker'],
                            linewidth=config['linewidth'],
                            markersize=8,
                            label=config['short_name'])
        
        ax2.set_xlabel('Problem Difficulty', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Average Score', fontsize=12, fontweight='bold')
        ax2.set_xlim(0.5, 5.5)  # è°ƒæ•´Xè½´èŒƒå›´ï¼Œå‡å°‘ä¸¤ä¾§ç©ºç™½
        
        # ä¿®æ”¹Yè½´åˆ»åº¦ï¼š9.0-10.0åˆ†æ®µç»†åŒ–
        ax2.set_ylim(9.0, 10.0)  # è®¾ç½®Yè½´èŒƒå›´ä¸º9.0-10.0
        ax2.set_yticks([9.0, 9.2, 9.4, 9.6, 9.8, 10.0])  # æ¯0.2ä¸€ä¸ªåˆ»åº¦
        ax2.set_yticklabels(['9.0', '9.2', '9.4', '9.6', '9.8', '10.0'])
        
        # ä¼˜åŒ–æ¨ªè½´æ ‡ç­¾ä½ç½®å’Œæ˜¾ç¤º
        ax2.set_xticks([1, 2, 3, 4, 5])
        ax2.set_xticklabels(['Level 1', 'Level 2', 'Level 3', 'Level 4', 'Level 5'], rotation=0)
        
        ax2.grid(True, alpha=0.3)
        ax2.legend(loc='lower left', frameon=True, fancybox=True, shadow=True)  # å°†å›¾ä¾‹ç§»åˆ°å·¦ä¸‹è§’
        
        # è®¡ç®—æ€»æ ·æœ¬æ•°ï¼ˆç”¨äºè¿”å›å€¼å’ŒæŠ¥å‘Šï¼‰
        total_samples = 0
        first_model_analysis = next(iter(all_analyses.values()), {})
        if first_model_analysis and first_model_analysis['level_stats']:
            for level, stats in first_model_analysis['level_stats'].items():
                total_samples += stats['count']
        
        plt.tight_layout()
        plt.savefig(self.plot_dir / 'hendrycks_math_overall_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… æ•´ä½“å¯¹æ¯”å›¾å·²ä¿å­˜: {self.plot_dir / 'hendrycks_math_overall_comparison.png'}")
        
        return total_samples
    
    def plot_individual_model_analysis(self, model_name: str, analysis: Dict):
        """ç»˜åˆ¶å•ä¸ªæ¨¡å‹çš„è¯¦ç»†åˆ†æ"""
        if not analysis:
            return
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'{model_name} è¯¦ç»†åˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾
        scores = analysis['scores']
        ax1.hist(scores, bins=20, alpha=0.7, color=self.models[model_name]['color'])
        ax1.axvline(analysis['mean_score'], color='red', linestyle='--', 
                    label=f'å¹³å‡å€¼: {analysis["mean_score"]:.2f}')
        ax1.set_title('åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾', fontsize=14, fontweight='bold')
        ax1.set_xlabel('åˆ†æ•°')
        ax1.set_ylabel('é¢‘æ¬¡')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. æŒ‰éš¾åº¦ç­‰çº§çš„åˆ†æ•°
        level_stats = analysis['level_stats']
        if level_stats:
            levels = sorted(level_stats.keys())
            level_scores = [level_stats[level]['mean'] for level in levels]
            level_counts = [level_stats[level]['count'] for level in levels]
            
            bars = ax2.bar(levels, level_scores, color=self.models[model_name]['color'], alpha=0.7)
            ax2.set_title('æŒ‰éš¾åº¦ç­‰çº§çš„åˆ†æ•°', fontsize=14, fontweight='bold')
            ax2.set_xlabel('éš¾åº¦ç­‰çº§')
            ax2.set_ylabel('å¹³å‡åˆ†æ•°')
            ax2.grid(True, alpha=0.3)
            
            # æ·»åŠ æ ·æœ¬æ•°é‡æ ‡ç­¾
            for bar, count in zip(bars, level_counts):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                        f'n={count}', ha='center', va='bottom', fontsize=10)
        
        # 3. æŒ‰é—®é¢˜ç±»å‹çš„åˆ†æ•°
        type_stats = analysis['type_stats']
        if type_stats:
            types = list(type_stats.keys())
            type_scores = [type_stats[problem_type]['mean'] for problem_type in types]
            type_counts = [type_stats[problem_type]['count'] for problem_type in types]
            
            bars = ax3.bar(types, type_scores, color=self.models[model_name]['color'], alpha=0.7)
            ax3.set_title('æŒ‰é—®é¢˜ç±»å‹çš„åˆ†æ•°', fontsize=14, fontweight='bold')
            ax3.set_xlabel('é—®é¢˜ç±»å‹')
            ax3.set_ylabel('å¹³å‡åˆ†æ•°')
            ax3.tick_params(axis='x', rotation=45)
            ax3.grid(True, alpha=0.3)
            
            # æ·»åŠ æ ·æœ¬æ•°é‡æ ‡ç­¾
            for bar, count in zip(bars, type_counts):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                        f'n={count}', ha='center', va='bottom', fontsize=10)
        
        # 4. ç»Ÿè®¡æ‘˜è¦
        ax4.axis('off')
        summary_text = f"""
ç»Ÿè®¡æ‘˜è¦:
æ€»æ ·æœ¬æ•°: {analysis['total_samples']}
æœ‰æ•ˆæ ·æœ¬æ•°: {analysis['valid_samples']}
å¹³å‡åˆ†æ•°: {analysis['mean_score']:.2f} Â± {analysis['std_score']:.2f}
æœ€é«˜åˆ†æ•°: {analysis['max_score']:.2f}
æœ€ä½åˆ†æ•°: {analysis['min_score']:.2f}
ä¸­ä½æ•°: {analysis['median_score']:.2f}
        """
        ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes, 
                fontsize=12, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        model_safe_name = model_name.replace('/', '_').replace('-', '_')
        plt.savefig(self.plot_dir / f'{model_safe_name}_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… {model_name} è¯¦ç»†åˆ†æå›¾å·²ä¿å­˜: {self.plot_dir / f'{model_safe_name}_analysis.png'}")
    
    def generate_summary_report(self, all_analyses: Dict[str, Dict]):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        report_file = self.plot_dir / 'hendrycks_math_summary_report.txt'
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("Hendrycks Mathå››ä¸ªæ¨¡å‹æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            # æ€»ä½“å¯¹æ¯”
            f.write("1. æ€»ä½“æ€§èƒ½å¯¹æ¯”\n")
            f.write("-" * 30 + "\n")
            
            for model_name, analysis in all_analyses.items():
                if analysis:
                    short_name = self.models[model_name]['short_name']
                    f.write(f"{short_name}æ¨¡å‹:\n")
                    f.write(f"  æ€»æ ·æœ¬æ•°: {analysis['total_samples']}\n")
                    f.write(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {analysis['valid_samples']}\n")
                    f.write(f"  å¹³å‡åˆ†æ•°: {analysis['mean_score']:.2f} Â± {analysis['std_score']:.2f}\n")
                    f.write(f"  åˆ†æ•°èŒƒå›´: {analysis['min_score']:.2f} - {analysis['max_score']:.2f}\n")
                    f.write(f"  ä¸­ä½æ•°: {analysis['median_score']:.2f}\n\n")
            
            # æŒ‰éš¾åº¦ç­‰çº§å¯¹æ¯”
            f.write("2. æŒ‰éš¾åº¦ç­‰çº§çš„æ€§èƒ½å¯¹æ¯”\n")
            f.write("-" * 30 + "\n")
            
            level_data = {}
            for model_name, analysis in all_analyses.items():
                if analysis and analysis['level_stats']:
                    short_name = self.models[model_name]['short_name']
                    for level, stats in analysis['level_stats'].items():
                        if level not in level_data:
                            level_data[level] = {}
                        level_data[level][short_name] = stats['mean']
            
            for level in sorted(level_data.keys()):
                f.write(f"éš¾åº¦ç­‰çº§ {level}:\n")
                for model_name, analysis in all_analyses.items():
                    if analysis:
                        short_name = self.models[model_name]['short_name']
                        score = level_data[level].get(short_name, 0)
                        f.write(f"  {short_name}: {score:.2f}\n")
                f.write("\n")
            
            # æŒ‰é—®é¢˜ç±»å‹å¯¹æ¯”
            f.write("3. æŒ‰é—®é¢˜ç±»å‹çš„æ€§èƒ½å¯¹æ¯”\n")
            f.write("-" * 30 + "\n")
            
            type_data = {}
            for model_name, analysis in all_analyses.items():
                if analysis and analysis['type_stats']:
                    short_name = self.models[model_name]['short_name']
                    for problem_type, stats in analysis['type_stats'].items():
                        if problem_type not in type_data:
                            type_data[problem_type] = {}
                        type_data[problem_type][short_name] = stats['mean']
            
            for problem_type in sorted(type_data.keys()):
                f.write(f"{problem_type}:\n")
                for model_name, analysis in all_analyses.items():
                    if analysis:
                        short_name = self.models[model_name]['short_name']
                        score = type_data[problem_type].get(short_name, 0)
                        f.write(f"  {short_name}: {score:.2f}\n")
                f.write("\n")
        
        print(f"âœ… æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ å¼€å§‹Hendrycks Mathå››ä¸ªæ¨¡å‹ç»“æœåˆ†æ...")
        
        all_analyses = {}
        
        # åŠ è½½æ‰€æœ‰æ¨¡å‹çš„ç»“æœ
        for model_name in self.models.keys():
            print(f"\nğŸ“Š åˆ†æ {model_name}...")
            results = self.load_model_results(model_name)
            analysis = self.analyze_results(results)
            all_analyses[model_name] = analysis
            
            if analysis:
                print(f"  å¹³å‡åˆ†æ•°: {analysis['mean_score']:.2f} Â± {analysis['std_score']:.2f}")
                print(f"  æ ·æœ¬æ•°é‡: {analysis['valid_samples']}/{analysis['total_samples']}")
        
        # ç”Ÿæˆå›¾è¡¨
        print("ğŸ“ˆ ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
        self.plot_overall_comparison(all_analyses)
        
        # ç§»é™¤å•ä¸ªæ¨¡å‹è¯¦ç»†åˆ†æå›¾çš„ç”Ÿæˆ
        # print("ğŸ“Š ç”Ÿæˆå•ä¸ªæ¨¡å‹è¯¦ç»†åˆ†æ...")
        # for model_name, analysis in all_analyses.items():
        #     if analysis:
        #         self.plot_individual_model_analysis(model_name, analysis)
        
        print("ğŸ“‹ ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
        self.generate_summary_report(all_analyses)
        
        print("\nâœ… Hendrycks Mathå››ä¸ªæ¨¡å‹ç»“æœåˆ†æå®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰å›¾è¡¨å’ŒæŠ¥å‘Šä¿å­˜åœ¨: {self.plot_dir}")

def main():
    """ä¸»å‡½æ•°"""
    plotter = HendrycksMathResultsPlotter()
    plotter.run_analysis()

if __name__ == "__main__":
    main() 