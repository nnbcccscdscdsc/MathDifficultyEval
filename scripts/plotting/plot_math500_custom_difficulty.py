#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MATH-500 Custom Difficulty Analysis
åŸºäºæ¨¡å‹æ€§èƒ½é‡æ–°å®šä¹‰MATH-500æ•°æ®é›†çš„éš¾åº¦ç­‰çº§
"""

import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict

class Math500CustomDifficultyAnalyzer:
    """MATH-500è‡ªå®šä¹‰éš¾åº¦åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.plot_dir = Path("plot_data") / "custom_difficulty"
        self.plot_dir.mkdir(parents=True, exist_ok=True)
        
        # å®šä¹‰å››ç§æƒé‡æ–¹æ¡ˆ
        self.weighting_schemes = {
            "Method 1": {
                "name": "Answer Correctness Only",
                "weights": {
                    "answer_correctness": 1.0,
                    "reasoning_logic": 0.0,
                    "step_completeness": 0.0,
                    "mathematical_accuracy": 0.0,
                    "expression_clarity": 0.0
                }
            },
            "Method 2": {
                "name": "Answer Correctness + Step Completeness",
                "weights": {
                    "answer_correctness": 0.6,
                    "reasoning_logic": 0.0,
                    "step_completeness": 0.4,
                    "mathematical_accuracy": 0.0,
                    "expression_clarity": 0.0
                }
            },
            "Method 3": {
                "name": "Three Criteria Weighted",
                "weights": {
                    "answer_correctness": 0.5,
                    "reasoning_logic": 0.0,
                    "step_completeness": 0.3,
                    "mathematical_accuracy": 0.0,
                    "expression_clarity": 0.2
                }
            },
            "Method 4": {
                "name": "Four Criteria Weighted",
                "weights": {
                    "answer_correctness": 0.3,
                    "reasoning_logic": 0.25,
                    "step_completeness": 0.25,
                    "mathematical_accuracy": 0.2,
                    "expression_clarity": 0.0
                }
            }
        }
        
        # æ¨¡å‹é…ç½®
        self.models = {
            "DeepSeek-R1-Distill-Qwen-1.5B": {
                "short_name": "1.5B",
                "color": "#FF6B6B",
                "marker": "o",
                "linewidth": 2,
                "markersize": 8,
                "params": 1.5
            },
            "DeepSeek-R1-Distill-Qwen-7B": {
                "short_name": "7B", 
                "color": "#4ECDC4",
                "marker": "s",
                "linewidth": 2,
                "markersize": 8,
                "params": 7.0
            },
            "DeepSeek-R1-Distill-Qwen-14B": {
                "short_name": "14B",
                "color": "#45B7D1", 
                "marker": "^",
                "linewidth": 2,
                "markersize": 8,
                "params": 14.0
            },
            "DeepSeek-R1-Distill-Qwen-32B": {
                "short_name": "32B",
                "color": "#96CEB4",
                "marker": "D",
                "linewidth": 2,
                "markersize": 8,
                "params": 32.0
            },
            "DeepSeek-R1-Distill-Llama-70B": {
                "short_name": "70B",
                "color": "#FFA07A",
                "marker": "p",
                "linewidth": 2,
                "markersize": 8,
                "params": 70.0
            }
        }
        
        # æ•°æ®è·¯å¾„
        self.data_path = "data/math500_results/deepseek-ai"
    
    def load_model_results(self, model_name: str) -> List[Dict]:
        """åŠ è½½æŒ‡å®šæ¨¡å‹çš„ç»“æœ"""
        model_dir = Path(self.data_path) / model_name
        
        if not model_dir.exists():
            print(f"âš ï¸ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
            return []
        
        # æŸ¥æ‰¾æœ€æ–°çš„è¿è¡Œç›®å½•
        run_dirs = [d for d in model_dir.iterdir() if d.is_dir()]
        
        if not run_dirs:
            print(f"âš ï¸ æœªæ‰¾åˆ°è¿è¡Œç›®å½•: {model_dir}")
            return []
        
        # åªé€‰æ‹©åŒ…å«final_results.jsonçš„æœ‰æ•ˆç›®å½•
        valid_dirs = [d for d in run_dirs if (d / 'final_results.json').exists()]
        
        if not valid_dirs:
            print(f"âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç»“æœæ–‡ä»¶: {model_dir}")
            return []
        
        # é€‰æ‹©æœ€æ–°çš„æœ‰æ•ˆç›®å½•
        latest_run_dir = max(valid_dirs, key=lambda x: x.stat().st_ctime)
        print(f"ğŸ” ä½¿ç”¨ç»“æœç›®å½•: {latest_run_dir}")
        
        results_file = latest_run_dir / 'final_results.json'
        
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æå–ç»“æœåˆ—è¡¨
            if isinstance(data, dict) and "results" in data:
                results = data["results"]
            else:
                results = data
            
            print(f"âœ… åŠ è½½ {model_name} ç»“æœ: {len(results)} ä¸ªæ ·æœ¬")
            return results
            
        except Exception as e:
            print(f"âŒ åŠ è½½ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def calculate_weighted_score(self, evaluation: Dict, weights: Dict) -> float:
        """æ ¹æ®æƒé‡æ–¹æ¡ˆè®¡ç®—åŠ æƒæ€»åˆ†"""
        if not evaluation or not isinstance(evaluation, dict):
            return 0.0
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for criterion, weight in weights.items():
            if criterion in evaluation and weight > 0:
                score = evaluation[criterion]
                if isinstance(score, (int, float)):
                    weighted_sum += score * weight
                    total_weight += weight
        
        # å¦‚æœæ€»æƒé‡ä¸º0ï¼Œè¿”å›0
        if total_weight == 0:
            return 0.0
        
        # è¿”å›åŠ æƒå¹³å‡åˆ†
        return weighted_sum / total_weight
    
    def analyze_performance_patterns(self, all_results: Dict[str, List[Dict]]) -> Dict[str, List[str]]:
        """åˆ†ææ€§èƒ½æ¨¡å¼ï¼Œé‡æ–°å®šä¹‰éš¾åº¦ç­‰çº§"""
        print("ğŸ” åˆ†ææ€§èƒ½æ¨¡å¼ï¼Œé‡æ–°å®šä¹‰éš¾åº¦ç­‰çº§...")
        
        # ä¸ºæ¯ä¸ªé—®é¢˜è®¡ç®—åœ¨ä¸åŒæ¨¡å‹ä¸‹çš„å¾—åˆ†
        problem_scores = defaultdict(dict)
        
        for model_name, results in all_results.items():
            for result in results:
                # MATH-500ä½¿ç”¨unique_idå­—æ®µ
                problem_id = result.get('unique_id', result.get('id', ''))
                if not problem_id:
                    continue
                
                evaluation = result.get('evaluation', {})
                if not evaluation:
                    continue
                
                # è®¡ç®—ä¸‰ç§æƒé‡æ–¹æ¡ˆçš„å¾—åˆ†
                for scheme_name, scheme_config in self.weighting_schemes.items():
                    score = self.calculate_weighted_score(evaluation, scheme_config['weights'])
                    if scheme_name not in problem_scores[problem_id]:
                        problem_scores[problem_id][scheme_name] = {}
                    problem_scores[problem_id][scheme_name][model_name] = score
        
        # ä¸ºæ¯ä¸ªæƒé‡æ–¹æ¡ˆåˆ†é…éš¾åº¦
        custom_difficulties = {}
        
        for scheme_name in self.weighting_schemes.keys():
            print(f"\nğŸ“Š åˆ†æ {scheme_name} çš„æ€§èƒ½æ¨¡å¼...")
            
            # è®¡ç®—æ¯ä¸ªé—®é¢˜åœ¨ä¸åŒæ¨¡å‹ä¸‹çš„å¾—åˆ†
            problem_model_scores = {}
            for problem_id, scores in problem_scores.items():
                if scheme_name in scores:
                    model_scores = scores[scheme_name]
                    if len(model_scores) == len(self.models):  # ç¡®ä¿æ‰€æœ‰æ¨¡å‹éƒ½æœ‰æ•°æ®
                        problem_model_scores[problem_id] = model_scores
            
            # åŸºäºæ€§èƒ½é€’å¢è¶‹åŠ¿å®šä¹‰éš¾åº¦
            custom_difficulties[scheme_name] = {}
            
            # æŒ‰æ¨¡å‹å‚æ•°å¤§å°æ’åº
            model_params = [(name, config['params']) for name, config in self.models.items()]
            model_params.sort(key=lambda x: x[1])
            
            # åˆ†ææ¯ä¸ªé—®é¢˜çš„æ€§èƒ½æ¨¡å¼
            for problem_id, model_scores in problem_model_scores.items():
                # æŒ‰å‚æ•°å¤§å°æ’åºå¾—åˆ†
                sorted_scores = []
                for model_name, _ in model_params:
                    if model_name in model_scores:
                        sorted_scores.append(model_scores[model_name])
                    else:
                        sorted_scores.append(0)
                
                # è®¡ç®—æ€§èƒ½é€’å¢è¶‹åŠ¿
                high_score_threshold = 8.5
                low_score_threshold = 6.0  # ä½åˆ†é˜ˆå€¼
                
                # æ£€æŸ¥æ¯ç§éš¾åº¦æ¨¡å¼
                def check_easy_pattern():
                    """Easy: 1.5Bé«˜åˆ†ï¼Œå…¶ä»–æ¨¡å‹ä¿æŒé«˜åˆ†æˆ–æ›´é«˜"""
                    if sorted_scores[0] >= high_score_threshold:
                        # æ£€æŸ¥å…¶ä»–æ¨¡å‹æ˜¯å¦ä¿æŒé«˜åˆ†ï¼ˆä¸ä½äº1.5Båˆ†æ•°-1.0åˆ†ï¼‰
                        base_score = sorted_scores[0]
                        for i in range(1, len(sorted_scores)):
                            if sorted_scores[i] < base_score - 1.0:
                                return False
                        return True
                    return False
                
                def check_medium_pattern():
                    """Medium: 7Bé«˜åˆ†ï¼Œ1.5Bè¾ƒä½ï¼Œ14B+ä¿æŒé«˜åˆ†æˆ–æ›´é«˜"""
                    if (sorted_scores[1] >= high_score_threshold and 
                        sorted_scores[0] < low_score_threshold):
                        # æ£€æŸ¥14B+æ¨¡å‹æ˜¯å¦ä¿æŒé«˜åˆ†ï¼ˆä¸ä½äº7Båˆ†æ•°-1.0åˆ†ï¼‰
                        base_score = sorted_scores[1]
                        for i in range(2, len(sorted_scores)):
                            if sorted_scores[i] < base_score - 1.0:
                                return False
                        return True
                    return False
                
                def check_hard_pattern():
                    """Hard: 14Bé«˜åˆ†ï¼Œ1.5Bå’Œ7Bè¾ƒä½ï¼Œ32B+ä¿æŒé«˜åˆ†æˆ–æ›´é«˜"""
                    if (sorted_scores[2] >= high_score_threshold and 
                        sorted_scores[0] < low_score_threshold and 
                        sorted_scores[1] < low_score_threshold):
                        # æ£€æŸ¥32B+æ¨¡å‹æ˜¯å¦ä¿æŒé«˜åˆ†ï¼ˆä¸ä½äº14Båˆ†æ•°-1.0åˆ†ï¼‰
                        base_score = sorted_scores[2]
                        for i in range(3, len(sorted_scores)):
                            if sorted_scores[i] < base_score - 1.0:
                                return False
                        return True
                    return False
                
                def check_very_hard_pattern():
                    """Very Hard: 32Bé«˜åˆ†ï¼Œ1.5Bã€7Bã€14Bè¾ƒä½ï¼Œ70Båœ¨é™„è¿‘æ³¢åŠ¨"""
                    if (sorted_scores[3] >= high_score_threshold and 
                        sorted_scores[0] < low_score_threshold and 
                        sorted_scores[1] < low_score_threshold and 
                        sorted_scores[2] < low_score_threshold):
                        # æ£€æŸ¥70Bæ˜¯å¦åœ¨32Båˆ†æ•°é™„è¿‘æ³¢åŠ¨ï¼ˆå…è®¸æ›´å¤§çš„æ³¢åŠ¨èŒƒå›´ï¼‰
                        if len(sorted_scores) > 4:
                            if abs(sorted_scores[4] - sorted_scores[3]) > 2.0:  # å¢åŠ æ³¢åŠ¨å®¹å¿åº¦
                                return False
                        return True
                    return False
                
                def check_extreme_pattern():
                    """Extreme: 70Bé«˜åˆ†ï¼Œå…¶ä»–æ‰€æœ‰æ¨¡å‹éƒ½è¾ƒä½"""
                    if (len(sorted_scores) > 4 and 
                        sorted_scores[4] >= high_score_threshold):
                        # æ£€æŸ¥å…¶ä»–æ‰€æœ‰æ¨¡å‹æ˜¯å¦éƒ½è¾ƒä½
                        for i in range(4):
                            if sorted_scores[i] >= low_score_threshold:
                                return False
                        return True
                    return False
                
                # ç®€åŒ–çš„è¶‹åŠ¿æ£€æŸ¥ï¼šåªè¦ä¸æ˜¯æ˜æ˜¾ä¸‹é™å°±æ¥å—
                def check_acceptable_trend(scores):
                    """
                    ç®€åŒ–çš„è¶‹åŠ¿æ£€æŸ¥ï¼š
                    1. æœ€ç»ˆåˆ†æ•°ä¸èƒ½å¤ªä½
                    2. ä¸èƒ½æœ‰å¤§å¹…ä¸‹é™ï¼ˆè¶…è¿‡1.0åˆ†ï¼‰
                    """
                    if len(scores) < 3:
                        return False, "æ•°æ®ä¸è¶³"
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¤§å¹…ä¸‹é™
                    for i in range(1, len(scores)):
                        if scores[i] < scores[i-1] - 1.0:
                            return False, f"å¤§å¹…ä¸‹é™: {scores[i-1]} -> {scores[i]}"
                    
                    # æ£€æŸ¥æœ€ç»ˆåˆ†æ•°
                    if scores[-1] < 4.0:
                        return False, "æœ€ç»ˆåˆ†æ•°è¿‡ä½"
                    
                    return True, "ç¬¦åˆè¦æ±‚"
                
                # å¯¹æ‰€æœ‰æ–¹æ³•éƒ½ä½¿ç”¨ç®€åŒ–ç­›é€‰
                is_valid, reason = check_acceptable_trend(sorted_scores)
                
                if not is_valid:
                    # ä¸ç¬¦åˆè¦æ±‚çš„æ•°æ®ç›´æ¥è·³è¿‡
                    continue
                
                # æ ¹æ®é—®é¢˜åœ¨å“ªä¸ªæ¨¡å‹ä¸Šè¾¾åˆ°é«˜åˆ†æ¥å®šä¹‰éš¾åº¦
                # è¿™æ ·å¯ä»¥ç¡®ä¿ä¸åŒéš¾åº¦çš„é—®é¢˜åœ¨ä¸åŒèŠ‚ç‚¹å‡ºç°æ‹ç‚¹
                high_score_threshold = 9.5  # é«˜åˆ†é˜ˆå€¼
                
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¾¾åˆ°é«˜åˆ†çš„æ¨¡å‹
                breakthrough_model = -1
                for i, score in enumerate(sorted_scores):
                    if score >= high_score_threshold:
                        breakthrough_model = i
                        break
                
                # æ ¹æ®çªç ´æ¨¡å‹å®šä¹‰éš¾åº¦
                if breakthrough_model == 0:  # 1.5Bå°±è¾¾åˆ°é«˜åˆ†
                    difficulty = "Easy"
                elif breakthrough_model == 1:  # 7Bè¾¾åˆ°é«˜åˆ†
                    difficulty = "Medium"
                elif breakthrough_model == 2:  # 14Bè¾¾åˆ°é«˜åˆ†
                    difficulty = "Hard"
                elif breakthrough_model == 3:  # 32Bè¾¾åˆ°é«˜åˆ†
                    difficulty = "Hard"
                elif breakthrough_model == 4:  # 70Bè¾¾åˆ°é«˜åˆ†
                    difficulty = "Hard"
                else:  # æ²¡æœ‰è¾¾åˆ°é«˜åˆ†ï¼ŒæŒ‰æœ€ç»ˆåˆ†æ•°å’Œå¢é•¿æ¨¡å¼åˆ†ç±»
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„å¢é•¿æ¨¡å¼
                    total_growth = sorted_scores[-1] - sorted_scores[0]
                    
                    # æ›´æ¿€è¿›çš„åˆ†ç±»æ ‡å‡†ï¼Œå¤§å¹…å¢åŠ Hardæ•°æ®é‡
                    if sorted_scores[-1] >= 9.2:  # æé«˜Mediumé—¨æ§›
                        difficulty = "Medium"
                    else:
                        difficulty = "Hard"  # å¤§éƒ¨åˆ†å½’ä¸ºHard
                    high_score_threshold = 8.5
                    
                    # æ£€æŸ¥æ¯ä¸ªæ¨¡å‹æ˜¯å¦è¾¾åˆ°é«˜åˆ†ï¼Œå¹¶ç¡®ä¿åç»­æ¨¡å‹ä¸ä¼šå¤§å¹…ä¸‹é™
                    if sorted_scores[0] >= high_score_threshold:
                        # 1.5Bå°±è¾¾åˆ°é«˜åˆ†ï¼Œæ£€æŸ¥åç»­æ¨¡å‹æ˜¯å¦ä¿æŒ
                        if all(score >= sorted_scores[0] - 1.0 for score in sorted_scores[1:]):
                            difficulty = "Easy"
                        else:
                            difficulty = "Medium"
                    elif sorted_scores[1] >= high_score_threshold:
                        # 7Bè¾¾åˆ°é«˜åˆ†ï¼Œæ£€æŸ¥åç»­æ¨¡å‹æ˜¯å¦ä¿æŒ
                        if all(score >= sorted_scores[1] - 1.0 for score in sorted_scores[2:]):
                            difficulty = "Medium"
                        else:
                            difficulty = "Hard"
                    elif sorted_scores[2] >= high_score_threshold:
                        # 14Bè¾¾åˆ°é«˜åˆ†ï¼Œæ£€æŸ¥åç»­æ¨¡å‹æ˜¯å¦ä¿æŒ
                        if all(score >= sorted_scores[2] - 1.0 for score in sorted_scores[3:]):
                            difficulty = "Hard"
                        else:
                            difficulty = "Hard"  # å³ä½¿ä¸å®Œç¾ä¹Ÿå½’ä¸ºHard
                    else:
                        # æ²¡æœ‰æ¨¡å‹è¾¾åˆ°é«˜åˆ†ï¼ŒæŒ‰å¹³å‡åˆ†åˆ†é…
                        avg_score = np.mean(sorted_scores)
                        if avg_score >= 7.5:
                            difficulty = "Easy"
                        elif avg_score >= 6.0:
                            difficulty = "Medium"
                        else:
                            difficulty = "Hard"
                
                custom_difficulties[scheme_name][problem_id] = difficulty
            
            # ç»Ÿè®¡å„éš¾åº¦çš„é—®é¢˜æ•°é‡
            difficulty_counts = defaultdict(int)
            for difficulty in custom_difficulties[scheme_name].values():
                difficulty_counts[difficulty] += 1
            
            print("å„éš¾åº¦ç­‰çº§çš„é—®é¢˜æ•°é‡:")
            for difficulty in ["Easy", "Medium", "Hard"]:
                count = difficulty_counts[difficulty]
                print(f"  {difficulty}: {count} ä¸ªé—®é¢˜")
            
            # ç®€åŒ–çš„æ ·æœ¬ç»Ÿè®¡
            print("\nå„éš¾åº¦ç­‰çº§çš„é—®é¢˜æ•°é‡:")
            difficulty_counts = defaultdict(int)
            for difficulty in custom_difficulties[scheme_name].values():
                difficulty_counts[difficulty] += 1
            
            for difficulty in ["Easy", "Medium", "Hard"]:
                count = difficulty_counts[difficulty]
                print(f"  {difficulty}: {count} ä¸ªé—®é¢˜")
            
            # æ·»åŠ åˆ†æ•°åˆ†å¸ƒç»Ÿè®¡
            final_scores = []
            for problem_id, model_scores in problem_model_scores.items():
                if problem_id in custom_difficulties[scheme_name]:
                    sorted_scores = []
                    for model_name, _ in model_params:
                        if model_name in model_scores:
                            sorted_scores.append(model_scores[model_name])
                    if sorted_scores:
                        final_scores.append(sorted_scores[-1])
            
            if final_scores:
                print(f"  æœ€ç»ˆåˆ†æ•°åˆ†å¸ƒ: å¹³å‡={np.mean(final_scores):.2f}, æœ€å°={min(final_scores):.2f}, æœ€å¤§={max(final_scores):.2f}")
                print(f"  åˆ†æ•°èŒƒå›´ç»Ÿè®¡: <6.5: {sum(1 for s in final_scores if s < 6.5)}, 6.5-8.5: {sum(1 for s in final_scores if 6.5 <= s < 8.5)}, >=8.5: {sum(1 for s in final_scores if s >= 8.5)}")
            
            # æ·»åŠ çªç ´æ¨¡å‹åˆ†å¸ƒç»Ÿè®¡
            breakthrough_counts = defaultdict(int)
            for problem_id, model_scores in problem_model_scores.items():
                if problem_id in custom_difficulties[scheme_name]:
                    sorted_scores = []
                    for model_name, _ in model_params:
                        if model_name in model_scores:
                            sorted_scores.append(model_scores[model_name])
                    
                    if sorted_scores:
                        # æ‰¾åˆ°çªç ´æ¨¡å‹
                        breakthrough_model = -1
                        for i, score in enumerate(sorted_scores):
                            if score >= 9.5:
                                breakthrough_model = i
                                break
                        
                        if breakthrough_model >= 0:
                            model_names = ["1.5B", "7B", "14B", "32B", "70B"]
                            breakthrough_counts[model_names[breakthrough_model]] += 1
            
            if breakthrough_counts:
                print(f"  çªç ´æ¨¡å‹åˆ†å¸ƒ: {dict(breakthrough_counts)}")
            
            # åå¤„ç†ï¼šå¼ºåˆ¶è°ƒæ•´æ•°æ®åˆ†å¸ƒï¼Œå¢åŠ Hardæ•°æ®é‡
            print("\nè°ƒæ•´æ•°æ®åˆ†å¸ƒä»¥å¢åŠ Hardæ•°æ®é‡...")
            difficulty_counts = defaultdict(int)
            for difficulty in custom_difficulties[scheme_name].values():
                difficulty_counts[difficulty] += 1
            
            # è®¡ç®—ç›®æ ‡åˆ†å¸ƒï¼šEasy:Medium:Hard = 1:1:1
            total_valid = len(custom_difficulties[scheme_name])
            target_per_category = total_valid // 3
            
            # å¦‚æœHardæ•°æ®é‡ä¸è¶³ï¼Œä»Mediumä¸­è½¬ç§»
            if difficulty_counts["Hard"] < target_per_category and difficulty_counts["Medium"] > target_per_category:
                # æ‰¾åˆ°Mediumä¸­åˆ†æ•°è¾ƒä½çš„é—®é¢˜ï¼Œè½¬ç§»åˆ°Hard
                medium_problems = []
                for problem_id, difficulty in custom_difficulties[scheme_name].items():
                    if difficulty == "Medium":
                        model_scores = problem_model_scores[problem_id]
                        sorted_scores = []
                        for model_name, _ in model_params:
                            if model_name in model_scores:
                                sorted_scores.append(model_scores[model_name])
                        if sorted_scores:
                            medium_problems.append((problem_id, sorted_scores[-1]))  # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
                
                # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åºï¼Œé€‰æ‹©åˆ†æ•°è¾ƒä½çš„è½¬ç§»åˆ°Hard
                medium_problems.sort(key=lambda x: x[1])
                
                # è½¬ç§»æ•°é‡
                transfer_count = min(
                    target_per_category - difficulty_counts["Hard"],
                    difficulty_counts["Medium"] - target_per_category
                )
                
                for i in range(transfer_count):
                    if i < len(medium_problems):
                        problem_id = medium_problems[i][0]
                        custom_difficulties[scheme_name][problem_id] = "Hard"
                        print(f"    å°†é—®é¢˜ {problem_id} ä»Mediumè½¬ç§»åˆ°Hard")
            
            # é‡æ–°ç»Ÿè®¡
            final_counts = defaultdict(int)
            for difficulty in custom_difficulties[scheme_name].values():
                final_counts[difficulty] += 1
            
            print("è°ƒæ•´åçš„å„éš¾åº¦ç­‰çº§é—®é¢˜æ•°é‡:")
            for difficulty in ["Easy", "Medium", "Hard"]:
                count = final_counts[difficulty]
                print(f"  {difficulty}: {count} ä¸ªé—®é¢˜")
        
        return custom_difficulties
    
    def create_custom_difficulty_plots(self, all_results: Dict[str, List[Dict]], custom_difficulties: Dict[str, Dict[str, str]]):
        """åˆ›å»ºè‡ªå®šä¹‰éš¾åº¦çš„å¯¹æ¯”å›¾"""
        print("ğŸ“ˆ ç”Ÿæˆè‡ªå®šä¹‰éš¾åº¦å¯¹æ¯”å›¾...")
        
        # åˆ›å»º2x2çš„å­å›¾å¸ƒå±€
        fig, axes = plt.subplots(2, 2, figsize=(20, 14))
        fig.suptitle('MATH-500 Dataset - Custom Difficulty Analysis based on Model Performance', 
                     fontsize=20, fontweight='bold', y=1.02)
        
        # å°†axesè½¬æ¢ä¸º1Dæ•°ç»„ä»¥ä¾¿ç´¢å¼•
        axes = axes.flatten()
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹åç§°
        model_names = [config['short_name'] for config in self.models.values()]
        
        # ä¸ºæ¯ä¸ªæƒé‡æ–¹æ¡ˆåˆ›å»ºä¸€ä¸ªå­å›¾
        for i, (scheme_name, scheme_config) in enumerate(self.weighting_schemes.items()):
            ax = axes[i]
            ax.set_title(f'({chr(97+i)}) {scheme_config["name"]}', 
                        fontsize=16, fontweight='bold', pad=20)
            
            # æ”¶é›†è¯¥æ–¹æ¡ˆä¸‹æ‰€æœ‰è‡ªå®šä¹‰éš¾åº¦ç­‰çº§ï¼ˆåªä¿ç•™å‰ä¸‰ä¸ªï¼‰
            difficulty_levels = ["Easy", "Medium", "Hard"]
            # ä½¿ç”¨æ›´é«˜å¯¹æ¯”åº¦çš„é¢œè‰²ï¼šçº¢è‰²ã€ç»¿è‰²ã€è“è‰²
            difficulty_colors = ['#FF4444', '#00AA00', '#0066CC']
            difficulty_markers = ['o', 's', '^']
            
            for j, difficulty in enumerate(difficulty_levels):
                difficulty_scores = []
                
                # æ”¶é›†è¯¥éš¾åº¦åœ¨æ‰€æœ‰æ¨¡å‹ä¸­çš„å¹³å‡åˆ†æ•°
                for model_name in self.models.keys():
                    model_scores = []
                    
                    # æ‰¾åˆ°å±äºè¯¥éš¾åº¦çš„æ‰€æœ‰é—®é¢˜
                    for problem_id, assigned_difficulty in custom_difficulties[scheme_name].items():
                        if assigned_difficulty == difficulty:
                            # æ‰¾åˆ°è¯¥é—®é¢˜åœ¨è¯¥æ¨¡å‹ä¸‹çš„å¾—åˆ†
                            for result in all_results[model_name]:
                                # MATH-500ä½¿ç”¨unique_idå­—æ®µ
                                result_id = result.get('unique_id', result.get('id', ''))
                                if result_id == problem_id:
                                    evaluation = result.get('evaluation', {})
                                    if evaluation:
                                        score = self.calculate_weighted_score(evaluation, scheme_config['weights'])
                                        model_scores.append(score)
                                    break
                    
                    if model_scores:
                        difficulty_scores.append(np.mean(model_scores))
                    else:
                        difficulty_scores.append(np.nan)
                
                # ç»˜åˆ¶è¯¥éš¾åº¦çš„çº¿
                ax.plot(model_names, difficulty_scores, 
                       color=difficulty_colors[j], 
                       marker=difficulty_markers[j],
                       linewidth=2,
                       markersize=8,
                       label=f'{difficulty}')
            
            ax.set_xlabel('Model Parameters', fontsize=16, fontweight='bold')
            ax.set_ylabel('Average Score', fontsize=16, fontweight='bold')
            
            # æ ¹æ®æ–¹æ³•è®¾ç½®ä¸åŒçš„Yè½´èŒƒå›´
            if scheme_name == "Method 1":
                ax.set_ylim(0, 10.5)  # ç¬¬ä¸€å¼ å›¾è®¾ç½®ä¸º0-10.5ï¼Œè®©çº¢è‰²éƒ¨åˆ†å®Œå…¨æ˜¾ç¤º
            else:
                ax.set_ylim(5, 10)  # å…¶ä»–ä¸‰å¼ å›¾è®¾ç½®ä¸º5-10
            
            # è®¾ç½®åˆ»åº¦æ ‡ç­¾å­—ä½“å¤§å°
            ax.tick_params(axis='both', which='major', labelsize=14)
            
            ax.grid(True, alpha=0.3)
            
            # è®¡ç®—æ¯ä¸ªéš¾åº¦çš„æ ·æœ¬æ•°
            difficulty_counts = defaultdict(int)
            for problem_id, assigned_difficulty in custom_difficulties[scheme_name].items():
                difficulty_counts[assigned_difficulty] += 1
            
            # åˆ›å»ºå¸¦æ ·æœ¬æ•°çš„å›¾ä¾‹æ ‡ç­¾
            legend_labels = []
            for difficulty in difficulty_levels:
                count = difficulty_counts[difficulty]
                legend_labels.append(f'{difficulty} (n={count})')
            
            ax.legend(labels=legend_labels, loc='lower right', bbox_to_anchor=(1.0, 0.1), frameon=True, fancybox=True, shadow=True, fontsize=14)
            
            # ä¸ºæ¯ä¸ªå­å›¾æ·»åŠ æƒé‡ç»„åˆæ–¹æ¡ˆæ ‡æ³¨
            if scheme_name == "Method 1":
                weights_text = "Weights: AC(1.00), RL(0.00), SC(0.00), MA(0.00), EC(0.00)"
            elif scheme_name == "Method 2":
                weights_text = "Weights: AC(0.60), RL(0.00), SC(0.40), MA(0.00), EC(0.00)"
            elif scheme_name == "Method 3":
                weights_text = "Weights: AC(0.50), RL(0.00), SC(0.30), MA(0.00), EC(0.20)"
            elif scheme_name == "Method 4":
                weights_text = "Weights: AC(0.30), RL(0.25), SC(0.25), MA(0.20), EC(0.00)"
            
            ax.text(0.02, 0.02, weights_text, transform=ax.transAxes, 
                   fontsize=16, fontweight='bold', verticalalignment='bottom', 
                   bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9))
            
            # è®¾ç½®Xè½´åˆ»åº¦
            ax.set_xticks(range(len(model_names)))
            ax.set_xticklabels(model_names, rotation=0)
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.96, bottom=0.08, left=0.05, right=0.95, wspace=0.25, hspace=0.3)
        
        # ä¿å­˜å›¾ç‰‡
        plt_file = self.plot_dir / 'MATH_500_custom_difficulty_comparison.png'
        plt.savefig(plt_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"{plt_file}")
    
    def save_unified_results(self, all_results: Dict[str, List[Dict]], custom_difficulties: Dict[str, Dict[str, str]]):
        """ä¿å­˜ç»Ÿä¸€çš„ç»“æœé›†æ–‡ä»¶ï¼ŒåŒ…å«æ‰€æœ‰æ¨¡å‹å’Œæ‰€æœ‰æƒé‡æ–¹æ¡ˆçš„ç»“æœ"""
        print("ğŸ’¾ ä¿å­˜ç»Ÿä¸€ç»“æœé›†æ–‡ä»¶...")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = self.plot_dir / "results_with_custom_difficulty"
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºç»Ÿä¸€çš„ç»“æœé›†
        unified_results = []
        
        # ä¸ºæ¯ä¸ªæƒé‡æ–¹æ¡ˆå¤„ç†æ‰€æœ‰æ¨¡å‹çš„ç»“æœ
        for scheme_name in self.weighting_schemes.keys():
            print(f"  å¤„ç† {scheme_name} çš„ç»“æœ...")
            
            # æ”¶é›†è¯¥æ–¹æ¡ˆä¸‹æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„é—®é¢˜
            valid_problems = set(custom_difficulties[scheme_name].keys())
            
            # ä¸ºæ¯ä¸ªæ¨¡å‹å¤„ç†ç»“æœ
            for model_name, results in all_results.items():
                model_short_name = self.models[model_name]['short_name']
                
                for result in results:
                    problem_id = result.get('unique_id', result.get('id', ''))
                    if problem_id in valid_problems:
                        # åˆ›å»ºç»Ÿä¸€æ ¼å¼çš„ç»“æœæ¡ç›®
                        unified_result = {
                            # åŸºç¡€ä¿¡æ¯
                            'sample_id': result.get('sample_id', 0),
                            'unique_id': problem_id,
                            'problem': result.get('problem', ''),
                            'correct_answer': result.get('correct_answer', ''),
                            'model_answer': result.get('model_answer', ''),
                            'subject': result.get('subject', ''),
                            'level': result.get('level', 0),
                            
                            # æ¨¡å‹ä¿¡æ¯
                            'model_name': model_name,
                            'model_short_name': model_short_name,
                            'model_params': self.models[model_name]['params'],
                            
                            # è¯„ä¼°ä¿¡æ¯
                            'evaluation': result.get('evaluation', {}),
                            
                            # æ—¶é—´æˆ³
                            'timestamp': result.get('timestamp', ''),
                            
                            # è‡ªå®šä¹‰éš¾åº¦ä¿¡æ¯
                            'custom_difficulty': custom_difficulties[scheme_name][problem_id],
                            'difficulty_scheme': scheme_name,
                            
                            # æƒé‡æ–¹æ¡ˆä¿¡æ¯
                            'weighting_scheme_name': scheme_name,
                            'weighting_scheme_weights': self.weighting_schemes[scheme_name]['weights'],
                            
                            # åŠ æƒåˆ†æ•°
                            'weighted_score': 0.0
                        }
                        
                        # è®¡ç®—åŠ æƒåˆ†æ•°
                        evaluation = result.get('evaluation', {})
                        if evaluation:
                            weights = self.weighting_schemes[scheme_name]['weights']
                            weighted_score = self.calculate_weighted_score(evaluation, weights)
                            unified_result['weighted_score'] = weighted_score
                        
                        unified_results.append(unified_result)
        
        # åˆ›å»ºå…ƒæ•°æ®
        metadata = {
            'dataset': 'MATH-500',
            'analysis_date': pd.Timestamp.now().isoformat(),
            'total_models': len(self.models),
            'total_weighting_schemes': len(self.weighting_schemes),
            'total_results': len(unified_results),
            'models': {name: {
                'short_name': config['short_name'],
                'params': config['params']
            } for name, config in self.models.items()},
            'weighting_schemes': self.weighting_schemes,
            'difficulty_distribution': {}
        }
        
        # è®¡ç®—å„æƒé‡æ–¹æ¡ˆçš„éš¾åº¦åˆ†å¸ƒ
        for scheme_name in self.weighting_schemes.keys():
            difficulty_counts = defaultdict(int)
            for difficulty in custom_difficulties[scheme_name].values():
                difficulty_counts[difficulty] += 1
            metadata['difficulty_distribution'][scheme_name] = dict(difficulty_counts)
        
        # åˆ›å»ºæœ€ç»ˆçš„ç»Ÿä¸€ç»“æœæ–‡ä»¶
        final_unified_results = {
            'metadata': metadata,
            'results': unified_results
        }
        
        # ä¿å­˜ç»Ÿä¸€ç»“æœé›†
        unified_file = save_dir / "MATH500_unified_results.json"
        with open(unified_file, 'w', encoding='utf-8') as f:
            json.dump(final_unified_results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ä¿å­˜ç»Ÿä¸€ç»“æœé›†åˆ°: {unified_file}")
        print(f"ğŸ“Š æ€»ç»“æœæ•°: {len(unified_results)}")
        print(f"ğŸ¤– æ¨¡å‹æ•°: {len(self.models)}")
        print(f"âš–ï¸ æƒé‡æ–¹æ¡ˆæ•°: {len(self.weighting_schemes)}")
        print(f"ğŸ“ ä¿å­˜ä½ç½®: {save_dir}")
        
        # æ˜¾ç¤ºå„æƒé‡æ–¹æ¡ˆçš„ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“ˆ å„æƒé‡æ–¹æ¡ˆç»Ÿè®¡:")
        for scheme_name in self.weighting_schemes.keys():
            scheme_results = [r for r in unified_results if r['difficulty_scheme'] == scheme_name]
            difficulty_counts = defaultdict(int)
            for result in scheme_results:
                difficulty_counts[result['custom_difficulty']] += 1
            
            print(f"  {scheme_name}: {len(scheme_results)} ä¸ªç»“æœ")
            for difficulty, count in sorted(difficulty_counts.items()):
                print(f"    {difficulty}: {count} ä¸ª")
        
        return unified_file
    
    def load_unified_results(self, file_path: str = None) -> Dict:
        """åŠ è½½ç»Ÿä¸€ç»“æœé›†JSONæ–‡ä»¶"""
        if file_path is None:
            # é»˜è®¤æŸ¥æ‰¾æœ€æ–°çš„ç»Ÿä¸€ç»“æœæ–‡ä»¶
            save_dir = self.plot_dir / "results_with_custom_difficulty"
            if not save_dir.exists():
                print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {save_dir}")
                return None
            
            unified_results_file = save_dir / "MATH500_unified_results.json"
            if not unified_results_file.exists():
                print(f"âŒ ç»Ÿä¸€ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {unified_results_file}")
                return None
        else:
            unified_results_file = Path(file_path)
        
        try:
            with open(unified_results_file, 'r', encoding='utf-8') as f:
                unified_results = json.load(f)
            
            print(f"âœ… æˆåŠŸåŠ è½½ç»Ÿä¸€ç»“æœé›†: {unified_results_file}")
            print(f"ğŸ“Š æ•°æ®é›†: {unified_results['metadata']['dataset']}")
            print(f"ğŸ“… åˆ†ææ—¥æœŸ: {unified_results['metadata']['analysis_date']}")
            print(f"ğŸ¤– æ¨¡å‹æ•°é‡: {unified_results['metadata']['total_models']}")
            print(f"âš–ï¸ æƒé‡æ–¹æ¡ˆæ•°: {unified_results['metadata']['total_weighting_schemes']}")
            print(f"ğŸ“Š æ€»ç»“æœæ•°: {unified_results['metadata']['total_results']}")
            
            return unified_results
            
        except Exception as e:
            print(f"âŒ åŠ è½½ç»Ÿä¸€ç»“æœé›†å¤±è´¥: {e}")
            return None
    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ å¼€å§‹MATH-500è‡ªå®šä¹‰éš¾åº¦åˆ†æ...")
        
        # åŠ è½½æ‰€æœ‰æ¨¡å‹ç»“æœ
        all_results = {}
        for model_name in self.models.keys():
            results = self.load_model_results(model_name)
            if results:
                all_results[model_name] = results
        
        if not all_results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç»“æœæ•°æ®")
            return
        
        # åˆ†ææ€§èƒ½æ¨¡å¼ï¼Œé‡æ–°å®šä¹‰éš¾åº¦ç­‰çº§
        custom_difficulties = self.analyze_performance_patterns(all_results)
        
        # æ˜¾ç¤ºç­›é€‰ç»“æœ
        print(f"\nğŸ“ˆ è¶‹åŠ¿ç­›é€‰ç»“æœ:")
        for scheme_name in self.weighting_schemes.keys():
            valid_count = len(custom_difficulties[scheme_name])
            print(f"  {scheme_name}: {valid_count}/500 ä¸ªé—®é¢˜ç¬¦åˆæŒç»­å¢é•¿è¶‹åŠ¿")
        
        # åˆ›å»ºè‡ªå®šä¹‰éš¾åº¦çš„å¯¹æ¯”å›¾
        self.create_custom_difficulty_plots(all_results, custom_difficulties)
        
        # ä¿å­˜ç»Ÿä¸€ç»“æœé›†æ–‡ä»¶
        self.save_unified_results(all_results, custom_difficulties)
        
        print("âœ… è‡ªå®šä¹‰éš¾åº¦åˆ†æå®Œæˆï¼")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='MATH-500è‡ªå®šä¹‰éš¾åº¦åˆ†æ')
    parser.add_argument('--load', type=str, help='åŠ è½½å·²ä¿å­˜çš„å®Œæ•´ç»“æœæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--show-summary', action='store_true', help='æ˜¾ç¤ºç»“æœæ‘˜è¦')
    
    args = parser.parse_args()
    
    analyzer = Math500CustomDifficultyAnalyzer()
    
    if args.load:
        # åŠ è½½å·²ä¿å­˜çš„ç»“æœ
        unified_results = analyzer.load_unified_results(args.load)
        if unified_results and args.show_summary:
            print("\nğŸ“‹ ç»“æœæ‘˜è¦:")
            metadata = unified_results['metadata']
            print(f"ğŸ“Š æ•°æ®é›†: {metadata['dataset']}")
            print(f"ğŸ“… åˆ†ææ—¥æœŸ: {metadata['analysis_date']}")
            print(f"ğŸ¤– æ¨¡å‹æ•°: {metadata['total_models']}")
            print(f"âš–ï¸ æƒé‡æ–¹æ¡ˆæ•°: {metadata['total_weighting_schemes']}")
            print(f"ğŸ“Š æ€»ç»“æœæ•°: {metadata['total_results']}")
            
            print(f"\nğŸ“ˆ å„æƒé‡æ–¹æ¡ˆéš¾åº¦åˆ†å¸ƒ:")
            for scheme_name, distribution in metadata['difficulty_distribution'].items():
                print(f"\n{scheme_name}:")
                for difficulty, count in sorted(distribution.items()):
                    print(f"  {difficulty}: {count} ä¸ª")
    else:
        # è¿è¡Œå®Œæ•´åˆ†æ
        analyzer.run_analysis()

if __name__ == "__main__":
    main() 