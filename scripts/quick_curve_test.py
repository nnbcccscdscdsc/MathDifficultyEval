#!/usr/bin/env python3
"""
å¿«é€ŸæŠ˜çº¿å›¾æµ‹è¯•è„šæœ¬ï¼šä½¿ç”¨ä¸¤ä¸ª7Bæ¨¡å‹å¿«é€Ÿç”Ÿæˆæ€§èƒ½æ›²çº¿
"""

import os
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
import time
from datetime import datetime
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from scripts.model_evaluation import ModelEvaluator
from scripts.results_analysis import ResultsAnalyzer
from scripts.utils import ConfigLoader, setup_logging

class QuickCurveTest:
    """å¿«é€ŸæŠ˜çº¿å›¾æµ‹è¯•"""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.config = ConfigLoader.load_config(config_path)
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # å¿«é€Ÿæµ‹è¯•æ¨¡å‹
        self.test_models = ["mistral-7b", "longalpaca-7b"]
        
        # æ¨¡å‹å‚æ•°æ˜ å°„ï¼ˆä¸ºäº†ç”Ÿæˆæ›²çº¿ï¼Œæˆ‘ä»¬ç»™å®ƒä»¬ä¸åŒçš„å‚æ•°å€¼ï¼‰
        self.model_params = {
            'mistral-7b': 7,      # 7Bå‚æ•°
            'longalpaca-7b': 8    # ç¨å¾®å¤§ä¸€ç‚¹ï¼Œæ¨¡æ‹Ÿå‚æ•°å·®å¼‚
        }
    
    def run_quick_evaluation(self, max_samples: int = 5):
        """è¿è¡Œå¿«é€Ÿè¯„ä¼°"""
        self.logger.info("å¼€å§‹å¿«é€ŸæŠ˜çº¿å›¾æµ‹è¯•")
        
        model_results = {}
        
        for model_name in self.test_models:
            self.logger.info(f"è¯„ä¼°æ¨¡å‹: {model_name}")
            
            try:
                # åˆ›å»ºè¯„ä¼°å™¨
                evaluator = ModelEvaluator()
                
                # åŠ è½½æ¨¡å‹
                evaluator.load_model(model_name, "4bit")
                
                # è¯„ä¼°æ•°æ®é›†
                results = evaluator.evaluate_dataset("sample", max_samples)
                
                # ä¿å­˜ç»“æœ
                summary = evaluator.save_results(results, model_name, "sample")
                
                # è½¬æ¢ä¸ºDataFrame
                df = pd.DataFrame(results)
                model_results[model_name] = df
                
                self.logger.info(f"æ¨¡å‹ {model_name} è¯„ä¼°å®Œæˆï¼Œå…± {len(results)} ä¸ªæ ·æœ¬")
                
                # æ‰“å°æ‘˜è¦
                if 'openai_score' in df.columns:
                    avg_openai_score = df['openai_score'].mean()
                    self.logger.info(f"æ¨¡å‹ {model_name} å¹³å‡OpenAIè¯„åˆ†: {avg_openai_score:.2f}")
                
                # æ¸…ç†å†…å­˜å’ŒGPUç¼“å­˜
                del evaluator
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # ç­‰å¾…ä¸€ä¸‹ï¼Œç¡®ä¿GPUèµ„æºé‡Šæ”¾
                import time
                time.sleep(5)
                
            except Exception as e:
                self.logger.error(f"è¯„ä¼°æ¨¡å‹ {model_name} å¤±è´¥: {e}")
                # æ¸…ç†GPUç¼“å­˜
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                continue
        
        return model_results
    
    def generate_curve_demo(self, model_results: Dict[str, pd.DataFrame]):
        """ç”ŸæˆæŠ˜çº¿å›¾æ¼”ç¤º"""
        if not model_results:
            self.logger.error("æ²¡æœ‰è¯„ä¼°ç»“æœ")
            return
        
        # åˆ›å»ºåˆ†æå™¨
        analyzer = ResultsAnalyzer()
        
        # ç”Ÿæˆæ¯”è¾ƒå›¾è¡¨
        analyzer.compare_models(model_results)
        
        # ç”Ÿæˆå‚æ•°æ›²çº¿å›¾
        analyzer.plot_model_parameter_curves(model_results)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        self.generate_demo_report(model_results)
    
    def generate_demo_report(self, model_results: Dict[str, pd.DataFrame]):
        """ç”Ÿæˆæ¼”ç¤ºæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        report = f"""
# å¿«é€ŸæŠ˜çº¿å›¾æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è§ˆ
- æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- æµ‹è¯•æ¨¡å‹: {', '.join(model_results.keys())}
- æµ‹è¯•ç›®çš„: éªŒè¯OpenAIè¯„åˆ†å’Œæ€§èƒ½æ›²çº¿ç”ŸæˆåŠŸèƒ½

## å„æ¨¡å‹æ€§èƒ½å¯¹æ¯”
"""
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        comparison_data = []
        for model_name, df in model_results.items():
            if len(df) == 0:
                continue
            
            metrics = {
                'model': model_name,
                'parameters': self.model_params.get(model_name, 7),
                'total_samples': len(df),
                'avg_accuracy': df['accuracy'].mean() if 'accuracy' in df.columns else 0,
                'avg_openai_score': df['openai_score'].mean() if 'openai_score' in df.columns else 0,
                'avg_generation_time': df['generation_time'].mean() if 'generation_time' in df.columns else 0
            }
            comparison_data.append(metrics)
        
        comparison_df = pd.DataFrame(comparison_data)
        
        for _, row in comparison_df.iterrows():
            report += f"""
### {row['model']} ({row['parameters']}Bå‚æ•°)
- æ ·æœ¬æ•°: {row['total_samples']}
- å¹³å‡å‡†ç¡®ç‡: {row['avg_accuracy']:.4f}
- å¹³å‡OpenAIè¯„åˆ†: {row['avg_openai_score']:.2f}
- å¹³å‡ç”Ÿæˆæ—¶é—´: {row['avg_generation_time']:.2f}ç§’
"""
        
        # æŒ‰éš¾åº¦åˆ†ç»„çš„è¯¦ç»†åˆ†æ
        report += "\n## å„éš¾åº¦ç­‰çº§è¯¦ç»†åˆ†æ\n"
        
        for model_name, df in model_results.items():
            report += f"\n### {model_name}\n"
            
            for difficulty in ['elementary', 'middle', 'college']:
                difficulty_df = df[df['difficulty'] == difficulty]
                if len(difficulty_df) > 0:
                    avg_score = difficulty_df['openai_score'].mean() if 'openai_score' in difficulty_df.columns else 0
                    report += f"- {difficulty}: {avg_score:.2f}åˆ† ({len(difficulty_df)}ä¸ªæ ·æœ¬)\n"
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.results_dir / f"quick_curve_test_report_{timestamp}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.logger.info(f"æ¼”ç¤ºæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return str(report_file)
    
    def run_demo(self, max_samples: int = 5):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("="*60)
        print("ğŸš€ å¿«é€ŸæŠ˜çº¿å›¾æµ‹è¯•æ¼”ç¤º")
        print("="*60)
        print(f"æµ‹è¯•æ¨¡å‹: {', '.join(self.test_models)}")
        print(f"æ ·æœ¬æ•°é‡: {max_samples} ä¸ª/æ¨¡å‹")
        print("="*60)
        
        # 1. è¿è¡Œè¯„ä¼°
        model_results = self.run_quick_evaluation(max_samples)
        
        if not model_results:
            print("âŒ æ²¡æœ‰æˆåŠŸè¯„ä¼°çš„æ¨¡å‹")
            return
        
        # 2. ç”Ÿæˆæ›²çº¿å’ŒæŠ¥å‘Š
        self.generate_curve_demo(model_results)
        
        # 3. æ‰“å°æ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ‰ å¿«é€Ÿæµ‹è¯•å®Œæˆï¼")
        print("="*60)
        
        # æ‰“å°æ€§èƒ½æ’å
        comparison_data = []
        for model_name, df in model_results.items():
            avg_score = df['openai_score'].mean() if 'openai_score' in df.columns else 0
            comparison_data.append((model_name, avg_score))
        
        comparison_data.sort(key=lambda x: x[1], reverse=True)
        
        print("ğŸ“Š æ€§èƒ½æ’å (æŒ‰OpenAIè¯„åˆ†):")
        for i, (model_name, score) in enumerate(comparison_data, 1):
            print(f"  {i}. {model_name}: {score:.2f}")
        
        print(f"\nğŸ“ ç»“æœæ–‡ä»¶ä½ç½®: {self.results_dir}")
        print("ğŸ“ˆ ç”Ÿæˆçš„å›¾è¡¨:")
        print("  - model_comparison.png (æ¨¡å‹å¯¹æ¯”å›¾)")
        print("  - model_parameter_curves.png (å‚æ•°æ›²çº¿å›¾)")
        print("  - model_parameter_curves_interactive.html (äº¤äº’å¼æ›²çº¿å›¾)")
        print("  - quick_curve_test_report_*.md (è¯¦ç»†æŠ¥å‘Š)")
        
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("  1. æŸ¥çœ‹ç”Ÿæˆçš„PNGå›¾ç‰‡")
        print("  2. æ‰“å¼€HTMLæ–‡ä»¶æŸ¥çœ‹äº¤äº’å¼å›¾è¡¨")
        print("  3. é˜…è¯»MarkdownæŠ¥å‘Šäº†è§£è¯¦ç»†ç»“æœ")

def main():
    parser = argparse.ArgumentParser(description="å¿«é€ŸæŠ˜çº¿å›¾æµ‹è¯•è„šæœ¬")
    parser.add_argument("--max-samples", type=int, default=5,
                       help="æ¯ä¸ªæ¨¡å‹çš„æœ€å¤§æ ·æœ¬æ•°é‡")
    parser.add_argument("--config", type=str, default="configs/config.yaml",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–æµ‹è¯•å™¨
        tester = QuickCurveTest(args.config)
        
        # è¿è¡Œæ¼”ç¤º
        tester.run_demo(args.max_samples)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 