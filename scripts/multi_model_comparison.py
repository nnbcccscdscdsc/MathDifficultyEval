#!/usr/bin/env python3
"""
å¤šæ¨¡å‹æ¯”è¾ƒè„šæœ¬ï¼šè¯„ä¼°å¤šä¸ªä¸åŒå‚æ•°çš„æ¨¡å‹å¹¶ç”Ÿæˆæ€§èƒ½æ›²çº¿
"""

import os
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
import time
from datetime import datetime
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from scripts.model_evaluation import ModelEvaluator
from scripts.results_analysis import ResultsAnalyzer
from scripts.utils import ConfigLoader, setup_logging

class MultiModelComparator:
    """å¤šæ¨¡å‹æ¯”è¾ƒå™¨"""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        """åˆå§‹åŒ–æ¯”è¾ƒå™¨"""
        self.config = ConfigLoader.load_config(config_path)
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
        self.supported_models = ["llama-7b", "llama-13b", "llama-70b"]
        
        # æ¨¡å‹å‚æ•°æ˜ å°„
        self.model_params = {
            'llama-7b': 7,
            'llama-13b': 13,
            'llama-70b': 70
        }
    
    def evaluate_models(self, models: List[str], dataset: str, 
                       quantization: str = "4bit", max_samples: Optional[int] = None) -> Dict[str, pd.DataFrame]:
        """è¯„ä¼°å¤šä¸ªæ¨¡å‹"""
        self.logger.info(f"å¼€å§‹è¯„ä¼°æ¨¡å‹: {models}")
        
        model_results = {}
        
        for model_name in models:
            if model_name not in self.supported_models:
                self.logger.warning(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
                continue
            
            self.logger.info(f"è¯„ä¼°æ¨¡å‹: {model_name}")
            
            try:
                # åˆ›å»ºè¯„ä¼°å™¨
                evaluator = ModelEvaluator()
                
                # åŠ è½½æ¨¡å‹
                evaluator.load_model(model_name, quantization)
                
                # è¯„ä¼°æ•°æ®é›†
                results = evaluator.evaluate_dataset(dataset, max_samples)
                
                # ä¿å­˜ç»“æœ
                summary = evaluator.save_results(results, model_name, dataset)
                
                # è½¬æ¢ä¸ºDataFrame
                df = pd.DataFrame(results)
                model_results[model_name] = df
                
                self.logger.info(f"æ¨¡å‹ {model_name} è¯„ä¼°å®Œæˆï¼Œå…± {len(results)} ä¸ªæ ·æœ¬")
                
                # æ‰“å°æ‘˜è¦
                if 'openai_score' in df.columns:
                    avg_openai_score = df['openai_score'].mean()
                    self.logger.info(f"æ¨¡å‹ {model_name} å¹³å‡OpenAIè¯„åˆ†: {avg_openai_score:.2f}")
                
                # æ¸…ç†å†…å­˜
                del evaluator
                
            except Exception as e:
                self.logger.error(f"è¯„ä¼°æ¨¡å‹ {model_name} å¤±è´¥: {e}")
                continue
        
        return model_results
    
    def generate_comparison_report(self, model_results: Dict[str, pd.DataFrame]) -> str:
        """ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š"""
        if not model_results:
            return "æ²¡æœ‰è¯„ä¼°ç»“æœ"
        
        report = f"""
# å¤šæ¨¡å‹æ¯”è¾ƒæŠ¥å‘Š

## è¯„ä¼°æ¦‚è§ˆ
- è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- è¯„ä¼°æ¨¡å‹æ•°é‡: {len(model_results)}
- è¯„ä¼°æ¨¡å‹: {', '.join(model_results.keys())}

## å„æ¨¡å‹æ€§èƒ½å¯¹æ¯”
"""
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        comparison_data = []
        for model_name, df in model_results.items():
            if len(df) == 0:
                continue
            
            metrics = {
                'model': model_name,
                'parameters': self.model_params.get(model_name, 0),
                'total_samples': len(df),
                'avg_accuracy': df['accuracy'].mean() if 'accuracy' in df.columns else 0,
                'avg_openai_score': df['openai_score'].mean() if 'openai_score' in df.columns else 0,
                'avg_generation_time': df['generation_time'].mean() if 'generation_time' in df.columns else 0
            }
            comparison_data.append(metrics)
        
        comparison_df = pd.DataFrame(comparison_data)
        
        # æŒ‰å‚æ•°æ•°é‡æ’åº
        comparison_df = comparison_df.sort_values('parameters')
        
        for _, row in comparison_df.iterrows():
            report += f"""
### {row['model']} ({row['parameters']}Bå‚æ•°)
- æ ·æœ¬æ•°: {row['total_samples']}
- å¹³å‡å‡†ç¡®ç‡: {row['avg_accuracy']:.4f}
- å¹³å‡OpenAIè¯„åˆ†: {row['avg_openai_score']:.2f}
- å¹³å‡ç”Ÿæˆæ—¶é—´: {row['avg_generation_time']:.2f}ç§’
"""
        
        # æ€§èƒ½è¶‹åŠ¿åˆ†æ
        if len(comparison_df) > 1:
            report += "\n## æ€§èƒ½è¶‹åŠ¿åˆ†æ\n"
            
            # è®¡ç®—æ€§èƒ½æå‡
            for i in range(1, len(comparison_df)):
                prev_model = comparison_df.iloc[i-1]
                curr_model = comparison_df.iloc[i]
                
                param_increase = curr_model['parameters'] - prev_model['parameters']
                score_increase = curr_model['avg_openai_score'] - prev_model['avg_openai_score']
                
                report += f"""
ä» {prev_model['model']} åˆ° {curr_model['model']}:
- å‚æ•°å¢åŠ : {param_increase}B
- OpenAIè¯„åˆ†æå‡: {score_increase:.2f}
- æ¯Bå‚æ•°æå‡: {score_increase/param_increase:.2f}
"""
        
        return report
    
    def save_comparison_results(self, model_results: Dict[str, pd.DataFrame], 
                               comparison_report: str) -> str:
        """ä¿å­˜æ¯”è¾ƒç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.results_dir / f"multi_model_comparison_{timestamp}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(comparison_report)
        
        # ä¿å­˜æ±‡æ€»æ•°æ®
        summary_data = []
        for model_name, df in model_results.items():
            if len(df) == 0:
                continue
            
            # æŒ‰éš¾åº¦åˆ†ç»„ç»Ÿè®¡
            for difficulty in ['elementary', 'middle', 'college']:
                difficulty_df = df[df['difficulty'] == difficulty]
                if len(difficulty_df) > 0:
                    summary_data.append({
                        'model': model_name,
                        'parameters': self.model_params.get(model_name, 0),
                        'difficulty': difficulty,
                        'sample_count': len(difficulty_df),
                        'avg_openai_score': difficulty_df['openai_score'].mean() if 'openai_score' in difficulty_df.columns else 0,
                        'avg_accuracy': difficulty_df['accuracy'].mean() if 'accuracy' in difficulty_df.columns else 0,
                        'avg_generation_time': difficulty_df['generation_time'].mean() if 'generation_time' in difficulty_df.columns else 0
                    })
        
        summary_df = pd.DataFrame(summary_data)
        summary_file = self.results_dir / f"multi_model_summary_{timestamp}.csv"
        summary_df.to_csv(summary_file, index=False, encoding='utf-8')
        
        self.logger.info(f"æ¯”è¾ƒç»“æœå·²ä¿å­˜: {report_file}")
        self.logger.info(f"æ±‡æ€»æ•°æ®å·²ä¿å­˜: {summary_file}")
        
        return str(report_file)
    
    def run_comparison(self, models: List[str], dataset: str, 
                      quantization: str = "4bit", max_samples: Optional[int] = None):
        """è¿è¡Œå®Œæ•´çš„æ¯”è¾ƒæµç¨‹"""
        self.logger.info("å¼€å§‹å¤šæ¨¡å‹æ¯”è¾ƒæµç¨‹")
        
        # 1. è¯„ä¼°æ‰€æœ‰æ¨¡å‹
        model_results = self.evaluate_models(models, dataset, quantization, max_samples)
        
        if not model_results:
            self.logger.error("æ²¡æœ‰æˆåŠŸè¯„ä¼°çš„æ¨¡å‹")
            return
        
        # 2. ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
        comparison_report = self.generate_comparison_report(model_results)
        
        # 3. ä¿å­˜ç»“æœ
        report_file = self.save_comparison_results(model_results, comparison_report)
        
        # 4. ç”Ÿæˆå¯è§†åŒ–
        analyzer = ResultsAnalyzer()
        
        # ç”Ÿæˆæ¯”è¾ƒå›¾è¡¨
        analyzer.compare_models(model_results)
        
        # ç”Ÿæˆå‚æ•°æ›²çº¿å›¾
        analyzer.plot_model_parameter_curves(model_results)
        
        # 5. æ‰“å°æ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ‰ å¤šæ¨¡å‹æ¯”è¾ƒå®Œæˆï¼")
        print("="*60)
        print(f"è¯„ä¼°æ¨¡å‹: {', '.join(model_results.keys())}")
        print(f"æ•°æ®é›†: {dataset}")
        print(f"é‡åŒ–æ–¹å¼: {quantization}")
        
        # æ‰“å°æ€§èƒ½æ’å
        comparison_data = []
        for model_name, df in model_results.items():
            avg_score = df['openai_score'].mean() if 'openai_score' in df.columns else 0
            comparison_data.append((model_name, avg_score))
        
        comparison_data.sort(key=lambda x: x[1], reverse=True)
        
        print("\nğŸ“Š æ€§èƒ½æ’å (æŒ‰OpenAIè¯„åˆ†):")
        for i, (model_name, score) in enumerate(comparison_data, 1):
            print(f"  {i}. {model_name}: {score:.2f}")
        
        print(f"\nğŸ“ ç»“æœæ–‡ä»¶ä½ç½®: {self.results_dir}")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")

def main():
    parser = argparse.ArgumentParser(description="å¤šæ¨¡å‹æ¯”è¾ƒè„šæœ¬")
    parser.add_argument("--models", nargs="+", default=["llama-7b", "llama-13b"],
                       choices=["llama-7b", "llama-13b", "llama-70b"],
                       help="è¦æ¯”è¾ƒçš„æ¨¡å‹åˆ—è¡¨")
    parser.add_argument("--dataset", type=str, default="sample",
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--quantization", type=str, default="4bit",
                       choices=["none", "4bit", "8bit"],
                       help="é‡åŒ–æ–¹å¼")
    parser.add_argument("--max-samples", type=int, default=None,
                       help="æ¯ä¸ªæ¨¡å‹çš„æœ€å¤§æ ·æœ¬æ•°é‡")
    parser.add_argument("--config", type=str, default="configs/config.yaml",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¯”è¾ƒå™¨
    comparator = MultiModelComparator(args.config)
    
    try:
        # è¿è¡Œæ¯”è¾ƒ
        comparator.run_comparison(
            models=args.models,
            dataset=args.dataset,
            quantization=args.quantization,
            max_samples=args.max_samples
        )
        
    except Exception as e:
        print(f"âŒ æ¯”è¾ƒå¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 