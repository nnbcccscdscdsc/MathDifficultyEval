#!/usr/bin/env python3
"""
ä¸“é—¨æµ‹è¯•mistral-community/Mistral-7B-v0.2æ¨¡å‹
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import logging
import time

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_mistral_community():
    """æµ‹è¯•mistral-community/Mistral-7B-v0.2æ¨¡å‹"""
    model_name = "mistral-community/Mistral-7B-v0.2"
    
    logger.info(f"ğŸ§ª æµ‹è¯•mistral-communityæ¨¡å‹: {model_name}")
    
    try:
        # 1. åŠ è½½tokenizer
        logger.info("åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, local_files_only=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        logger.info(f"EOS token ID: {tokenizer.eos_token_id}")
        logger.info(f"PAD token ID: {tokenizer.pad_token_id}")
        
        # 2. åŠ è½½æ¨¡å‹ - å…ˆå°è¯•ä¸ä½¿ç”¨é‡åŒ–
        logger.info("åŠ è½½æ¨¡å‹ï¼ˆä¸ä½¿ç”¨é‡åŒ–ï¼‰...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            local_files_only=True
        )
        
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        # 3. æµ‹è¯•æ¨ç†
        test_question = "What is 2 + 2?"
        
        # å°è¯•ä¸åŒçš„æç¤ºæ ¼å¼
        test_prompts = [
            {
                "name": "ç®€å•æ ¼å¼",
                "prompt": f"{test_question}\nAnswer:"
            },
            {
                "name": "Mistralæ ¼å¼",
                "prompt": f"[INST] {test_question} [/INST]"
            },
            {
                "name": "å¸¦<s>æ ‡ç­¾",
                "prompt": f"<s>[INST] {test_question} [/INST]"
            }
        ]
        
        for i, test_case in enumerate(test_prompts):
            logger.info(f"\n{'='*50}")
            logger.info(f"æµ‹è¯•æç¤ºæ ¼å¼ {i+1}: {test_case['name']}")
            logger.info(f"{'='*50}")
            
            prompt = test_case['prompt']
            logger.info(f"æç¤º: {prompt}")
            
            # ç¼–ç è¾“å…¥
            inputs = tokenizer(prompt, return_tensors="pt")
            input_ids = inputs.input_ids.to(model.device)
            
            logger.info(f"è¾“å…¥é•¿åº¦: {input_ids.shape[1]}")
            logger.info(f"è¾“å…¥tokens: {input_ids}")
            
            logger.info("å¼€å§‹ç”Ÿæˆ...")
            start_time = time.time()
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = model.generate(
                    input_ids,
                    max_new_tokens=10,
                    do_sample=False,
                    num_beams=1,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            end_time = time.time()
            generation_time = end_time - start_time
            
            logger.info(f"ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {generation_time:.2f}ç§’")
            logger.info(f"è¾“å‡ºå½¢çŠ¶: {outputs.shape}")
            logger.info(f"è¾“å‡ºtokens: {outputs}")
            
            # è§£ç 
            full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            logger.info(f"å®Œæ•´è¾“å‡º: {full_response}")
            
            # æå–ç­”æ¡ˆ
            if prompt in full_response:
                answer = full_response[len(prompt):].strip()
            else:
                answer = full_response.strip()
            
            logger.info(f"æå–çš„ç­”æ¡ˆ: '{answer}'")
            
            if answer.strip():
                logger.info("âœ… ç”ŸæˆæˆåŠŸï¼")
                return answer.strip()
            else:
                logger.warning("âš ï¸ ç”Ÿæˆäº†ç©ºç­”æ¡ˆ")
            
            logger.info("-" * 30)
        
        logger.error("âŒ æ‰€æœ‰æç¤ºæ ¼å¼éƒ½å¤±è´¥äº†")
        return ""
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return ""

def test_mistral_community_with_quantization():
    """æµ‹è¯•mistral-communityæ¨¡å‹ï¼ˆä½¿ç”¨é‡åŒ–ï¼‰"""
    model_name = "mistral-community/Mistral-7B-v0.2"
    
    logger.info(f"ğŸ§ª æµ‹è¯•mistral-communityæ¨¡å‹ï¼ˆä½¿ç”¨é‡åŒ–ï¼‰: {model_name}")
    
    try:
        # 1. åŠ è½½tokenizer
        logger.info("åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, local_files_only=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # 2. åŠ è½½æ¨¡å‹ - ä½¿ç”¨é‡åŒ–
        logger.info("åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨4bité‡åŒ–ï¼‰...")
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            local_files_only=True
        )
        
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        # 3. æµ‹è¯•æ¨ç†
        test_question = "What is 2 + 2?"
        prompt = f"[INST] {test_question} [/INST]"
        
        logger.info(f"æç¤º: {prompt}")
        
        # ç¼–ç è¾“å…¥
        inputs = tokenizer(prompt, return_tensors="pt")
        input_ids = inputs.input_ids.to(model.device)
        
        logger.info(f"è¾“å…¥é•¿åº¦: {input_ids.shape[1]}")
        
        logger.info("å¼€å§‹ç”Ÿæˆ...")
        start_time = time.time()
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                input_ids,
                max_new_tokens=10,
                do_sample=False,
                num_beams=1,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        end_time = time.time()
        generation_time = end_time - start_time
        
        logger.info(f"ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {generation_time:.2f}ç§’")
        logger.info(f"è¾“å‡ºå½¢çŠ¶: {outputs.shape}")
        
        # è§£ç 
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        logger.info(f"å®Œæ•´è¾“å‡º: {full_response}")
        
        # æå–ç­”æ¡ˆ
        if "[/INST]" in full_response:
            answer = full_response.split("[/INST]")[-1].strip()
        else:
            answer = full_response.strip()
        
        logger.info(f"æå–çš„ç­”æ¡ˆ: '{answer}'")
        
        if answer.strip():
            logger.info("âœ… é‡åŒ–æ¨¡å‹ç”ŸæˆæˆåŠŸï¼")
        else:
            logger.warning("âš ï¸ é‡åŒ–æ¨¡å‹ç”Ÿæˆäº†ç©ºç­”æ¡ˆ")
        
        return answer.strip()
        
    except Exception as e:
        logger.error(f"âŒ é‡åŒ–æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return ""

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ” æµ‹è¯•mistral-community/Mistral-7B-v0.2æ¨¡å‹")
    
    # 1. å…ˆæµ‹è¯•ä¸ä½¿ç”¨é‡åŒ–
    logger.info(f"\n{'='*60}")
    answer_no_quant = test_mistral_community()
    logger.info(f"{'='*60}")
    
    # 2. å¦‚æœå¤±è´¥ï¼Œæµ‹è¯•ä½¿ç”¨é‡åŒ–
    if not answer_no_quant.strip():
        logger.info("æ— é‡åŒ–ç‰ˆæœ¬å¤±è´¥ï¼Œå°è¯•é‡åŒ–ç‰ˆæœ¬...")
        logger.info(f"\n{'='*60}")
        answer_quant = test_mistral_community_with_quantization()
        logger.info(f"{'='*60}")
        
        # æ€»ç»“
        logger.info("\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
        logger.info(f"æ— é‡åŒ–ç‰ˆæœ¬: {'âœ… æˆåŠŸ' if answer_no_quant.strip() else 'âŒ å¤±è´¥'} - '{answer_no_quant}'")
        logger.info(f"é‡åŒ–ç‰ˆæœ¬: {'âœ… æˆåŠŸ' if answer_quant.strip() else 'âŒ å¤±è´¥'} - '{answer_quant}'")
    else:
        logger.info("\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
        logger.info(f"æ— é‡åŒ–ç‰ˆæœ¬: âœ… æˆåŠŸ - '{answer_no_quant}'")

if __name__ == "__main__":
    main() 